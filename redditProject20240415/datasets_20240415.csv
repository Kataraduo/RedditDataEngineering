id,title,selftext,score,num_comments,author,created_utc,url,gilded,subreddit
3bxlg7,I have every publicly available Reddit comment for research. ~ 1.7 billion comments @ 250 GB compressed. Any interest in this?,"I am currently doing a massive analysis of Reddit's entire publicly available comment dataset.  The dataset is ~1.7 billion JSON objects complete with the comment, score, author, subreddit, position in comment tree and other fields that are available through Reddit's API.  

I'm currently doing NLP analysis and also putting the entire dataset into a large searchable database using Sphinxsearch (also testing ElasticSearch).  

This dataset is over 1 terabyte uncompressed, so this would be best for larger research projects.  If you're interested in a sample month of comments, that can be arranged as well.  I am trying to find a place to host this large dataset -- I'm reaching out to Amazon since they have open data initiatives.

**EDIT:  ~~I'm putting up a Digital Ocean box with 2 TB of bandwidth and will throw an entire months worth of comments up (~ 5 gigs compressed)~~  It's now a torrent.  This will give you guys an opportunity to examine the data.  The file is structured with JSON blocks delimited by new lines (\n).**

**____________________________________________________**

One month of comments is now available here:

**Download Link:** [Torrent](https://mega.nz/#!ysBWXRqK!yPXLr25PgJi184pbJU3GtnqUY4wG7YvuPpxJjEmnb9A)

**Direct Magnet File:** magnet:?xt=urn:btih:32916ad30ce4c90ee4c47a95bd0075e44ac15dd2&dn=RC%5F2015-01.bz2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Fopen.demonii.com%3A1337&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969

**Tracker:** udp://tracker.openbittorrent.com:80

**Total Comments:** 53,851,542

**Compression Type:** bzip2 *(5,452,413,560 bytes compressed | 31,648,374,104 bytes uncompressed)*

**md5:** a3fc3d9db18786e4486381a7f37d08e2  RC_2015-01.bz2

**____________________________________________________**

**Example JSON Block:**

    {""gilded"":0,""author_flair_text"":""Male"",""author_flair_css_class"":""male"",""retrieved_on"":1425124228,""ups"":3,""subreddit_id"":""t5_2s30g"",""edited"":false,""controversiality"":0,""parent_id"":""t1_cnapn0k"",""subreddit"":""AskMen"",""body"":""I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. "",""created_utc"":""1420070668"",""downs"":0,""score"":3,""author"":""TheDukeofEtown"",""archived"":false,""distinguished"":null,""id"":""cnasd6x"",""score_hidden"":false,""name"":""t1_cnasd6x"",""link_id"":""t3_2qyhmp""}


**UPDATE (Saturday 2015-07-03 13:26 ET)**

I'm getting a huge response from this and won't be able to immediately reply to everyone.  I am pinging some people who are helping.  There are two major issues at this point.  Getting the data from my local system to wherever and figuring out bandwidth (since this is a very large dataset).  Please keep checking for new updates.  I am working to make this data publicly available ASAP.  If you're a larger organization or university and have the ability to help seed this initially (will probably require 100 TB of bandwidth to get it rolling), please let me know.  If you can agree to do this, I'll give your organization priority over the data first.

**UPDATE 2 (15:18)**

I've purchased a seedbox.  I'll be updating the link above to the sample file.  Once I can get the full dataset to the seedbox, I'll post the torrent and magnet link to that as well.  I want to thank /u/hak8or for all his help during this process.  It's been a while since I've created torrents and he has been a huge help with explaining how it all works.  Thanks man!

**UPDATE 3 (21:09)**

I'm creating the complete torrent.  There was an issue with my seedbox not allowing public trackers for uploads, so I had to create a private tracker.  I should have a link up shortly to the massive torrent.  I would really appreciate it if people at least seed at 1:1 ratio -- and if you can do more, that's even better!  The size looks to be around ~160 GB -- a bit less than I thought.

**UPDATE 4 (00:49 July 4)**

I'm retiring for the evening.  I'm currently seeding the entire archive to two seedboxes plus two other people.  I'll post the link tomorrow evening once the seedboxes are at 100%.  This will help prevent choking the upload from my home connection if too many people jump on at once.  The seedboxes upload at around 35MB a second in the best case scenario.  We should be good tomorrow evening when I post it.  Happy July 4'th to my American friends!

**UPDATE 5 (14:44)**

Send more beer!  The seedboxes are around 75% and should be finishing up within the next 8 hours.  My next update before I retire for the night will be a magnet link to the main archive.  Thanks!

**UPDATE 6 (20:17)**

**This is the update you've been waiting for!**

The **entire** archive:  

    magnet:?xt=urn:btih:7690f71ea949b868080401c749e878f98de34d3d&dn=reddit%5Fdata&tr=http%3A%2F%2Ftracker.pushshift.io%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80

Please seed!

**UPDATE 7 (July 11 14:19)**

User /u/fhoffa has done a lot of great work making this data available within Google's BigQuery.   Please check out this link for more information:   /r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/

Awesome work!

",1106,250,Stuck_In_the_Matrix,2015-07-03 00:05:46,https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/,2,datasets
lsnml4,"I spent the last 8 months during lockdown pouring my soul into a website that allows you to visualize virtually every U.S. company's international supply chain. E.x. What products, how much, which factories and where does Lululemon import from? (Just type a company in the search box)",,550,17,tmsteph,2021-02-26 02:31:54,https://www.importyeti.com,0,datasets
exnzrd,Coronavirus Datasets,"You have probably seen most of these, but I thought I'd share anyway:

**Spreadsheets and Datasets:**

* [https://www.worldometers.info/coronavirus/](https://www.worldometers.info/coronavirus/)
* [John Hopkins University Github](https://github.com/CSSEGISandData/2019-nCoV) confirmed case numbers.
* [Google Sheets From DXY.cn](https://docs.google.com/spreadsheets/d/1jS24DjSPVWa4iuxuD4OAXrE3QeI8c9BC1hSlqr-NMiU/edit#gid=1187587451) (Contains some patient information \[age,gender,etc\] )
* [Kaggle Dataset](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset)
* [Strain Data](https://github.com/nextstrain/ncov) repo
* [https://covid2019.app/](https://covid2019.app/)  (Google Sheets, thanks /u/supertyler)
* [ECDC](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) (Daily Spreadsheets, Thanks /u/n3ongrau)

**Other Good sources:**

* [BNO](https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/) Seems to have latest number w/ sources. (scrape)
* [What we can find out on a Bioinformatics Level](https://innophore.com/2019-ncov/)
* [DXY.cn Chinese online community for Medical Professionals](https://ncov.dxy.cn/ncovh5/view/pneumonia) \*translate page.
* [John Hopkins University Live Map](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)
* [Mutations](https://nextstrain.org/ncov) (thanks /u/Mynewestaccount34578)
* [Protein Data Bank File](https://3dprint.nih.gov/discover/3DPX-012867)
* [Early Transmission Dynamics](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) Provides statistics on the early cases, median age, gender etc.

**\[IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* [*SARS-CoV-2*](https://en.wikipedia.org/wiki/2019_novel_coronavirus)*. Many datasets will show a spike on that date*.\]

**There have been a bunch of great comments with links to further resources below!**  
\[Last Edit: 15/03/2020\] ",411,183,Mars-Is-A-Tank,2020-02-02 12:51:57,https://www.reddit.com/r/datasets/comments/exnzrd/coronavirus_datasets/,1,datasets
7eo14t,Imagine having to pay extra to download or share datasets. Save net neutrality!,,378,9,hypd09,2017-11-22 04:03:59,https://www.battleforthenet.com/?utm_source=AN&utm_medium=email&utm_campaign=BFTNCallTool&utm_content=voteannouncement&ref=fftf_fftfan1120_30&link_id=0&can_id=185bf77ffd26b044bcbf9d7fadbab34e&email_referrer=email_265020&email_subject=net-neutrality-dies-in-one-month-unless-we-stop-it,0,datasets
9d8k9f,"Google releases Dataset Search: ""Similar to how Google Scholar works, Dataset Search lets you find datasets wherever they’re hosted""",,373,9,danwin,2018-09-05 17:03:57,https://www.blog.google/products/search/making-it-easier-discover-datasets/,0,datasets
excg1h,Congrats! Web scraping is legal! (US precedent)," Disputes about whether web scraping is legal have been going on for a long time. And now, a couple of months ago, the scandalous case of web scraping between hiQ v. LinkedIn was completed.

You can read about the progress of the case here: [US court fully legalized website scraping and technically prohibited it.](https://parsers.me/us-court-fully-legalized-website-scraping-and-technically-prohibited-it/)

Finally, the court concludes: ""Giving companies like LinkedIn the freedom to decide who can collect and use data – data that companies do not own, that is publicly available to everyone, and that these companies themselves collect and use – creates a risk of information monopolies that will violate the public interest”.",361,29,Gill_Chloet,2020-02-01 20:54:13,https://www.reddit.com/r/datasets/comments/excg1h/congrats_web_scraping_is_legal_us_precedent/,0,datasets
cjdxhz,Metadata for 2.6 million Pornhub videos spanning 320k playlists,"I scraped metadata for 2.6M pornhub videos based on the 320k most recently updated playlists as of mid-July 2019. In total, the data is 350MB in compressed json form, separated into playlist, video, and matching/cross-referencing files. They're directly downloadable from these links:

[https://datahub.io/racydata/final/r/dfp.json.gz](https://datahub.io/racydata/final/r/dfp.json.gz) (14MB)

[https://datahub.io/racydata/final/r/dfv.json.gz](https://datahub.io/racydata/final/r/dfv.json.gz) (120MB)

[https://datahub.io/racydata/final/r/dfm.json.gz](https://datahub.io/racydata/final/r/dfm.json.gz) (210MB)

&#x200B;

And here's an example jupyter notebook that uses the matching data (40 million pairs of (videoid,playlistid)) to make a sparse matrix with dimensions of (number of unique videos x number of playlists) and reduce the dimensionality with SVD. Then you can get ""recommendations"" for playlists/videos similar to a particular playlist/video based on the distance in this reduced dimensional space.

[https://gist.github.com/racydata/92ae85ea47da7c2d0bf50442bb0e83ea](https://gist.github.com/racydata/92ae85ea47da7c2d0bf50442bb0e83ea)

The notebook also shows what columns you can play with in those three files.",352,15,ImgurRouletteBot,2019-07-29 16:54:08,https://www.reddit.com/r/datasets/comments/cjdxhz/metadata_for_26_million_pornhub_videos_spanning/,0,datasets
oybg5f,"All Digitized Texas Appeals Court Cases Since 1900 - 12GB - 696,036 cases","[Kaggle dataset page](https://www.kaggle.com/judyrecords/all-digitized-texas-appeals-court-cases-since-1900?select=TxAppeals_Case_schema_mysql.sql)

Scope of Data

* All electronically-available Texas Appeals Court cases filed since 1900 (as of 2021-08-01).
* Courts included: Texas Supreme Court, Court of Criminal Appeals, 14 Appeals Courts (regional)
* Total cases: 696,036.
* Full dataset approx 12GB.
* Sample dataset approx 10MB.
* Download 500 sample rows or full dataset -- in either SQL or JSON format.
* Data source: [https://search.txcourts.gov/CaseSearch.aspx?coa=cossup](https://search.txcourts.gov/CaseSearch.aspx?coa=cossup)
* Aggregated by: [https://www.judyrecords.com](https://www.judyrecords.com/)
* Parsed fields include caseId, createdAt (Unix timestamp), siteCaseNum, courtKey, and httpReq.
* Detailed case information is not parsed within each case currently, but can be parsed from standardized HTML structure.
* Sample case from data source: [https://search.txcourts.gov/Case.aspx?cn=01-20-00103-CV&coa=coa01](https://search.txcourts.gov/Case.aspx?cn=01-20-00103-CV&coa=coa01)
* Court structure of Texas: [https://www.txcourts.gov/media/1452084/court-structure-chart-february-2021.pdf](https://www.txcourts.gov/media/1452084/court-structure-chart-february-2021.pdf)

Case Count by Court

|Court|Case Count|Court Key|
|:-|:-|:-|
|Texas Supreme Court|65,945|cossup|
|Texas Court of Criminal Appeals|242,915|coscca|
|Texas Court of Appeals #1|46,663|coa01|
|Texas Court of Appeals #2|34,458|coa02|
|Texas Court of Appeals #3|24,629|coa03|
|Texas Court of Appeals #4|33,469|coa04|
|Texas Court of Appeals #5|69,112|coa05|
|Texas Court of Appeals #6|12,206|coa06|
|Texas Court of Appeals #7|17,136|coa07|
|Texas Court of Appeals #8|16,180|coa08|
|Texas Court of Appeals #9|18,710|coa09|
|Texas Court of Appeals #10|14,550|coa10|
|Texas Court of Appeals #11|13,058|coa11|
|Texas Court of Appeals #12|14,366|coa12|
|Texas Court of Appeals #13|26,440|coa13|
|Texas Court of Appeals #14|46,199|coa14|

&#x200B;",313,4,aoeusnth48,2021-08-05 06:03:31,https://www.reddit.com/r/datasets/comments/oybg5f/all_digitized_texas_appeals_court_cases_since/,0,datasets
gyfbb0,Protests engaging 3.5% of a population rarely fail,,309,25,cavedave,2020-06-07 16:22:39,https://docs.google.com/spreadsheets/d/1ahA7I9uzEq5heyTUDsjR2OI_442P8_p83DmMUp4ItSc/edit#gid=1526202549,0,datasets
kzh789,"Since I didn't see anything else good in Kaggle, I scraped all of Trump's speeches(~3.4 Million characters) and put it all in a single txt file",,301,29,ARNisUsername,2021-01-17 22:57:54,https://www.kaggle.com/arnavsharmaas/all-donald-trump-transcripts,0,datasets
n1z013,11 TB dataset of drone imagery with annotations for small object detection and tracking,,292,9,haseen-sapne,2021-04-30 17:35:12,https://i.redd.it/ey8sn2dejcw61.gif,0,datasets
blsail,Google has a beta version of a DATABASE SEARCH ENGINE!,"  

Its meant to be a companion to Google Scholar and it is so great! Wonderful idea and it is great to see one easy place to go to and search for datasets on a variety of topics. Happy researching!

[https://toolbox.google.com/datasetsearch](https://toolbox.google.com/datasetsearch)

Edit: My first gold!?!?!! Thank you fellow beautiful nerds ❤️",291,14,its_a_liv,2019-05-07 15:48:02,https://www.reddit.com/r/datasets/comments/blsail/google_has_a_beta_version_of_a_database_search/,1,datasets
11n6ro1,"Comprehensive NBA Basketball SQLite Database on Kaggle Now Updated — Across 16 tables, includes 30 teams, 4800+ players, 60,000+ games (every game since the inaugural 1946-47 NBA season), Box Scores for over 95% of all games, 13M+ rows of Play-by-Play data, and CSV Table Dumps — Updates Daily 👍",,281,18,onelonedatum,2023-03-09 22:56:46,https://www.kaggle.com/datasets/wyattowalsh/basketball,0,datasets
l12fpy,All Trump's twitter insults from 2015 to 2021 in CSV.,"[https://gofile.io/d/YQ2g4j](https://gofile.io/d/YQ2g4j) (1.8MB)

Attrs: \[""id"",""date"",""tweet"", ""insult\_x"",""target\_x"",..\]

Extracted from the NYT story: [https://www.nytimes.com/interactive/2021/01/19/upshot/trump-complete-insult-list.html](https://www.nytimes.com/interactive/2021/01/19/upshot/trump-complete-insult-list.html)

**UPDATE**

* Dataset is now one unique **tweet** per row.
* **insult\_x** and **target\_x** exploded into separate columns (sparse matrix).
* Ordered by **date** ASC.
* Other minor issues fixed (random newlines, other dupes).
* Internal quotes are """"double quoted"""".",273,18,None,2021-01-20 05:14:39,https://www.reddit.com/r/datasets/comments/l12fpy/all_trumps_twitter_insults_from_2015_to_2021_in/,0,datasets
fg4w8h,South Korea releases 7382 COVID-19 case details in GitHub repository,"[https://github.com/jihoo-kim/Coronavirus-Dataset/](https://github.com/jihoo-kim/Coronavirus-Dataset/)  


If you want those merged in the same schema with Singapore and Hong Kong, we did that on DoltHub:

[https://www.dolthub.com/repositories/Liquidata/corona-virus/data/master/case\_details](https://www.dolthub.com/repositories/Liquidata/corona-virus/data/master/case_details)

That has 7658 cases currently tracked. Dolt data sync with upstreams hourly.",277,18,timsehn,2020-03-10 00:20:23,https://www.reddit.com/r/datasets/comments/fg4w8h/south_korea_releases_7382_covid19_case_details_in/,0,datasets
icty0r,We made 40k+ open government datasets queryable through a public PostgreSQL endpoint,,273,13,mildbyte,2020-08-19 18:59:49,https://www.splitgraph.com/connect,0,datasets
ll1l5o,fivethirtyeight released a whole load of datasets that they use for their own analytics,,260,5,clausy,2021-02-16 11:04:36,https://data.fivethirtyeight.com,0,datasets
i50xcd,is there Line graphs comparing these to data sets over the past 20 to 30 years?,,254,22,dachuck4639,2020-08-06 21:42:40,https://i.imgur.com/xSkcsxp.jpg,0,datasets
ml09ma,"New NBA dataset on Kaggle! - Every game 60,000+ (1946-2021) w/ box scores, line scores, series info, and more - every player 4500+ w/ draft data, career stats, biometrics, and more - and every team 30 w/ franchise histories, coaches/staffing, and more. Updated daily, with plans for expansion!",,246,36,onelonedatum,2021-04-06 01:30:24,https://www.kaggle.com/wyattowalsh/basketball,0,datasets
d28nzb,"Web scraping doesn’t violate anti-hacking law, appeals court rules","Of possible interest.

Scraping a public website without the approval of the website's owner isn't a violation of the Computer Fraud and Abuse Act, an appeals court ruled on Monday. The ruling comes in a legal battle that pits Microsoft-owned LinkedIn against a small data-analytics company called hiQ Labs.

[https://arstechnica.com/tech-policy/2019/09/web-scraping-doesnt-violate-anti-hacking-law-appeals-court-rules/](https://arstechnica.com/tech-policy/2019/09/web-scraping-doesnt-violate-anti-hacking-law-appeals-court-rules/)",245,26,ppival,2019-09-10 14:56:12,https://www.reddit.com/r/datasets/comments/d28nzb/web_scraping_doesnt_violate_antihacking_law/,0,datasets
i60egd,Looking for Image Dataset of People Wearing Covid-19 Masks Badly (with Nostrils Visible),,240,38,extravert_af,2020-08-08 14:53:42,https://i.redd.it/pz282a7alsf51.png,0,datasets
frxy6y,"Please Don't Make Up ""Synthetic"" Datasets and Share Unless EXPLICITLY Labeled as Such","Earlier today, there was a post here about a new dataset on Kaggle:

[https://www.reddit.com/r/datasets/comments/frjk5o/churn\_analysis/](https://www.reddit.com/r/datasets/comments/frjk5o/churn_analysis/)

**TLDR;** *I wasted a ton of time on something because a member of this community was fishing for upvotes (and did a very poor job creating a dataset deserving of analysis).*

The dataset was not ""useful"" yet it had 20+ upvotes, solicited by the OP who said, ""Please upvote if it's 'useful.'""

The data set is ""synthetic."" It was generated by the user, but this WAS NOT STATED. Also, the data is not even a realistic sample. I wasted time looking at it before I knew this. I wasted much time writing a response on Kaggle, inquiring about the median values of customer life, and explaining that I have done churn studies and telecom customer attrition studies previously, and in my eyes the data seemed to be a sample that was not representative, etc., etc.

This is the first time I've wasted time on something like this. I will be very careful to make sure it's the last time. Ironically, I also got locked out of Kaggle as a result of my participation. After posting a lengthy discussion response (not yet knowing the data was synthetic), Kaggle/Google made me answer a data science question, like a captcha, and/or respond as to why I thought I might have tripped off their spam-sensor algo. Great bastion of quality that Google is so often \*not\*, the challenge question did not work, and I am locked out of Kaggle.

I feel kind of stupid for putting myself in this situation, but I feel equally angry about the original post.

You know, the first thing I did was get a row count and it was 3,333, and I said, ""That's kind of funny."" I should have stopped right then and there. Sorry, rant over.  : - )",235,16,oldMuso,2020-03-30 19:56:18,https://www.reddit.com/r/datasets/comments/frxy6y/please_dont_make_up_synthetic_datasets_and_share/,0,datasets
hzkbf6,"I made a little tool to download high-resolution weather data from Wunderground. Wunderground hosts high-resolution (mostly 5 minute interval) measurements from thousands of weather stations. With my scraper, you can download that data in CSV format.",,230,19,Karlpy,2020-07-28 18:43:09,https://github.com/Karlheinzniebuhr/the-weather-scraper,0,datasets
et9vgn,Google Dataset search out of beta: Discovering millions of datasets on the web,,207,5,dfhsr,2020-01-24 12:39:01,https://blog.google/products/search/discovering-millions-datasets-web/,0,datasets
chb3uo,"Wow 😲 , Lyft just open sourced its autonomous driving dataset from its Level 5 self-driving fleet!",,203,3,kyuorree0suhtee,2019-07-24 17:21:20,https://www.reddit.com/r/datascience/comments/cgwvds/wow_lyft_just_open_sourced_its_autonomous_driving/,0,datasets
fi5uys,A free API for data on the Corona Virus,"Hi Reddit! 

I wanted to find a good API for COVID19 data but the ones I came across seemed less than ideal. I hacked this together over a few hours and will be extending the routes as time goes on. Data is pulled from the Johns Hopkins CSSE github repo and will update daily.

The idea is for people to be able to use this to build graphs, mobile apps, etc.

Hope it's helpful!

[https://covid19api.com](https://covid19api.com)",197,28,ksred,2020-03-13 20:05:21,https://www.reddit.com/r/datasets/comments/fi5uys/a_free_api_for_data_on_the_corona_virus/,0,datasets
hsa8j5,CDC covid data now not available to public,,190,24,cavedave,2020-07-16 14:05:55,https://twitter.com/charlesornstein/status/1283754463086481410,0,datasets
f9ftfm,Dataset containing The Onion headlines and r/NotTheOnion headlines intended as a fun and perhaps tough classification problem.,"The dataset contains 9,000 Onion headlines labeled as 1 and 15,000 r/NotTheOnion headlines labeled as 0 in the OnionOrNot.csv file. There is also a jupyter notebook showing how I extract the headlines from the Pushshift API and train some different simple neural networks to classify the headlines, achieving about 87% validation accuracy. Probably wouldn't take much to outperform me, let me know if you do!

[https://github.com/lukefeilberg/onion](https://github.com/lukefeilberg/onion)",191,9,333luke,2020-02-25 19:40:35,https://www.reddit.com/r/datasets/comments/f9ftfm/dataset_containing_the_onion_headlines_and/,0,datasets
kwi3yl,All geotagged metadata from the Parler dump as a .csv file with timestamps and video durations,,189,37,acanthias13,2021-01-13 15:01:37,https://gofile.io/d/PUxeV4,0,datasets
kyqbxn,"The CIA Has Declassified 2,780 Pages of UFO-Related Documents, and They’re Now Free to Download",,187,30,cavedave,2021-01-16 20:10:07,https://www.theblackvault.com/documentarchive/ufos-the-central-intelligence-agency-cia-collection/#prettyPhoto,1,datasets
gxsdqk,"Archive of 43,475 Donald Trump Twitter screenshots from 2009 to 2020",,189,10,soheilpro,2020-06-06 14:53:16,https://pikaso.me/blog/trump-twitter-archive,0,datasets
b4yy6p,"480,000 Rotten Tomato Critic Reviews","I scraped nearly all Rotten Tomatoes critic reviews. I kept 240,000 fresh, and 240,000 rotten reviews. They are labeled. Column 1 = `Freshness` [fresh, rotten], column 2 = `Text` of review. 

Available on my [Google Drive](https://drive.google.com/file/d/1N8WCMci_jpDHwCVgSED-B9yts-q9_Bb5/view?usp=sharing). Here is [the repo](https://github.com/nicolas-gervais/rotten-tomatoes-dataset), it is maintained as of November 2019.",189,16,nicolas-gervais,2019-03-24 17:30:51,https://www.reddit.com/r/datasets/comments/b4yy6p/480000_rotten_tomato_critic_reviews/,0,datasets
ho0gec,"[Self promotion] A while ago, we struggled to find accurate FREE datasets to analyze. I will now share them with you so you can spend 20% of your time finding the needed data and 80% on analyzing and finding insights.","&#x200B;

In 2020, it’s estimated that the digital sphere consists of 44 zettabytes of data, so there’s certainly no shortage of free and interesting data.

There are plenty of repositories curating data sets to suit all your needs, and many of these sites also filter out the not-so-great ones, meaning you don’t have to waste time downloading useless CSV files. 

If you want to learn how to analyze data, improve your data literacy skills, or learn how to create data visualizations, readily available data sets are a great palace to start.

In this blog post, we’ll take a look at some of our favorite places to find free data sets, so you can spend less time searching and more time uncovering insights.

&#x200B;

* ***Fivethirtyeight***

***Link -*** [***https://data.fivethirtyeight.com***](https://data.fivethirtyeight.com)

FiveThirtyEight is an independent collection of data on US politics, US sport and other general interest datasets. It specializes in the collation and ranking of reliable political and opinion polls. We’ve used them in a number of projects, finding out some interesting things along the way, like when Donald Trump is most active on Twitter ([Sign up to VAYU](https://vayu.gyana.co.uk/signup) for free to view the template).

&#x200B;

* ***Google Trends***

***Link -*** [***https://trends.google.com/trends/***](https://trends.google.com/trends/)

Google provides readily accessible data sets on search trends, and you can customize the parameters to easily find whatever it is you’re interested in. We recommend exporting the dataset and running it through [VAYU](https://www.gyana.co.uk/) for one-click visualizations and advanced analysis.

&#x200B;

* ***ProPublica Data Store***

***Link -*** [***https://www.propublica.org/datastore/***](https://www.propublica.org/datastore/)

ProPublica, probably best known for their award-winning investigative journalism, collects data pertaining to the US economy, finance, health, industry, politics and more. They have both free and premium datasets, should you need to delve deeper into whatever it is you’re exploring.

&#x200B;

* ***Centers for Disease and Control Prevention***

***Link -*** [***https://www.cdc.gov/datastatistics/index.html***](https://www.cdc.gov/datastatistics/index.html)

The CDC collects the abundance of health data provided by US government research and sources, including data and research on alcohol, life expectancy, obesity and chronic diseases. This is a great resource for analyzing and understanding public health.

&#x200B;

***Please feel free to check*** [***this link***](https://www.gyana.co.uk/post/6-great-websites-with-free-data-sets/) ***for the rest of them, we also do recommend running them through Vayu to find and share interesting insights.***",181,6,catalin_bis,2020-07-09 10:00:16,https://www.reddit.com/r/datasets/comments/ho0gec/self_promotion_a_while_ago_we_struggled_to_find/,0,datasets
8s0wrr,"Dataset of 3500+ ads by Pro-Russia group, preprocessed (.pdf -> .png, .txt, .json)","Hey r/datasets. Last month, Democrats in the United States House Intelligence Committee released 3500+ Facebook and Instagram ads purchased by the Internet Research Agency, a Russian group that purportedly tries to influence Americans political views. Congress originally released the dataset as a set of 3500+ PDF files with text and images. The PDF files weren't very structured, so it was hard to analyze them without some preprocessing. As part of a separate project, I've done some of that preprocessing, and wanted to share!

To the extent possible, I've extracted and cropped the ad images from those PDFs into .png files, and used some basic OCR to extract the text into .txt files. I've also created a JSON file that stores some information about these ads, such as ad cost (converted from Rubles to $USD), creation dates, interest groups targeted, audience locations targeted, and more. All of this data is available on Github if you're interested in taking a look: [https://github.com/russian-ad-explorer/russian-ad-datasets](https://github.com/russian-ad-explorer/russian-ad-datasets).

I also made a data explorer for the ads as a project, which you can check out here: [https://russian-ad-explorer.github.io/](https://russian-ad-explorer.github.io/). It has some ad categorizations for section of the US targeted (e.g. ""Pacific"", ""Midwest"") and for audience groups targeted (e.g. ""African American"", ""Incarcerated"") that weren't available in the original datasets. Some csv files with those categorizations are on the dataset Github repo too.

(The original dataset as posted by the US Congress was posted on r/datasets a while ago [https://www.reddit.com/r/datasets/comments/8igag4/dataset\_of\_3519\_ads\_that\_prorussia\_group\_the/](https://www.reddit.com/r/datasets/comments/8igag4/dataset_of_3519_ads_that_prorussia_group_the/))",180,6,beeeeeeers,2018-06-18 16:34:59,https://www.reddit.com/r/datasets/comments/8s0wrr/dataset_of_3500_ads_by_prorussia_group/,1,datasets
sa9vuu,"Goodreads book reviews dataset - 10 million books, 6 million reviews","Just thought I'd share this [Goodreads dataset here](https://github.com/zygmuntz/goodbooks-10k). It took me quite a lot of internet sleuthing to find an interesting, complete and large dataset to practice machine learning and more specifically recommender systems.

This data was originally pulled from Goodreads in 2017 by Zygmunt Zając . It contains detailed metadata information for **10 000 books** (sorry about the typo in the title), as well as 6 million individual numerical ratings collected from 53 000 users. There is no demographic information available for users, but the different files included in the release form an interesting basis for a recommender system.

I have released an expansion pack of sorts for this dataset, that adds book descriptions, genres and other features, enabling the use of various NLP strategies. **[See here for the augmented dataset.](https://github.com/malcolmosh/goodbooks-10k-extended/blob/master/README.md)** Cheers.",179,8,malcolm_osh,2022-01-22 19:21:29,https://www.reddit.com/r/datasets/comments/sa9vuu/goodreads_book_reviews_dataset_10_million_books_6/,0,datasets
fcyhx7,Johns Hopkins Covid-19 Github Data Repo,,181,4,QuirkySpiceBush,2020-03-03 17:58:16,https://github.com/CSSEGISandData/COVID-19,0,datasets
c58s1j,"Massive database of over 10,000 chord progressions from classical and popular songs - from Mozart to Deadmau5 to Bohemian Rhapsody to the Five Nights at Freddy's song.",,177,3,ppsh_2016,2019-06-25 13:45:03,https://www.hooktheory.com/theorytab/charts/chart/top,0,datasets
ekwgv0,"The Names of 1.8 Million Emancipated Slaves Are Now Searchable in the World’s Largest Genealogical Database,",,173,8,cavedave,2020-01-06 16:35:57,http://www.discoverfreedmen.org/,0,datasets
m5q049,I think I accidentally started a movement - Policing the Police by scraping court data - *An Update*,,177,6,transtwin,2021-03-15 17:30:42,/r/privacy/comments/m59o2g/i_think_i_accidentally_started_a_movement/,0,datasets
3rih8y,"I have listed every publicly available open data portals around the world. The list gathers ~1600 portals, in 200 countries.","Working for a SaaS company in need of loads of structured data, I've started to compile a list of all open data portals around the world as my own go-to resource. 

After taking my colleague [Nicolas](https://twitter.com/NTerpo) on the project, we ended up with a list of more than 1600 portals. We gathered our own listings, scrapped third-party datasets, cleaned the whole thing (elbow grease, Clojure) and created a list (w/ Ruby).

Instead of keeping it in a dusty corner of my computer, I thought I'll share it with the open data community / data geeks.

This is a work in progress, I'll work on enriching the data available, add new portals...  

I hope this'll help. Thank you all! 

[The list is available here.](https://www.opendatasoft.com/a-comprehensive-list-of-all-open-data-portals-around-the-world/?utm_source=general&utm_medium=social&utm_campaign=opendatainception)

The whole process is explained [here.](https://www.opendatasoft.com/2015/11/02/how-we-put-together-a-list-of-1600-open-data-portals-around-the-world-to-help-open-data-community/)

**[UPDATE 05/11/2015] Thank you so much for all your feedback! We have used the dataset generated to create a website called [opendatainception.io](http://opendatainception.io/) where you can now browse data on a map.** 

Still much work to do to enrich/edit... but we'll get there. You can browse data by navigating or through the search box. When typing a query there, the data will automatically refine on the map. 

 **[UPDATE 02/12/2015] Hey guys! We have had a tremendous amount of feedback during the first two weeks. We worked hard to clean the list to a near perfection. :)**

Now, you can enjoy a list with no dead URLs (I've checked them myself, one by one, yup!), with more precise coordinates, and more portals. 

Also, at first we were building the list as an HTML list from the dataset with some Ruby script. It was a kinda pain and not always super reliable. To be more efficient and reflect the changes instantly as we were making them, we went for some open source widgets instead (built w/ angular). 

Now, the page displays a dynamic list, always synced up with the dataset. You still can look for countries and stuff.

Hope that'll help! 

Thanks again for your feedback! 


",173,28,Remozito,2015-11-04 17:05:07,https://www.reddit.com/r/datasets/comments/3rih8y/i_have_listed_every_publicly_available_open_data/,0,datasets
x7eeid,Health insurance companies may have just dumped a trillion prices onto the internet,,172,10,alecs-dolt,2022-09-06 16:06:27,https://www.dolthub.com/blog/2022-09-02-a-trillion-prices/,0,datasets
fjs48a,COVID-19 Open Research Dataset Challenge (CORD-19),"# [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)

In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.

# Call to Action

We are issuing a call to action to the world's artificial intelligence experts to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions. The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date. This allows the worldwide AI research community the opportunity to apply text and data mining approaches to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide. There is a growing urgency for these approaches because of the rapid increase in coronavirus literature, making it difficult for the medical community to keep up.

A list of our initial key questions can be found under the Tasks section of this dataset. These key scientific questions are drawn from the NASEM’s SCIED (National Academies of Sciences, Engineering, and Medicine’s Standing Committee on Emerging Infectious Diseases and 21st Century Health Threats) [research topics](https://www.nationalacademies.org/event/03-11-2020/standing-committee-on-emerging-infectious-diseases-and-21st-century-health-threats-virtual-meeting-1) and the World Health Organization’s [R&D Blueprint](https://www.who.int/blueprint/priority-diseases/key-action/Global_Research_Forum_FINAL_VERSION_for_web_14_feb_2020.pdf?ua=1) for COVID-19.

Many of these questions are suitable for text mining, and we encourage researchers to develop text mining tools to provide insights on these questions.

# Prizes

Kaggle is sponsoring a $1,000 per task award to the winner whose submission is identified as best meeting the evaluation criteria. The winner may elect to receive this award as a charitable donation to COVID-19 relief/research efforts or as a monetary payment. More details on the prizes and timeline can be found on the discussion post.

# Resources and News Coverage

[SemanticScholar's COVID-19 Open Research Dataset (CORD-19) Page](https://pages.semanticscholar.org/coronavirus-research)

[WhiteHouse.gov Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset](https://www.whitehouse.gov/briefings-statements/call-action-tech-community-new-machine-readable-covid-19-dataset/)

[Venture Beat: Microsoft, White House, and Allen Institute release coronavirus data set for medical and NLP researchers](https://venturebeat.com/2020/03/16/microsoft-white-house-and-allen-institute-release-coronavirus-data-set-for-medical-and-nlp-researchers/)

[ZDNET: White House leads effort to publish COVID-19 open research data set](https://www.zdnet.com/article/white-house-leads-effort-to-publish-covid-19-open-research-data-set/)

[GeekWire: AI2 and Microsoft join the White House’s push to enlist AI for the war on coronavirus](https://www.geekwire.com/2020/ai2-microsoft-team-tech-leaders-use-ai-war-coronavirus/)

[TechCrunch: With launch of COVID-19 data hub, the White House issues a ‘call to action’ for AI researchers](https://techcrunch.com/2020/03/16/coronavirus-machine-learning-cord-19-chan-zuckerberg-ostp/)",168,14,shrine,2020-03-16 20:48:54,https://www.reddit.com/r/datasets/comments/fjs48a/covid19_open_research_dataset_challenge_cord19/,0,datasets
emvlyz,"Why is a 22GB database containing 56 million US folks' personal details sitting on the open internet using a Chinese IP address? Seriously, why? [Did anyone find a copy of this? Seems legal and convenient]",,168,8,Deleetdk,2020-01-10 19:10:38,https://www.theregister.co.uk/2020/01/09/checkpeoplecom_data_exposed/,0,datasets
ht41ep,I need apple images dataset to train a model to distinguish between the good apple and the bad apple.,,167,18,aymenSekhri,2020-07-17 21:13:00,https://i.redd.it/1p8szuazghb51.jpg,0,datasets
jr32vb,Debunking an election fraud claim using open data from Pennsylvania,,167,43,zachm,2020-11-09 18:24:11,https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/,0,datasets
m0hr81,"We are digitisers at the Natural History Museum in London, on a mission to digitise 80 million specimens and free their data to the world. Ask us anything!","**We’ll be live 4-6PM UTC!**

**Thanks for a great AMA! We're logging off now, but keep the questions coming as we will check back and answer the most popular ones tomorrow :)**

The Natural History Museum in London has 80 million items (and counting!) in its collections, from the tiniest specks of stardust to the largest animal that ever lived – the blue whale. 

The [Digital Collections Programme](https://www.nhm.ac.uk/our-science/our-work/digital-collections.html) is a project to digitise these specimens and give the global scientific community access to unrivalled historical, geographic and taxonomic specimen data gathered in the last 250 years. Mobilising this data can facilitate research into some of the most pressing scientific and societal challenges.

Digitising involves creating a digital record of a specimen which can consist of all types of information such as images, and geographical and historical information about where and when a specimen was collected. The possibilities for digitisation are quite literally limitless – as technology evolves, so do possible uses and analyses of the collections. We are currently exploring how machine learning and automation can help us capture information from specimen images and their labels.

With such a wide variety of specimens, digitising looks different for every single collection. How we digitise a fly specimen on a microscope slide is very different to how we might digitise a bat in a spirit jar! We develop new workflows in response to the type of specimens we are dealing with. Sometimes we have to get really creative, and have even published on workflows which have involved using pieces of LEGO to hold specimens in place while we are imaging them.

Mobilising this data and making it open access is at the heart of the project. All of the specimen data is released on our [Data Portal](https://data.nhm.ac.uk/), and we also feed the data into international databases such as [GBIF.](https://www.gbif.org/)

Our team for this AMA includes:

* **Lizzy Devenish** *–* *s*enior digitiser currently planning digitisation workflows for collections involved in the Museum's newly announced Science and Digitisation Centre at Harwell Science Campus. Personally interested in fossils, skulls, and skeletons!
* **Peter Wing** – digitiser interested in entomological specimens (particularly Diptera and Lepidoptera). Currently working on a project to provide digital surrogate loans to scientists and a new workflow for imaging carpological specimens
* **Helen Hardy** – programme manager who oversees digitisation strategy and works with other collections internationally
* **Krisztina Lohonya** – digitiser with a particularly interest in Herbaria. Currently working on a project to digitise some stonefly and Legume specimens in the collection
* **Laurence Livermore** – innovation manager who oversees the digitisation team and does research on software-based automation. Interested in insects, open data and Wikipedia
* **Josh Humphries** – Data Portal technical lead, primarily working on maintaining and improving our Data Portal
* **Ginger Butcher** – software engineer primarily focused on maintaining and improving the Data Portal, but also working on various data processing and machine learning projects

Proof: [https://twitter.com/NHM\_Digitise/status/1368943500188774400](https://twitter.com/NHM_Digitise/status/1368943500188774400)

Edit: Added link to proof :)",161,38,NHM_Digitise,2021-03-08 15:13:41,https://www.reddit.com/r/datasets/comments/m0hr81/we_are_digitisers_at_the_natural_history_museum/,0,datasets
b6rcwv,"I scraped 32,000 cars, including the price and 115 specifications","Edit: NEW cars

Specs include MSRP, Gas Mileage, Engine, EPA Class, Style Name, Drivetrain, SAE Net Torque @ RPM, Fuel System, Engine Type, SAE Net Horsepower @ RPM, Displacement, etc. The cars are all cars available on the American market.

Get the CSV (44MB) on my [drive](https://drive.google.com/file/d/1HnpfG2xj_6EZ7Tgle2QB9Yn_YS7r7uVA/view?usp=sharing), or look how I did it on [Github](https://github.com/nicolas-gervais/predicting-car-price-from-scraped-data/blob/master/scraping).",168,21,nicolas-gervais,2019-03-29 01:37:44,https://www.reddit.com/r/datasets/comments/b6rcwv/i_scraped_32000_cars_including_the_price_and_115/,0,datasets
fswg9f,"The Alexa rankings are rather bananas right now, CDC.gov has climbed above pornhub, zillow and craigslist for the US rankings. The other stuff is somewhat static, but Reddit has fallen to #6 from it's typical position at #5 - maybe because less people are browsing at the office?",,160,8,NMJ87,2020-04-01 09:21:56,https://www.alexa.com/topsites/countries/US,0,datasets
d7d32w,"100,000 free AI-generated headshots put stock photo companies on notice",,162,14,cavedave,2019-09-21 17:00:24,https://www.theverge.com/2019/9/20/20875362/100000-fake-ai-photos-stock-photography-royalty-free,0,datasets
akb4mr,What is a dataset that you can’t believe is available to the public?,,156,30,cookie_jarmaican,2019-01-27 10:59:04,https://www.reddit.com/r/datasets/comments/akb4mr/what_is_a_dataset_that_you_cant_believe_is/,0,datasets
rsujun,Dislikes and other metadata for 4.56 Billion YouTube videos crawled by Archive Team in flat file and JSON format (torrent),,158,5,jopik1,2021-12-31 14:02:41,https://old.reddit.com/r/DataHoarder/comments/rsu7lf/dislikes_and_other_metadata_for_456_billion/,0,datasets
cj3ipd,"Jeopardy dataset with 349,000 clues","Current version: [Github](https://github.com/jwolle1/jeopardy_clue_dataset)

No longer updated: [Google Drive](https://drive.google.com/drive/folders/1fxY181PdiA1KoJRG23ZVLC2Y5CIRqQWx)

___

edit - For the most current version use the Github link. More information there.",156,9,jwolle1,2019-07-29 00:03:25,https://www.reddit.com/r/datasets/comments/cj3ipd/jeopardy_dataset_with_349000_clues/,0,datasets
11yyoth,4682 episodes of The Alex Jones Show (15875 hours) transcribed [self-promotion?],"I've spent a few months running [OpenAI Whisper](https://github.com/openai/whisper) on the available episodes of The Alex Jones show, and was pointed to this subreddit by u/UglyChihuahua. I used the medium English model, as that's all I had GPU memory for, but used [Whisper.cpp](https://github.com/ggerganov/whisper.cpp) and the large model when the medium model got confused. 

It's about 1.2GB of text with timestamps. 

I've added all the transcripts to a [github repository](https://github.com/Fudge/infowars), and also created a simple [web site](http://fight.fudgie.org) with search, simple stats, and links into the relevant audio clip.",153,66,fudgie,2023-03-22 22:13:02,https://www.reddit.com/r/datasets/comments/11yyoth/4682_episodes_of_the_alex_jones_show_15875_hours/,2,datasets
l520kn,"I built a JSON'ified collection of Star Trek TNG, VOY, and DS9 episode transcripts with speaker, location, and some episode metadata, including a JSON schema",,148,9,CharlesStross,2021-01-26 00:40:54,https://github.com/jkingsman/Star-Trek-Script-Programmatics/tree/master/processed,0,datasets
ez7k6e,50+ free Datasets for Data Science Projects - Journey of Analytics,,156,12,cavedave,2020-02-05 10:23:19,https://blog.journeyofanalytics.com/50-free-datasets-for-data-science-projects/,0,datasets
d18qpl,Google open-sources datasets for AI assistants with human-level understanding,,150,3,cavedave,2019-09-08 09:16:23,https://venturebeat.com/2019/09/06/google-open-sources-datasets-for-ai-assistants-with-human-level-understanding/,0,datasets
fwiede,South Korea released medical history for all COVID-19 patients based on their insurance claims for the past five years.,,151,22,PHealthy,2020-04-07 10:47:11,https://hira-covid19.net/,0,datasets
cpv8hb,40 Million Pages of U.S. Case Law from Harvard Law School Library,"I'm writing to share a dataset from the Harvard Library Innovation Lab! 

In October we shared the Caselaw Access Project ([https://case.law/](https://case.law/)) API and Bulk Data Service, making 40 million pages of case law freely available online in machine readable format, digitized from the collections of the Harvard Law School Library.

We want to see what themes people can pull out of the data! Here are some ways to get started ([case.law/tools/](https://case.law/tools/)), including:

* CAP API: [https://case.law/api/](https://case.law/api/)
* Bulk Data Service: [https://case.law/bulk/](https://case.law/bulk/)
* Historical Trends (visualize how words are used in U.S. case law over time!): [https://case.law/trends/](https://case.law/trends/)

We're excited to see what people find and build with this data set. Share what you find with us at: [https://case.law/contact/](https://case.law/contact/)

Thanks!",150,12,caselawaccess,2019-08-13 16:07:48,https://www.reddit.com/r/datasets/comments/cpv8hb/40_million_pages_of_us_case_law_from_harvard_law/,0,datasets
l25u1e,"Voterfraud2020, a public Twitter dataset with 7.6M tweets and 25.6M retweets related to voter fraud claims, including aggregate data of every link and YouTube video, and account suspension status.",,148,2,smurfyjenkins,2021-01-21 19:35:01,https://twitter.com/informor/status/1352289036854046725,0,datasets
hc2i37,We're building a labeling platform for image segmentation. Looking for feedback!,,144,19,segments-bert,2020-06-19 15:16:37,https://i.redd.it/7dqoz81mvv551.png,0,datasets
al1dbf,"How we made 17,000 police officers’ records into a searchable public database, and what you can learn from our saga",,142,1,danwin,2019-01-29 15:56:31,https://source.opennews.org/articles/how-we-made-force-report-database/,0,datasets
7dllgr,Paradise Papers database is now online,,143,0,None,2017-11-17 15:15:52,https://offshoreleaks.icij.org/pages/database,0,datasets
hycdur,"Civilian Complaints Against New York City Police Officers [33k rows, 12k complaint reports)",,144,1,danwin,2020-07-26 18:52:12,https://www.propublica.org/datastore/dataset/civilian-complaints-against-new-york-city-police-officers,0,datasets
9ih486,600 GB corpus of all paywalled scholarly sources of Wikipedia,,141,0,None,2018-09-24 11:27:54,https://www.reddit.com/r/DataHoarder/comments/9ifnda/600_gb_corpus_of_all_paywalled_scholarly_sources/,0,datasets
g6d1cr,We've updated our database... malicious online activity related to Covid-19,"Shared this data last week and got some really great feedback. We've now got a partnership with a new WHOIS provider allowing us to paint an incredibly detailed picture of malicious online activity throughout the pandemic.    


I'm certain more can be done with the data we've pulled together. Please download it, play with it, let me know if you have any thoughts.

 [https://github.com/ProPrivacy/covid-19](https://github.com/ProPrivacy/covid-19) 

 [https://proprivacy.com/tools/scam-website-checker](https://proprivacy.com/tools/scam-website-checker) 

[https://public.tableau.com/views/TrackingonlinemaliciousactivityrelatedtoCoronavirus/TrackingonlinemaliciousactivityrelatedtoCoronavirusCOVID-19?:display\_count=y&publish=yes&:origin=viz\_share\_link](https://public.tableau.com/views/TrackingonlinemaliciousactivityrelatedtoCoronavirus/TrackingonlinemaliciousactivityrelatedtoCoronavirusCOVID-19?:display_count=y&publish=yes&:origin=viz_share_link)",143,15,papa_privacy,2020-04-23 00:39:12,https://www.reddit.com/r/datasets/comments/g6d1cr/weve_updated_our_database_malicious_online/,0,datasets
fq298s,NYTimes COVID-19 Dataset,,138,9,QuirkySpiceBush,2020-03-27 18:15:41,https://github.com/nytimes/covid-19-data,0,datasets
etrfn4,New Google search for datasets,,140,8,imanexpertama,2020-01-25 14:32:04,https://towardsdatascience.com/google-just-published-25-million-free-datasets-d83940e24284,0,datasets
10xidpp,"500,000 Tweets sampled from the Twitter API before API access was shut down",,140,13,robert_ritz,2023-02-09 02:11:11,https://deepnote.com/workspace/datafantic-3bd1a992-4cfb-4c56-aaaf-931ce087ce8c/project/2022-12-12-Bootstrap-a-labeled-dataset-with-a-large-language-model-7e0a65cb-31c9-404a-80c8-c48d28054cc0/notebook/01%20-%20Download%20Tweets-2f91d2feb49f428093af398356e5e750,0,datasets
8drldg,In 1937 100k Irish children were encouraged to seek out the oldest person they knew and gather their stories. This has been compiled into an archive searchable by any topic ranging from the supernatural to natural remedies,,134,0,cavedave,2018-04-20 22:30:09,https://www.irishtimes.com/life-and-style/people/ireland-s-darkest-oddest-and-weirdest-secrets-uncovered-1.3418059?mode=amp,0,datasets
jc4qrg,Beach litter mapped. All picked up and the data is open 💚🌎,,137,18,littercoin,2020-10-16 06:23:37,https://i.redd.it/ca3qk8c8sat51.jpg,0,datasets
hpi95f,Data for small business loans issued for Covid relief (> $150k) was published to public,,133,6,idiosync_reddit,2020-07-11 21:35:33,https://home.treasury.gov/policy-issues/cares-act/assistance-for-small-businesses/sba-paycheck-protection-program-loan-level-data,0,datasets
jk6yas,Facebook release CommonCrawl dataset of 2.5TB of clean unsupervised text from 100 languages,"Data [http://data.statmt.org/cc-100/](http://data.statmt.org/cc-100/)

scripts [https://github.com/facebookresearch/cc\_net#extract-xlm-r-data](https://github.com/facebookresearch/cc_net#extract-xlm-r-data)

Twitter where I found it [https://twitter.com/alex\_conneau/status/1321507120848625665](https://twitter.com/alex_conneau/status/1321507120848625665)",137,1,cavedave,2020-10-29 09:32:34,https://www.reddit.com/r/datasets/comments/jk6yas/facebook_release_commoncrawl_dataset_of_25tb_of/,0,datasets
if9wgx,"Over 200 000 000 unique coronavirus related tweets, n-gram counts, hashtag counts, and tweet ids by date, collected since January 17th","Hi all, I've been collecting covid-19 related tweets since mid January and have amassed over 200 000 000 unique tweets. Various statistic such as n-gram counts (uni, bi and tri) and hashtag counts have been processed and published in my GitHub repo for anyone to analyze: https://github.com/delvinso/covid19_one_hundred_million_unique_tweets.

If you have any questions and/or comments and criticisms on how the dataset can be maintained going forward, please let me know.",135,7,turnip_cakes,2020-08-23 19:51:25,https://www.reddit.com/r/datasets/comments/if9wgx/over_200_000_000_unique_coronavirus_related/,0,datasets
i9iu89,"A Database of 5,000 Historical Cookbooks",,131,6,cavedave,2020-08-14 09:26:06,https://www.atlasobscura.com/articles/how-to-find-historic-cookbooks,0,datasets
iep5ho,"Catalogue of all the world's edible plants - searchable by nutrient (Zinc, Pro-vit A, C, Iron, Protein) or area. 31,000+ plants",,130,11,PrioritySilent,2020-08-22 19:50:43,/r/preppers/comments/iedq94/catalogue_of_all_the_worlds_edible_plants/,0,datasets
a346wl,"List of 67k NSFW Tumblrs submitted to Reddit in the last 7 years, sorted by frequency.",,129,34,itdnhr,2018-12-04 19:55:13,https://gist.github.com/itdaniher/9d909049085b761cd0cc23bf7a2b7bf0,0,datasets
xyd5n4,List of public data sets incase its helpful,"* [World Health Organization (WHO)](https://www.who.int/data/gho/): Data available on the WHO’s site cover a variety of health-related topics, such as COVID-19, air pollution, and even brain health. There are fact sheets and direct access to various datasets, including: 
   * [Mental health](https://www.who.int/health-topics/mental-health#tab=tab_1)
   * [Road traffic mortality](https://www.who.int/data/gho/data/themes/topics/topic-details/GHO/road-traffic-mortality)
* [FiveThirtyEight](https://data.fivethirtyeight.com/): This is a very popular analysis website that provides direct access to some of their datasets. Topics include sports, politics, science & health, culture, and economics. Check out some of these interesting finds:
   * [Political polls](https://projects.fivethirtyeight.com/polls/)
   * [The best NBA players](https://projects.fivethirtyeight.com/nba-player-ratings/)
* [Data.gov](https://www.data.gov/): The U.S. government has its own open data collection. The site includes information on agriculture, climate, energy, and many other topics. Here are a few unique datasets:
   * [Maritime limits and boundaries](https://catalog.data.gov/dataset/maritime-limits-and-boundaries-of-united-states-of-america#topic=ocean_navigation)
   * [Tornado tracks](https://catalog.data.gov/dataset/tornado-tracks-and-icons-1950-2006)
   * [Census data for the United States of America](https://www.census.gov/data.html)
   * [Census data for EU countries](https://ec.europa.eu/eurostat/web/population-demography/population-housing-censuses)
* [Data Unicef](https://data.unicef.org/): UNICEF’s Data and Analytics team provides global access to data on children. This organization believes that the right data in the right hands can help us make informed and equitable decisions. You can view a variety of topics and data from various countries.

Looking for something specific? [Google Dataset Search](https://datasetsearch.research.google.com/) works like a google search bar for datasets. We think the following datasets look really interesting!

* [Orchids](https://datasetsearch.research.google.com/search?query=Value%20of%20the%20import%20and%20export%20of%20orchids%20in%20the%20Netherlands%202008-2020&docid=L2cvMTFweDF5bnRzOQ%3D%3D) — Did you know the total value of trees, plants, and flowers exported from the Netherlands in 2020 was nearly 9.8 billion euros? 
* [Biodiversity at U.S. national parks](https://datasetsearch.research.google.com/search?query=national%20parks&docid=L2cvMTFqbl82ZmdmeQ%3D%3D) — Did you know that Haliaeetus leucocephalus (also know as a Bald Eagle) can be found in just about every U.S. National Park? Check out this data file to explore animal and plant species that have been identified and verified by evidence in national parks.

[Revenue of the cosmetic & beauty industry in the U.S.](https://datasetsearch.research.google.com/search?query=cosmetics&docid=L2cvMTFuZmJqOWtsXw%3D%3D) — Talk about big money: the revenue of the U.S. cosmetic industry was estimated to amount to about 49.2 billion U.S. dollars in 2019.",131,6,matarrwolfenstein,2022-10-07 23:13:54,https://www.reddit.com/r/datasets/comments/xyd5n4/list_of_public_data_sets_incase_its_helpful/,0,datasets
law4us,Reddit - r/wallstreetbets post since 2012,,127,12,Unanimad,2021-02-02 14:03:49,https://www.kaggle.com/unanimad/reddit-rwallstreetbets,0,datasets
ivvbxp,What can I do with such a dataset? Can I use it to predict sales? Any Ideas?,,130,40,a7madx7,2020-09-19 16:08:11,https://i.redd.it/j0s5k1pto4o51.png,0,datasets
dhc3t9,I've been scraping detailed PG&E outage into a GitHub repo every ten minutes for months,,129,12,simonw,2019-10-13 15:36:24,https://simonwillison.net/2019/Oct/10/pge-outages/,0,datasets
mmrbfq,We made an absolutely free API to search news articles published online,,130,15,kotartemiy,2021-04-08 13:03:44,https://free-docs.newscatcherapi.com/#introduction,0,datasets
ki0ijk,[self-promotion] Spotify 1.2M+ songs dataset,"I scraped (edit: part of) Spotify's song database. The end result is a dataset containing over 1.2 million songs, with titles, artists, release dates, and tons of per-track audio features provided by the [Spotify API](https://developer.spotify.com/documentation/web-api/). You can check it out here: [https://www.kaggle.com/rodolfofigueroa/spotify-12m-songs](https://www.kaggle.com/rodolfofigueroa/spotify-12m-songs)

I will be updating it and adding extended datasets in the following weeks, so stay tuned! Also, if you have any questions, feel free to ask.",134,25,rodolfofigueroa,2020-12-22 07:18:57,https://www.reddit.com/r/datasets/comments/ki0ijk/selfpromotion_spotify_12m_songs_dataset/,0,datasets
ehthj1,"[OC] 174,000 pictures of cars, labeled by make, model, year, price, horsepower, body style, etc.","Download it [here](https://drive.google.com/open?id=1TQQuT60bddyeGBVfwNOk6nxYavxQdZJD) from my Google Drive. The size is 681MB compressed.

You can visit my GitHub repo [here](https://github.com/nicolas-gervais/predicting-car-price-from-scraped-data/tree/master/picture-scraper) (Python), where I give examples and give a lot more information. Leave a star if you enjoy the dataset! 

It's basically every single picture from the site [thecarconnection.com](https://thecarconnection.com). For more details, visit the repo. Picture size is approximately 320x210 but you can also scrape the large version of these pictures if you tweak the scraper. I did a quick classification example using a CNN: [Audi vs BMW with CNN](https://github.com/nicolas-gervais/predicting-car-price-from-scraped-data/blob/master/picture-scraper/Example%20—%20Audi%20vs%20BMW%20ConvNet.ipynb).

Complete list of variables included for *all* pics:

    'Make', 'Model', 'Year', 'MSRP', 'Front Wheel Size (in)', 'SAE Net Horsepower @ RPM', 
    'Displacement', 'Engine Type', 'Width, Max w/o mirrors (in)', 'Height, Overall (in)', 'Length,
     Overall (in)', 'Gas Mileage', 'Drivetrain', 'Passenger Capacity', 'Passenger Doors', 'Body Style'",130,5,nicolas-gervais,2019-12-30 22:42:58,https://www.reddit.com/r/datasets/comments/ehthj1/oc_174000_pictures_of_cars_labeled_by_make_model/,0,datasets
fn5cxz,"Coronavirus Tech Handbook: Includes data sources list, tools for organising & crowdsourced specialist advice for doctors, teachers, charities etc",,127,3,hannah_o_rourke,2020-03-22 19:20:37,http://www.coronavirustechhandbook.com,0,datasets
eev0yi,Announcing the Schrute R package,"Check out this fun package that contains the entire transcripts from the US version of The Office TV show. Fun for NLP or text analysis.

[https://technistema.com/posts/introducing-the-schrute-package-the-entire-transcripts-from-the-office/](https://technistema.com/posts/introducing-the-schrute-package-the-entire-transcripts-from-the-office/)",128,4,None,2019-12-24 02:53:17,https://www.reddit.com/r/datasets/comments/eev0yi/announcing_the_schrute_r_package/,0,datasets
109vyp9,JP Morgan Says Startup Founder Used Millions Of Fake Customers To Dupe It Into An Acquisition,,123,20,cavedave,2023-01-12 09:59:23,https://www.forbes.com/sites/alexandralevine/2023/01/11/jp-morgan-fake-customers-frank-charlie-javice/?sh=38da4eb014d4,0,datasets
108hlnc,I spent the last 5 months working on a website that shows you whether the investment strategies trending on Reddit actually work. I use every possible free API related to finance. How can I further improve it? [self-promotion],,129,25,marcinxyz,2023-01-10 18:52:26,https://app.inegy.io/,0,datasets
sfq1zk,32 million TikTok Videos Dataset (2020),"Hello! I'm sharing a dataset of metadata for 32,489,068 TikTok videos, scraped between 2020-07-22 and 2020-10-13. All the data was publicly available with no login required at the time of scraping. The data is available as flat JSON, and as a MySQL database. There are probably minor inconsistencies between the two formats, but they should be 99% similar. Everything in the JSON file is unaltered response from TikTok, the MySQL database is a bit more trimmed down.

Total uncompressed size is around 200GB

magnet:?xt=urn:btih:475ea4ba18becf5e5f54cd0200999c7c45674fe6&dn=tiktok-2020%5F07-10&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce

## Other Stats

In addition to the videos, there is metadata on:

- 12,382,540 sounds

- 2,533,869 challenges (hashtags)

- 218,479 authors (video creators)

## Credits

Thanks to [David Teather](https://github.com/davidteather) for his TikTok-API project!

https://github.com/davidteather/TikTok-Api",126,16,subuserdo,2022-01-29 19:49:06,https://www.reddit.com/r/datasets/comments/sfq1zk/32_million_tiktok_videos_dataset_2020/,0,datasets
ia4kxx,Unsplash releases massive open-source image dataset with 2M high-quality photos,,127,5,chkgxkdlyl44,2020-08-15 09:44:30,https://www.dpreview.com/news/2164828014/unsplash-releases-massive-open-source-image-dataset-with-2m-high-quality-photos,0,datasets
146rudn,Reddit API changes. What do you think?,"Lots of subs are going to go dark/private because reddit will raise the price of api calls to them. 

&#x200B;

/r/datasets is more pro cheap/free data than most subs. What do you think of the idea of going dark? Example explanation from another sub.  
[https://old.reddit.com/r/redditisfun/comments/144gmfq/rif\_will\_shut\_down\_on\_june\_30\_2023\_in\_response\_to/](https://old.reddit.com/r/redditisfun/comments/144gmfq/rif_will_shut_down_on_june_30_2023_in_response_to/)",126,33,cavedave,2023-06-11 11:43:07,https://www.reddit.com/r/datasets/comments/146rudn/reddit_api_changes_what_do_you_think/,0,datasets
k3w1hq,"Looking for NSFW motion data: how people move. Acceleration fine, displacement great.","This is to inform software being written for open-source machines with linear motion functionality. The machines are novel in that they allow for arbitrary displacement over time, and are not locked into a specific stroke length. (Ie, both displacement and speed are variable during use.)

One company has analyzed NSFW videos to [determine motions seen during oral sex.](https://autoblow.com/bjpaper/) While this may be a promising approach to get some data, it's not ideal, both for the amount of labor involved and the accuracy of the data being collected.

This data may not exist anywhere, but figured I'd go fishing and see what might be out there.",125,26,bawdyblueprints,2020-11-30 13:45:51,https://www.reddit.com/r/datasets/comments/k3w1hq/looking_for_nsfw_motion_data_how_people_move/,0,datasets
hf470e,US Counties Dataset,"[https://github.com/evangambit/JsonOfCounties](https://github.com/evangambit/JsonOfCounties)

Repository of 3000 counties, including age distributions, racial distributions, average income, population, location, size, covid deaths, fatal police shootings, and suicides.

Compilations of county-level data is hard to find, not least because slightly different naming conventions (especially in Alaska and Virginia) make it difficult to automatically merge datasets together.

This dataset can be used (for example) to reproduce [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6391295/?fbclid=IwAR2Y0h6D-cEWXqk4_dooBX2MgUUrADyEIHN6iQFmbDc1qXf0MYHK3qWbUPo) (which used 50 datapoints – one for each state) by using 3000 data points instead.

Note that some data is missing for some counties (most commonly CDC death data, which is frequently suppressed for small counties due to privacy concerns).

Let me know if there is other data you'd want to see added.",125,11,you-get-an-upvote,2020-06-24 16:31:44,https://www.reddit.com/r/datasets/comments/hf470e/us_counties_dataset/,1,datasets
p898kj,[Self Promotion] Free Restaurant Menus database covering 6.5M menu items across 44k restaurants,"Hi r/datasets

We completed our menu item data bounty this week. We paid contributors to create a database of menu items, nutritional information and prices. Blog is here:

[https://www.dolthub.com/blog/2021-08-20-menus-bounty-retrospective/](https://www.dolthub.com/blog/2021-08-20-menus-bounty-retrospective/)

Database is here:

[https://www.dolthub.com/repositories/dolthub/menus](https://www.dolthub.com/repositories/dolthub/menus)

Data is free and open for you to do what you want with.",122,8,timsehn,2021-08-20 16:57:00,https://www.reddit.com/r/datasets/comments/p898kj/self_promotion_free_restaurant_menus_database/,0,datasets
hlc0ic,"7,500 Watercolor Paintings of Every Known Fruit in the World 1886",,121,7,cavedave,2020-07-04 22:28:58,http://www.openculture.com/2019/06/the-us-government-commissioned-7500-watercolor-paintings.html,0,datasets
gnfwb6,"Salaries for all Texas public employees (site includes Download as CSV link), 148,000+ rows",,127,3,danwin,2020-05-20 17:38:29,https://salaries.texastribune.org/,0,datasets
s5dwdd,Cost breakdown to run a chess website. It takes $420K to run lichess per year.,,123,12,cavedave,2022-01-16 15:16:27,http://lichess.org/costs,0,datasets
5mumo4,Uber is releasing detailed historical transit data to the public.,,122,13,visualminder,2017-01-09 01:14:26,https://movement.uber.com/cities,0,datasets
d85tk5,"Audi has released a new dataset for autonomous driving, A2D2",,119,1,pdillis,2019-09-23 12:25:54,https://www.audi-electronics-venture.de/aev/web/de/driving-dataset.html,0,datasets
mdbaks,"Found this absolute gem - free ZIP (US) and Postal (Canada) codes, with seemingly accurate geocoordinates as well",,119,11,highnorthhitter,2021-03-25 23:24:33,https://www.serviceobjects.com/blog/free-zip-code-and-postal-code-database-with-geocoordinates/,0,datasets
l58b7z,"I scraped all QAnon posts into a machine readable JSON blob, including replied-to posts and links and media information",,119,27,CharlesStross,2021-01-26 06:45:43,https://github.com/jkingsman/JSON-QAnon,0,datasets
gmch6g,NFL play-by-play data since 2000 in SQL form,"[https://www.dolthub.com/repositories/Liquidata/nfl-play-by-play](https://www.dolthub.com/repositories/Liquidata/nfl-play-by-play)

The NFL recently shut off it's play-by-play data API. The above is all the data going back to 2000.",120,13,timsehn,2020-05-18 23:05:01,https://www.reddit.com/r/datasets/comments/gmch6g/nfl_playbyplay_data_since_2000_in_sql_form/,0,datasets
3mg812,Full Reddit Submission Corpus now available (2006 thru August 2015),"The **full Reddit Submission Corpus** is now available here:  

http://reddit-data.s3.amazonaws.com/RS_full_corpus.bz2 (42,674,151,378 bytes compressed)

**sha256sum:** 91a3547555288ab53649d2115a3850b956bcc99bf3ab2fefeda18c590cc8b276

This represents all publicly available Reddit submissions from January 2006 - August 31, 2015).  

**Several notes on this data:**

Data is complete from *January 01, 2008 thru August 31, 2015*.  Partial data is available for years 2006 and 2007.  The reason for this is that the id's used when Reddit was just a baby were scattered a bit -- but I am making an attempt to grab all data from 2006 and 2007 and will make a supplementary upload for that data once I'm satisfied that I've found all data that is available.

I have added a key called ""retrieved_on"" with a unix timestamp for each submission in this dataset.  If you're doing analysis on scores, late August data may still be too young and you may want to wait for the August and September additions that I will make available in October.  

This dataset represents approximately 200 million submission objects with score data, author, title, self_text, media tags and all other attributes available via the Reddit API.  

This dataset will go nicely with the full Reddit Comment Corpus that I released a couple months ago.  The link_id from each comment corresponds to the id key in each of the submission objects in this dataset.  

**Next steps**

I will provide monthly updates for both comment data and submission data going forward.  Each new month usually adds over 50 million comments and approximately 10 million submissions (this fluctuates a bit).  Also, I will split this large file up into individual months in the next few days.  

**Better Reddit Search**

My goal now is to take all of this data and create a usable Reddit search function that uses comment data to vastly improve search results.  Reddit's current search generally doesn't do much more than look at keywords in the submission title, but the new search I am building will use the approximately 2 billion comments to improve results.  For instance, if someone does a search for Einstein, the current search will return results where the submission title or self text contain the word Einstein.  Using comments, the search I am building will be able to see how often Einstein is mentioned in the body of comments and weight those submissions accordingly.  

An example of this would be if someone posted a question in /r/askscience ""How is the general theory of relativity different than the special theory of relativity?""  Many of the comments would contain ""Einstein"" in the comment bodies, thereby making that submission relevant when someone does a search for ""Einstein.""  This is just one of the methods for improving Reddit's search function.  I hope to have a Beta search in place in early December.  

_________________________________________

*If you find this data useful for your research or project, please consider making a donation so that I can continue making timely monthly contributions.  Donations help cover server costs, time involved, etc.  Donations are always much appreciated!*

[Donation page](https://pushshift.io/donations/)

As always, if you have any questions, feel free to leave comments!  ",118,77,Stuck_In_the_Matrix,2015-09-26 10:29:13,https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/,1,datasets
ux019h,US Presidential Debate Transcripts as Dialogues in JSON format 1960-2020,"Hi everyone! First post here. I have made a dataset containing all US presidential and vice-presidential debate transcripts from 1960 to 2020. More information, accredition and the dataset itself can be found here on Kaggle: [https://www.kaggle.com/datasets/arenagrenade/us-presidential-debate-transcripts-19602020](https://www.kaggle.com/datasets/arenagrenade/us-presidential-debate-transcripts-19602020).

How would you guys use it?",114,7,Arena-Grenade,2022-05-24 20:20:08,https://www.reddit.com/r/datasets/comments/ux019h/us_presidential_debate_transcripts_as_dialogues/,0,datasets
cmvhwi,Millions of Books Are Secretly in the Public Domain. You Can Download Them Free,,115,8,cavedave,2019-08-06 20:04:04,https://www.vice.com/en_us/article/kz4e3e/millions-of-books-are-secretly-in-the-public-domain-you-can-download-them-free,0,datasets
pzhpay,55 Percent of Police Killings Are Misclassified as Other Causes of Death,,116,7,cavedave,2021-10-01 21:35:24,https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)01609-3/fulltext#seccestitle180,0,datasets
nqu8fv,[Self Promotion] Open Database of 100M Hospital Prices,"Hey r/datasets,

It's me again, the CEO of DoltHub. We finished our second hospital price transparency bounty last week. We paid to have an open database built of 100M hospital prices, this time with a little bit more manageable schema. 

Check out the blog post here:

[https://www.dolthub.com/blog/2021-06-02-hospital-price-v2-retrospective/](https://www.dolthub.com/blog/2021-06-02-hospital-price-v2-retrospective/)

And the database here:

[https://www.dolthub.com/repositories/dolthub/hospital-price-transparency-v2](https://www.dolthub.com/repositories/dolthub/hospital-price-transparency-v2)

We're looking for people to use it to tell us how it can be improved.",116,5,timsehn,2021-06-02 19:34:17,https://www.reddit.com/r/datasets/comments/nqu8fv/self_promotion_open_database_of_100m_hospital/,0,datasets
gwo41w,Lancet retracts major Covid-19 paper amid scrutiny of the data underlying the paper,,116,45,cavedave,2020-06-04 19:18:45,https://www.statnews.com/2020/06/04/lancet-retracts-major-covid-19-paper-that-raised-safety-concerns-about-malaria-drugs/,0,datasets
g2qqrq,Introducing the Spotify Podcast Dataset,,119,4,philogb,2020-04-16 23:52:14,https://labs.spotify.com/2020/04/16/introducing-the-spotify-podcast-dataset-and-trec-challenge-2020/amp/,0,datasets
fgvvuy,A pipeline and Python/Pandas environment for the Johns Hopkins COVID-19 data,"[https://github.com/willhaslett/covid-19-growth](https://github.com/willhaslett/covid-19-growth)

Want to do your own analytics on the JH COVID-19 data? This provides a sensible starting point in Python/Pandas, wired up to the daily JH CSV files. Has a US focus as of now. Support for filtering by arbitrary regions.",119,8,braindongle,2020-03-11 11:50:32,https://www.reddit.com/r/datasets/comments/fgvvuy/a_pipeline_and_pythonpandas_environment_for_the/,0,datasets
f9r6pa,Smithsonian Releases 2.8 Million Images Into Public Domain,,116,3,cavedave,2020-02-26 10:18:32,https://www.smithsonianmag.com/smithsonian-institution/smithsonian-releases-28-million-images-public-domain-180974263/,0,datasets
5czkdz,"7,375 Donald Trump tweets in Excel and CSV",,117,35,None,2016-11-15 00:40:57,http://www.crowdbabble.com/blog/the-11-best-tweets-of-all-time-by-donald-trump/,0,datasets
lwyuc6,[Self promotion] Free and Open Hospital Price Database with 1400 hospitals and 72.7M prices,"Hi r/datasets,

CEO of [DoltHub](https://www.dolthub.com) here. We ran a data bounty for the last 6 weeks to collect US hospital prices. We ended up getting 1400 hospitals and 72.7M prices all in the same SQL schema. It's an awesome database to play with. Enjoy.

Blog article for you review: https://www.dolthub.com/blog/2021-03-03-hpt-bounty-review/

Actual database: https://www.dolthub.com/repositories/dolthub/hospital-price-transparency",115,6,timsehn,2021-03-03 17:06:37,https://www.reddit.com/r/datasets/comments/lwyuc6/self_promotion_free_and_open_hospital_price/,0,datasets
fobhdi,Dataset of 40+ million tweets of COVID19 chatter," 

Just released: Dataset of 40+ million tweets of COVID19 chatter

Details: [http://www.panacealab.org/covid19/](http://www.panacealab.org/covid19/)

Direct link to dataset: [https://doi.org/10.5281/zenodo.3723940](https://doi.org/10.5281/zenodo.3723940)

This dataset will be constantly updated (read details on website)",114,8,jmbanda,2020-03-24 19:34:22,https://www.reddit.com/r/datasets/comments/fobhdi/dataset_of_40_million_tweets_of_covid19_chatter/,0,datasets
yk0o85,Broken McDonald's Ice cream machines worldwide,,113,5,cavedave,2022-11-02 09:40:13,https://mcbroken.com/,0,datasets
gqzbph,"football.json 2019/20 Update - Free open public domain football data in JSON incl. English Premier League, Bundesliga, Primera División, Serie A and more - No API key required ;-)",,113,1,geraldbauer,2020-05-26 15:37:59,https://github.com/openfootball/football.json,0,datasets
bb0zbp,"Spotify acoustic data for 340,000 songs from Billboard 200 albums, January 1963 - January 2019",,114,11,snappcrack,2019-04-08 23:51:50,https://components.one/datasets/billboard-200/,0,datasets
7xzigy,"NBC News publishes CSV download of 200,000 malicious Russian tweets from the 2016 election",,114,7,None,2018-02-16 15:38:46,https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731?cid=sm_npd_nn_tw_ma,0,datasets
hm6r0e,"List of people that died while on police encounter, with as much details as possible (cause of death, sex, reason why cops are involved, etc...)","The best list that I have is this: https://github.com/washingtonpost/data-police-shootings , but that doesn't including [chokings](https://ns.reddit.com/r/Bad_Cop_No_Donut/comments/hbc7gb/everyone_meet_tony_timpa_he_called_911_for_help/) and things like that (only shootings). 

I would like the most comprehensive list of death when police are involved even if the police is not responsible.",113,17,covidtwentytwenty,2020-07-06 12:29:57,https://www.reddit.com/r/datasets/comments/hm6r0e/list_of_people_that_died_while_on_police/,0,datasets
dvud1o,The Trace Obtained Data On 4.3 Million Violent Crimes From 50+ Police and Sheriff’s Departments. Download It Here,,111,4,cavedave,2019-11-13 16:41:12,https://www.thetrace.org/violent-crime-data/,0,datasets
g6ulbg,U.S. Supreme Court Case Dataset,"Hi Everyone,

I created a dataset of cleaned Supreme Court transcripts (speaker name, speaker duration, court details, etc.) and information on Supreme Court justices (place of birth, age, race, parent's occupation, religion, etc.). The data was supposed to be used for a research project, but it ended up falling through. I wanted to share it here in case anyone could find any good uses for it. Here's a link to the [GitHub repo](https://github.com/EricWiener/supreme-court-cases). Please let me know if you have any questions.

Thanks,

Eric",112,15,EricW_CS,2020-04-23 20:47:13,https://www.reddit.com/r/datasets/comments/g6ulbg/us_supreme_court_case_dataset/,0,datasets
cnn2hi,Looking for a dataset for a project? Check this out:,,114,10,RickDeveloper,2019-08-08 15:10:36,https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research,0,datasets
8zwx7v,Microsoft’s Bing Maps team published an open dataset describing the outlines of nearly 125 million buildings in the United States.,,114,7,yourbasicgeek,2018-07-18 16:20:47,https://github.com/Microsoft/USBuildingFootprints,0,datasets
8u5hd3,UC Berkeley Open Sources Largest Self-Driving Dataset,,112,3,Vardox,2018-06-27 01:17:12,http://bdd-data.berkeley.edu,0,datasets
6d4c29,BuzzFeed publishes 40 years worth of federal payroll records (~30GB),,110,11,danwin,2017-05-24 19:17:23,https://www.buzzfeed.com/jsvine/sharing-hundreds-of-millions-of-federal-payroll-records,0,datasets
1322qoi,Why a public database of hospital prices doesn't exist yet,,112,19,alecs-dolt,2023-04-28 17:10:54,https://www.dolthub.com/blog/2023-04-21-open-source-hospital-price-transparency-3/,0,datasets
ug1by8,Is it legal to make an open source GitHub repo showing how to scrape data from Realtor/Zillow.com?,"I have a private repo I'm working on for personal reasons (I'm looking to buy a house), but I'm happy to make it open source for others to use. I don't want to make money from this, nor do I want my code used to make money etc.",111,48,radlinsky,2022-05-01 14:42:34,https://www.reddit.com/r/datasets/comments/ug1by8/is_it_legal_to_make_an_open_source_github_repo/,0,datasets
lfyr43,An open-access dataset of 80 million Indian legal case records,,111,8,cavedave,2021-02-09 09:20:32,https://devdatalab.medium.com/big-data-for-justice-f53e0e14c9c9,0,datasets
kx9e3t,"80,000 tweets from the day of the capitol hill riots","[https://github.com/cwhaley112/capitol-riot-tweets](https://github.com/cwhaley112/capitol-riot-tweets)

Not as big as the Parler data dumps, but some cool NLP work could be done here.

\~25% of tweets have a US state/DC label. \~2% are geotagged.

Can post the script I made to create daily datasets like this one if there's interest.",111,9,cwhaley112,2021-01-14 16:47:52,https://www.reddit.com/r/datasets/comments/kx9e3t/80000_tweets_from_the_day_of_the_capitol_hill/,0,datasets
gaukz5,I've scraped around 800 million characters worth of comments from the top 50 subreddits,"Hi,

&#x200B;

I've been working on a machine learning side project amidst the quarantine, and for that, I have scraped around the 1000 top posts from the top 50 most subscribed subreddits, and saved 100 comments of each into a data set.

I ended up going with different data for my project, but decided that I might as well share it.

[You can find the dataset here](https://github.com/CrakenHUN/RedditCommentsDataset)

&#x200B;

[And in case you want to toy around with the scraper scripts I used to gather the comments, here they are](https://github.com/CrakenHUN/RedditScraperScripts)  (this includes two scripts, one optimized to save comments from one subreddit in a really user friendly way, and one of them to do so for a list of subreddits. All necessary data for setting them up is included in the repository)",106,39,None,2020-04-30 12:18:31,https://www.reddit.com/r/datasets/comments/gaukz5/ive_scraped_around_800_million_characters_worth/,0,datasets
vkcqow,100+ Machine Learning Datasets Curated For You,,105,4,TigerRumMonkey,2022-06-25 11:08:31,https://www.projectpro.io/article/100-machine-learning-datasets-curated-for-you/407,0,datasets
hixfeo,How to obtain median income data for zip codes,"Every week or so for about the last two months I keep seeing requests about how to get median income for zip codes in the U.S.  Below is a quick and dirty guide, followed by links to official training webinars on [census.gov](https://census.gov) and then a website on why you shouldn't use zip codes as a geography.

How to get the data:

1. Go to [data.census.gov](https://data.census.gov).
2. In the ""I'm looking for..."" search bar, type in ""median income""
3. A quick answer in a box pops up. Underneath that, it says ""tables"". Click on the text that says ""Income in the Past 12 Months (in 2018 inflation-adjusted dollars)"". This takes you to a table with an income distribution and mean and median income.
4. On the upper rightish corner there will be the year. It will say something like ""2018: ACS 1-year estimates"". Click on this and select the 5-year estimates. You can select years for past data as well. Zip codes aren't available for 1-year data, though. 2018 is the most current year available as the time that I am writing this.  As a side note, you can find the release dates here:  [https://www.census.gov/programs-surveys/acs/news/data-releases.html](https://www.census.gov/programs-surveys/acs/news/data-releases.html) 
5. To the right of that click on ""Customize Data"". This pops up a ribbon. Click on ""Geographies"".
6. Click on the toggle thingy at the top of the menu under ""Geography"" to show summary levels. After it shows a 3-digit number before each geography (e.g. 010-nation), scroll a ways down to where it says ""860 - 5-digit ZCTA"". Click on this. A side bar opens up. You can select all Zip Codes in the US or specific ones. At the top, if you click on the title by the magnifying glass, you can search for a zip code. Just be sure to start it the same was as they are listed. It looks like you have to type ""ZCTA5"" and then a space and then the zip code. As a note, ZCTA is Census-speak for ""Zip Code Tabulation Area"".
7. Once you chosen a few, hit close, and BOOM! you're data shows up. If you choose all Zip Codes, it won't display as there are too many. But you can download them.

Now, there are a bunch of training videos to help you out.  One link is the Census Academy:  [https://www.census.gov/data/academy/topics/data-tools.html](https://www.census.gov/data/academy/topics/data-tools.html).

There are also webinars:  [https://www.census.gov/data/academy/webinars.html](https://www.census.gov/data/academy/webinars.html) 

Instead of using [data.census.gov](https://data.census.gov), the Census also has an API.  The landing page is here:  [https://www.census.gov/data/developers.html](https://www.census.gov/data/developers.html).  

There is also a webinar on how to use the API:   [https://www.census.gov/data/academy/webinars/2019/api-acs.html](https://www.census.gov/data/academy/webinars/2019/api-acs.html).

You might want to find something besides median income.  There are a lot of different tables and data products.  Here is one way to find tables:   [https://www.census.gov/acs/www/data/data-tables-and-tools/](https://www.census.gov/acs/www/data/data-tables-and-tools/) 

Finally, as a caveat, here is a website about why Zip Codes may not be the best geography to use for analyzing data:   [https://carto.com/blog/zip-codes-spatial-analysis/](https://carto.com/blog/zip-codes-spatial-analysis/)",106,24,Mcletters,2020-06-30 22:07:43,https://www.reddit.com/r/datasets/comments/hixfeo/how_to_obtain_median_income_data_for_zip_codes/,0,datasets
fk9eep,More than 1 million results of domestic soccer leagues,"Hi Guys,

I know that these days everybody is interested in Covid19 data, but I thought I share a dataset for those who want to get there mind of this nasty virus and look at something fun. 

The dataset I am sharing consists of more than a million results of 207 domestic top-tier soccer leagues around the world, also including international tournaments (UCL,ECL,etc)
from 1888-2019.

I gathered this dataset over the course of 8 years from a lot of online and offline sources and combining those was really tedious. I use it for a pet project ([soccerverse.com](http://soccerverse.com/)), but apart from that it is just getting dusty on my hard drive.

The only thing I ask for in return is that you share any fun insight you get from the data with me.

The data is available on [github](https://github.com/schochastics/football-data) and [kaggle](https://www.kaggle.com/schochastics/domestic-football-results-from-1888-to-2019)",107,9,munky86,2020-03-17 17:38:10,https://www.reddit.com/r/datasets/comments/fk9eep/more_than_1_million_results_of_domestic_soccer/,0,datasets
u7zhqa,Amazon releases 51-language dataset for language understanding,,107,2,cavedave,2022-04-20 15:46:46,https://www.amazon.science/blog/amazon-releases-51-language-dataset-for-language-understanding,0,datasets
mu1ww9,"survivoR R package: ""a collection of datasets detailing events and the cast across all 40 seasons of the US Survivor, including castaway information, vote history, immunity and reward challenge winners, jury votes, and viewers""",,111,8,antirabbit,2021-04-19 14:33:44,http://gradientdescending.com/survivor-now-on-cran/,0,datasets
hgiz71,"From Lyft researchers: The largest self-driving dataset for motion prediction to date, with over 1,000 hours of data!",,106,0,MLtinkerer,2020-06-26 23:52:25,/r/LatestInML/comments/hgibwr/from_lyft_researchers_the_largest_selfdriving/,0,datasets
h9o379,"What to do with 2,500,000+ memes","I have a bit over 2.5 million unlabeled memes that I've downloaded, which is around 170GB. I have no real use for them and I'd like to distribute them to anyone who wants them.

Since it's a chore to upload (and maybe difficult with my ISP's upload limits) I wanted to post here and see if there's interest. 

Definitely let me know what you'd do with them!",108,38,cogitoergodum,2020-06-15 20:23:01,https://www.reddit.com/r/datasets/comments/h9o379/what_to_do_with_2500000_memes/,0,datasets
9xa77f,The 50 Best Public Datasets for Machine Learning,,110,0,cavedave,2018-11-15 10:56:32,https://medium.com/datadriveninvestor/the-50-best-public-datasets-for-machine-learning-d80e9f030279,0,datasets
iot64s,Free dataset covering 5 billion vehicles in a combined time span of 3.8 years from over 40 cities,"**UTD19 is a large-scale traffic dataset from over 23'541 stationary detectors on urban roads in 40 cities worldwide making it the largest multi-city traffic dataset publicly available.**

[https://utd19.ethz.ch/](https://utd19.ethz.ch/)

In total, we detected almost 5 billion vehicles covering a combined time span of 3.8 years in over 40 cities incl. London, Tokyo, or Zurich. The UTD19 traffic data that we collected at the Institute for Transport Planning and Systems - ETH Zurich and the CITIES Center for Interacting Urban Networks - NYU Abu Dhabi is free for all research use. You only have to sign up, agree with our conditions, and you are all set.",109,9,ddechamb,2020-09-08 13:04:18,https://www.reddit.com/r/datasets/comments/iot64s/free_dataset_covering_5_billion_vehicles_in_a/,0,datasets
gq4noe,A site to easily download the John Hopkins Covid data straight into Excel. Apply your filters and download.,,109,1,shreddit47,2020-05-25 04:49:36,https://coronachaser.com,0,datasets
ox98sn,We open sourced a 5.7M vehicle dataset covering 1.2k dealers between June 2018-2020.,,106,0,Competitive-Int,2021-08-03 18:09:48,https://www.kaggle.com/cisautomotiveapi/large-car-dataset/,0,datasets
kh4poa,I converted Amazon's chatbot messaging dataset into a .csv file for Kaggle. It has over 8000 conversations and over 180k messages,"Link: [https://www.kaggle.com/arnavsharmaas/chatbot-dataset-topical-chat](https://www.kaggle.com/arnavsharmaas/chatbot-dataset-topical-chat)

There is more information of the chatbot in the description in Kaggle.

EDIT(PS): If you cannot download this dataset due to the ""too many requests"" error, please go here and download it:

[https://docs.google.com/spreadsheets/d/1dFdlvgmyXfN3SriVn5Byv\_BNtyroICxdgrQKBzuMA1U/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1dFdlvgmyXfN3SriVn5Byv_BNtyroICxdgrQKBzuMA1U/edit?usp=sharing)",104,6,ARNisUsername,2020-12-20 23:15:26,https://www.reddit.com/r/datasets/comments/kh4poa/i_converted_amazons_chatbot_messaging_dataset/,0,datasets
ekca7t,"[P] 64,000 pictures of cars, labeled by make, model, year, price, horsepower, body style, etc.",,104,0,cavedave,2020-01-05 12:13:34,/r/MachineLearning/comments/ek5zwv/p_64000_pictures_of_cars_labeled_by_make_model/,0,datasets
deq5uf,A list of the biggest datasets for machine learning,,106,3,synthphreak,2019-10-07 21:33:40,https://www.datasetlist.com/,0,datasets
d9c6vo,To Fight Deepfakes Google Released Deepfakes Dataset,This content removed to opt-out of Reddit's sale of posts as training data to Google. See here: https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/ Or here: https://www.techmeme.com/240221/p50#a240221p50,103,6,DavidJAntifacebook,2019-09-26 00:42:05,https://www.reddit.com/r/datasets/comments/d9c6vo/to_fight_deepfakes_google_released_deepfakes/,0,datasets
bxfo7l,Uber Movement: anonymised traffic data of lots of cities.,,106,3,cavedave,2019-06-06 11:33:57,https://movement.uber.com/?lang=en-GB,0,datasets
5ff46c,"Full Publicly available Reddit dataset will be searchable by Feb 15, 2017 including full comment search.","I just wanted to update everyone on the progress I am making to make available all 3+ billion comments and submissions available via a comprehensive search API.

I've figured out the hardware requirements and I am in the process of purchasing more servers.  The main search server will be able to handle
comment searches for any phrase or word within one second across 3+ billion comments.  API will allow developers to select comments by date range, subreddit, author and also receive faceted metadata with the search. 

For instance, searching for ""Denver"" will go through all 3+ billion comments and rank all submissions based on the frequency of that word appearing in comments.  It would return the top subreddits for specific terms, the top authors, the top links and also give corresponding similar topics for the searched term.  

I'm offering this service free of charge to developers who are interested in creating a front-end search system for Reddit that will rival anything Reddit has done with search in the past.  

Please let me know if you are interested in getting access to this.  February 15 is when the new system goes live, but BETA access with begin in late December / early January.

**Specs for new search server**

* Dual E5-2667v4 Xeon processors (16 cores / 32 virtual)
* 768 GB of ram
* 10 TB of NVMe SSD backed storage
* Ubuntu 16.04 LTS Server w/ ZFS filesystem
* Postgres 9.6 RMDBS
* Sphinxsearch (full-text indexing)

",105,76,Stuck_In_the_Matrix,2016-11-28 23:31:52,https://www.reddit.com/r/datasets/comments/5ff46c/full_publicly_available_reddit_dataset_will_be/,0,datasets
dwvu3z,"200,000+ Jeopardy! Questions",,102,5,cavedave,2019-11-15 19:46:05,https://www.kaggle.com/tunguz/200000-jeopardy-questions,0,datasets
dd84u8,1 million simplified recipes dataset (simplified-recipes-1M) [OC],"The dataset contains more than 1 million meticulously cleaned up and preprocessed recipe ingredient lists.

[**Download**, more information and **original sources and credits**](https://dominikschmidt.xyz/simplified-recipes-1M/)

[I also trained a neural network on this dataset to improve existing recipes, **see the results here.**](https://dominikschmidt.xyz/recipe-net/)",103,13,dominik_schmidt,2019-10-04 14:30:48,https://www.reddit.com/r/datasets/comments/dd84u8/1_million_simplified_recipes_dataset/,1,datasets
63spoc,1.9GB of Urban Dictionary definitions (1999 - May 2016),"I've been meaning to post this for a while - hopefully someone manages to do something cool with it! Posting anonymously because I'm not sure how protective the UD founders are, but I think they'd be cool the the data science community playing around with it.

Each word is on it's own line. Here's the last line of the file to give you an idea of the structure:

    { ""_id"" : { ""$oid"" : ""572fa4e412bfe10f2cf1a640"" }, ""defid"" : 9041433, ""definition"" : ""Italian-American slang used for \""*****\"", \""****\"". If \""*****\"" goes too heavy for one. And one wants to put it delicately, it can be used. Especially in Brooklyn NY."", ""permalink"" : ""http://puchiacchia.urbanup.com/9041433"", ""thumbs_up"" : 0, ""author"" : ""The Benighted"", ""word"" : ""puchiacchia"", ""current_vote"" : """", ""example"" : ""-You're such a *****!\r\n+What did you just say to me?!\r\n-Alright alright, you're a puchiacchia then LOL!"", ""thumbs_down"" : 0, ""tags"" : [  ], ""sounds"" : [  ], ""lowercase_word"" : ""puchiacchia"" }

You can use `tail -f words.json` if you're on linux (or mac?) to have a look yourself once you've extracted the file. Here's an image of the prettified json: http://i.imgur.com/nmKJsBc.png

You can see that same definition with the API here: http://api.urbandictionary.com/v0/define?defid=9041433 Note that the `_id` property was added by me and can be disregarded. Ditto for the `lowercase_word` property.

Unfortunately it's missing the last year of data, because it was scraped in May 2016, but perhaps someone will be able to grab the last year's worth and throw them in the comments if there's enough interest in this dataset. You can scrape word ids from here: http://www.urbandictionary.com/yesterday.php?date=2017-03-29&page=2 (note that each date has many pages) and then just throw them into the urbandictionary api link above. Best bet would be to start one month prior (April), and just ignore if it's already in the DB, because I'm not exactly sure what date in May I finished.

It compressed to around 400mb. If someone could upload mirrors or make a torrent out of it and post it in the comments that'd be great because I won't be able to personally host it for more than a couple of months. Also, it's a `7z` file, which will be a bit weird for some, but there are freeware/open-source extractors for it on every major platform - just head over to google. Here's the file:

* https://archive.org/details/UrbanDictionary1999-May2016DefinitionsCorpus
* Mirror 1: https://figshare.com/articles/UrbanDictionary_1999-May2016_Definitions_Corpus/4828954
* Please post a link in the comments if you've made another mirror.

Cheers! :)",102,7,aqswde123456,2017-04-06 12:51:31,https://www.reddit.com/r/datasets/comments/63spoc/19gb_of_urban_dictionary_definitions_1999_may_2016/,0,datasets
igyoxu,I made a python package that loads the OpenSubtitles dataset using memory mapping - English version of the dataset has 440M sentences,,102,4,cdminix,2020-08-26 13:21:21,https://github.com/MiniXC/opensubtitles-dataloader,0,datasets
e2z2gj,"I collected the emojis used in 3,015,922,953 tweets since 2013 and created this website",,103,26,enric94,2019-11-28 14:57:37,https://emoji.enricmor.eu,0,datasets
dcx1bo,"Metadata from 218,000 PornHub videos, Jan. 2008 - Dec. 2018",,104,15,snappcrack,2019-10-03 20:54:01,https://components.one/datasets/metadata-from-218000-pornhub-videos-jan-2008-dec-2018/,0,datasets
8rndor,"Data on 1,340 Coffee Bean Reviews (aroma, acidity, flavor, altitude, + more)",,105,12,JLD_,2018-06-17 00:09:41,https://github.com/jldbc/coffee-quality-database,0,datasets
gl3fq1,The Economists excess deaths dataset,,101,11,OffTheChartsC,2020-05-16 22:10:20,https://github.com/TheEconomist/covid-19-excess-deaths-tracker,0,datasets
f6w7na,"Flight price data from multiple airlines and vendors. It is comparing more than 70 vendors to provide the cheapest prices in JSON. This might be helpful in analyzing flight prices. It also provide flight tracking API with speed, coordinates, altitude,etc. Definitely check it out.",,103,6,yakult2450,2020-02-20 17:14:13,https://www.flightapi.io/,0,datasets
be0x41,"Chicago first city to publish data on ride-hailing trips, drivers, and vehicles",,103,2,danwin,2019-04-16 23:44:06,https://chicago.curbed.com/2019/4/15/18311340/uber-lyft-chicago-data-fares-drivers,0,datasets
k04cs8,Thought this might be an interesting tid bid related to the industry (crosspost from /books) - Data-mining reveals that 80% of books published 1924-63 never had their copyrights renewed and are now in the public domain,,99,4,nycetouch2,2020-11-24 12:47:01,https://boingboing.net/2019/08/01/80pct-pd.html?fbclid=IwAR3pMnu6G0d-M6Ldf_i5muga3g_m0NVkMQ5u6NiE-f_FapGKftJO76_hxbw,0,datasets
gd9evj,Free graphical CSV file editor for Windows 10,"I wrote a graphical CSV file editor for my own needs and then made it user friendly, robust and fast enough so I could sell it on Microsoft Store. Unfortunately my marketing skills are not up to my coding and  engineering skills, so not very many people are buying it... so I thought I could just as well give it away here on Reddit for free now. There's no catch, no ads or other annoyances - I really just want it to be put to use wherever it makes sense.

It's different from other CSV editors and Excel because it shows data  graphically as line plots instead of in a grid. See if it seems useful for you here: [https://www.microsoft.com/store/apps/9NP4JT39W71D](https://www.microsoft.com/store/apps/9NP4JT39W71D)

If it does, open Microsoft Store and in the menu select Redeem code. Here's the code: G427R-MK62P-4V4MC-J26FT-43CFZ . The code expires Sunday  May 10th at 23:59 UTC.

Hope that's useful for someone!",100,13,jerha202,2020-05-04 10:51:17,https://www.reddit.com/r/datasets/comments/gd9evj/free_graphical_csv_file_editor_for_windows_10/,0,datasets
fh55ei,"[Project] I've compiled weather/climate date for the confirmed COVID19 infection sites, if anyone wants it",,97,2,cavedave,2020-03-11 21:54:00,/r/MachineLearning/comments/fh2rr6/project_ive_compiled_weatherclimate_date_for_the/,0,datasets
93f0c2,FiveThirtyEight and Clemson Researchers Release Russian Troll Factory Tweets,,103,12,UnreasonableDoubter,2018-07-31 14:37:15,https://github.com/fivethirtyeight/russian-troll-tweets/,0,datasets
7xhnwu,"200K tweets from Russian trolls manipulating 2016 election; deleted by twitter, unavailable elsewhere",,103,22,everywhere_anyhow,2018-02-14 12:31:25,https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731,0,datasets
5sm5z9,Open Data advocate Hans Rosling passed away this morning,,99,6,cavedave,2017-02-07 15:59:02,https://m.gapminder.org/news/sad-to-announce-hans-rosling-passed-away-this-morning/,0,datasets
ib1bkm,Where can someone find a public dataset on where/which USPS postal boxes and sorting machines have been removed?,I'm not able to find this yet.,104,16,kylecurator,2020-08-16 21:55:36,https://www.reddit.com/r/datasets/comments/ib1bkm/where_can_someone_find_a_public_dataset_on/,0,datasets
d4k68r,"27,000 Games from Steam Store with SteamSpy data such as Owners and Playtime","Dataset on Kaggle:

[https://www.kaggle.com/nikdavis/steam-store-games](https://www.kaggle.com/nikdavis/steam-store-games)

Scraping, cleaning, and EDA on my personal blog (may help give you some ideas of how to use it):

[https://nik-davis.github.io/tag/steam.html](https://nik-davis.github.io/tag/steam.html)

Downloaded from the Steam and SteamSpy APIs. Includes release data, developers, publishers, genres, positive/negative ratings, average/median playtime, owners (estimation from SteamSpy. Pretty inaccurate though), and price, as well as descriptions, media data (such as links to screenshots), system requirements, and support info (like company url and email).

Created this for a personal learning project and I had a lot of fun creating and investigating it, so thought I'd share it here for you all. Feel free to use it however you like, but credit where necessary would be appreciated. 

Have fun!",103,7,Rokanov,2019-09-15 13:03:52,https://www.reddit.com/r/datasets/comments/d4k68r/27000_games_from_steam_store_with_steamspy_data/,0,datasets
cqhwij,Stanford Geospatial Center's Massive Repository List of Open Datasets ...,,102,2,StinkyFangers,2019-08-14 23:40:44,https://github.com/StanfordGeospatialCenter/awesome-public-datasets,0,datasets
m9d9r6,This Transparency Project Is Creating a Massive Collection of Police Data - started on Reddit,,99,12,transtwin,2021-03-20 18:41:14,https://www.vice.com/en/article/5dpxvq/this-transparency-project-is-creating-a-massive-collection-of-police-data,0,datasets
gj488a,Which companies received COVID stimulus in SQL form,"[https://www.dolthub.com/repositories/Liquidata/covid-stimulus-watch](https://www.dolthub.com/repositories/Liquidata/covid-stimulus-watch)

Thanks to the folks at COVID Stimulus Watch. 

[https://data.covidstimuluswatch.org/prog.php?detail=opening](https://data.covidstimuluswatch.org/prog.php?detail=opening)",97,10,timsehn,2020-05-13 18:08:55,https://www.reddit.com/r/datasets/comments/gj488a/which_companies_received_covid_stimulus_in_sql/,0,datasets
gfioy3,TED Talks – Ultimate Dataset,"[Kaggle link](https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset)

I created a Python scraper – [(TEDscraper)](https://github.com/corralm/TEDscraper) – to scrape TED talk data including transcripts in over 100 languages from TED.com. 

I published the datasets for 12 languages on Kaggle [TED - Ultimate Dataset](https://www.kaggle.com/miguelcorraljr/ted-ultimate-dataset).

Hopefully this is useful for someone looking to do some NLP or other analysis.",103,7,corralm,2020-05-08 00:23:38,https://www.reddit.com/r/datasets/comments/gfioy3/ted_talks_ultimate_dataset/,0,datasets
f7nlpg,SETI Researchers Release Petabytes of Data in the Search For Aliens,,99,5,calufa,2020-02-22 04:33:35,https://twitter.com/nius_tv/status/1230851689948884992,0,datasets
bg3tuj,22 years ( 1995 - 2017 ) worth of news scrapped from Wikipedia ( with dates ) · GitHub,,97,4,pncnmnp,2019-04-22 16:04:06,https://gist.github.com/pncnmnp/3321df2e82eb9b8b1b2f59131c7144b2,0,datasets
n3ph2d,Coronavirus Datsets,"Carried on from [Second Discussion Thread](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/)(Archived)
> Carried on from [Original Thread](https://www.reddit.com/r/datasets/comments/exnzrd/coronavirus_datasets/)(Archived)
> 
> > You have probably seen most of these, but I thought I'd share anyway:
> 
> > **Spreadsheets and Datasets:**
> > 
> > * [https://www.worldometers.info/coronavirus/](https://www.worldometers.info/coronavirus/)
> > * [John Hopkins University Github](https://github.com/CSSEGISandData/2019-nCoV) confirmed case numbers.
> > * [Google Sheets From DXY.cn](https://docs.google.com/spreadsheets/d/1jS24DjSPVWa4iuxuD4OAXrE3QeI8c9BC1hSlqr-NMiU/edit#gid=1187587451) (Contains some patient information \[age,gender,etc\] )
> > * [Kaggle Dataset](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset)
> > * [Strain Data](https://github.com/nextstrain/ncov) repo
> > * [https://covid2019.app/](https://covid2019.app/)  (Google Sheets, thanks /u/supertyler)
> > * [ECDC](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) (Daily Spreadsheets, Thanks /u/n3ongrau)
> > 
> > **Other Good sources:**
> > 
> > * [BNO](https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/) Seems to have latest number w/ sources. (scrape)
> > * [What we can find out on a Bioinformatics Level](https://innophore.com/2019-ncov/)
> > * [DXY.cn Chinese online community for Medical Professionals](https://ncov.dxy.cn/ncovh5/view/pneumonia) \*translate page.
> > * [John Hopkins University Live Map](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)
> > * [Mutations](https://nextstrain.org/ncov) (thanks /u/Mynewestaccount34578)
> > * [Protein Data Bank File](https://3dprint.nih.gov/discover/3DPX-012867)
> > * [Early Transmission Dynamics](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) Provides statistics on the early cases, median age, gender etc.
> > 
> > **\[IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* [*SARS-CoV-2*](https://en.wikipedia.org/wiki/2019_novel_coronavirus)*. Many datasets will show a spike on that date*.\]
> > 
> > **There have been a bunch of great comments with links to further resources below!**  
> > \[Last Edit: 15/03/2020\]

- [COVID-19 Mobility Data Aggregator](https://github.com/ActiveConclusion/COVID19_mobility)  [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/g0rwpih/)
- [County level mask mandate data set(US)](https://docs.google.com/spreadsheets/d/1CYmIiNDeBrUcVyazo21jCXyE4eemE1hmc71jCOnoGW8/edit#gid=796552815) [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/g0pz1qw/)  
- [NYT county level cases and mask usage](https://github.com/nytimes/covid-19-data) [^[source ^comment]](https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/gdk385v/)
- Please check the comments of the previous threads for more datasets.",99,32,hypd09,2021-05-03 06:21:17,https://www.reddit.com/r/datasets/comments/n3ph2d/coronavirus_datsets/,0,datasets
guxy4j,"~ 148 000 000 coronavirus related tweets, n-gram counts, hashtag counts, and tweet ids by date. January 17th - May 31st",,96,3,turnip_cakes,2020-06-02 01:41:39,https://github.com/delvinso/covid19_one_hundred_million_unique_tweets,0,datasets
dutrsa,"Data Set for Members of Congress, what they voted Yes/No To, and their Associated Funding from different lobbyist groups","I remember seeing a while back during the vote on Net Neutrality a list of representatives and the money they accepted from Telecom and their vote for/against net neutrality. Most they were against net neutrality also had some funding from telecom lobbyists.

I was wondering if there's a similar set of data with other issues, not just net neutrality. It would be interesting to use this data to help voters make decisions on representatives.

If the data doesn't exist, i wonder if it would be possible to aggregate two lists of data (I.e. congress votes and their funding sources)

I'm a software engineer by nature, and not super familiar with the data science side of things. I'd love to be able to put together a website that displays this data.",99,5,The-FrozenHearth,2019-11-11 15:19:02,https://www.reddit.com/r/datasets/comments/dutrsa/data_set_for_members_of_congress_what_they_voted/,0,datasets
8v6ta9,Microsoft releases Footprints of 125 millions buildings in US,,94,3,ashwinids,2018-07-01 02:14:29,https://github.com/Microsoft/USBuildingFootprints,0,datasets
gvndth,100 million csv format data from 1988 to 2019 Japan trade statistics,,100,2,zanjibar,2020-06-03 04:58:07,https://www.kaggle.com/zanjibar/100-million-data-csv,0,datasets
ede38h,"[dataset] 68149 portraits tagged by height, weight, race, eye color, hair color, sex, age, name, and marks/scars (x-post /r/computervision)",,93,1,asdohajsldasd,2019-12-20 18:38:21,https://www.kaggle.com/davidjfisher/illinois-doc-labeled-faces-dataset,0,datasets
e7wcln,"Dataset: The adult lifespan of 115,650 European nobles from 800 to 1800.",,93,4,smurfyjenkins,2019-12-08 17:16:32,https://www.openicpsr.org/openicpsr/project/100492/version/V1/view;jsessionid=F18C09149C5E10E42484410E90D4A9F3,0,datasets
elso8t,"The Vatican Library has Digitizes Tens of Thousands of Manuscripts, Books, Coins, and More and they are now online.",,99,4,cavedave,2020-01-08 13:25:57,https://digi.vatlib.it/,0,datasets
ecaz7o,"Common Voice: an open source, multi-language dataset of voices by Mozilla",,98,6,cavedave,2019-12-18 10:40:49,https://voice.mozilla.org/en/datasets,0,datasets
31nyfm,Amazon's huge list of free datasets,,96,5,None,2015-04-06 20:01:49,https://aws.amazon.com/datasets,0,datasets
an53a6,"70,000 high quality faces with varying ages, ethnicities, and image backgrounds",,93,0,elilev3,2019-02-04 19:12:04,https://github.com/NVlabs/ffhq-dataset,0,datasets
1uyd0t,"200,000+ Jeopardy! Questions in a JSON file","[Here's](https://drive.google.com/file/d/0BwT5wj_P7BKXb2hfM3d2RHU1ckE/view?usp=sharing) a json file containing 216,930 Jeopardy questions, answers and other data.  See the bottom of the post for a CSV verson.  The format of the json
object is described below.  Questions were obtained by crawling www.j-archive.com [OC]

According to j-archive, the total number of Jeopardy! questions over the show's span
(as of this post) is 252,583 - so this is approximately 83% of them. In particular, 
around the last two years of game play are missing.

The json file is an unordered list of questions where each question has

* 'category' : the question category, e.g. ""HISTORY""

* 'value' : $ value of the question as string, e.g. ""$200""

     Note: This is ""None"" for Final Jeopardy! and Tiebreaker questions

* 'question' : text of question
	
     Note: This sometimes contains hyperlinks and other things messy text such
                       as when there's a picture or video question

* 'answer' : text of answer

* 'round' : one of ""Jeopardy!"",""Double Jeopardy!"",""Final Jeopardy!"" or ""Tiebreaker""
			
     Note: Tiebreaker questions do happen but they're very rare (like once every 20 years)

* 'show_number' : string of show number, e.g '4680'

* 'air_date' : the show air date in format YYYY-MM-DD

I'd love to hear of anything that gets done with this! Let me know! :D

I was thinking it would be cool to have a site where you could sort/search all of the questions
in a bunch of different ways, such as only $1000+ questions or only category ""WORLD CAPITALS"".
Also, somehow using this for a reddit bot could be interesting with the right idea.

LINK UPDATED AGAIN, OCT 2021:

(Google Drive): [JSON](https://drive.google.com/file/d/0BwT5wj_P7BKXb2hfM3d2RHU1ckE/view?usp=sharing&resourcekey=0-1abK4cJq-mqxFoSg86ieIg), [CSV](https://drive.google.com/file/d/0BwT5wj_P7BKXUl9tOUJWYzVvUjA/view?usp=sharing&resourcekey=0-uFrn8bQkUfSCvJlmtKGCdQ)",92,21,trexmatt,2014-01-11 11:13:40,https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/,0,datasets
1bo6b2s,Why use R instead of Python for data stuff?,Curious why I would ever use R instead of python for data related tasks.,96,77,Nickaroo321,2024-03-26 12:09:30,https://www.reddit.com/r/datasets/comments/1bo6b2s/why_use_r_instead_of_python_for_data_stuff/,0,datasets
q1450h,Pandora Papers - making actual data available?,"The news are all over the place, the data isn't. If it is a leak, some of the data should be already freely available. Or is a leak of the leak needed?

Did Panama Papers data ever become public?

[ICIJ - Data Analysis of Pandora Papers](https://www.icij.org/investigations/pandora-papers/about-pandora-papers-leak-dataset/)",93,10,alfa1381,2021-10-04 12:08:50,https://www.reddit.com/r/datasets/comments/q1450h/pandora_papers_making_actual_data_available/,0,datasets
ogg5pm,10 Open Data Sources You Wish You Knew,,90,7,_paige_joseph,2021-07-08 20:50:13,http://omnisci.link/ques8j,0,datasets
lbz0rc,"awesome-democracy-data: a curated list of awesome data sources related to elections, electoral reforms, and democratic political systems.",,89,2,andrewcstewart,2021-02-03 21:36:56,https://github.com/andrewcstewart/awesome-democracy-data,0,datasets
iqmlc0,A handful of datasets for teaching data science and data visualization,,96,2,cavedave,2020-09-11 08:24:00,https://observablehq.com/@mkfreeman/teaching-datasets,0,datasets
hlpblu,"Dataset: All 43 very large mass atrocities perpetrated by governments or non-state actors since 1945 with at least 50,000 civilian fatalities",,96,4,smurfyjenkins,2020-07-05 16:23:06,https://journals.sagepub.com/doi/full/10.1177/0022343319900912,0,datasets
151wjsa,4.5M headlines between 2007-2023 (From 10 major news sites),"Some context: I'm a high school student with an interest in data, and I wanted to explore how political bias has changed over the past decade or so. 

I ended up scraping headlines from the past 15 years from major publications (New York Times, CNN, FOX, New York Post, BBC, Washington Post, USA Today, Daily Mail, CNBC, and The Guardian)

&#x200B;

Here's the link if you're interested!

https://www.kaggle.com/datasets/jordankrishnayah/45m-headlines-from-2007-2022-10-largest-sites",93,14,imJordanNYC,2023-07-17 09:02:26,https://www.reddit.com/r/datasets/comments/151wjsa/45m_headlines_between_20072023_from_10_major_news/,1,datasets
kqxl6l,611 text datasets in 467 languages in the new v1.2 release of HuggingFace datasets library,"[HuggingFace](https://huggingface.co), a Natural Language Processing startup has just release the  v1.2 of its text datasets library with:

* 611 datasets that can be downloaded to be ready to use in one line of python,
* 467 languages covered, 99 with at least 10 datasets
* efficient pre-processing to free the user from memory constraints.

Repository: [https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)

From the [README.md of the repo](https://github.com/huggingface/datasets):

🤗Datasets is a lightweight python library providing two main features:

* one-line dataloaders for many public dataset: one liners to download and pre-process any of the 611 public datasets (in 467 languages and dialects!) explorable and searchable [here](https://huggingface.co/datasets). With a  command like squad\_dataset = load\_datasets(""squad""), any of these datasets is ready to use in a dataloader for Numpy/Pandas/PyTorch/TensorFlow/JAX,
* efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as local datasets in CSV/JSON/text files. With simple commandes like tokenized\_dataset = dataset.map(tokenize\_function) a dataset is efficiently prepared for inspection, evaluation or training of a predictive model.

Some additional links from the [README](https://github.com/huggingface/datasets): [🎓 **Documentation**](https://huggingface.co/docs/datasets/) [🕹 **Colab tutorial**](https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) [🔎 **Find a dataset in the Hub**](https://huggingface.co/datasets) [🌟 **Add a new dataset to the Hub**](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md)",88,3,Thomjazz,2021-01-05 12:30:33,https://www.reddit.com/r/datasets/comments/kqxl6l/611_text_datasets_in_467_languages_in_the_new_v12/,1,datasets
gedfjp,400 NLP Datasets,,90,3,Seankala,2020-05-06 04:40:43,https://datasets.quantumstat.com/,0,datasets
eh5cxo,NBA Games dataset : from 2004 season to 2019,"Hi everyone,

This weekend I uploaded a new dataset into **Kaggle** regarding NBA Games, you can find games stats, ranking, players statistics from 2004 season to december 2019. [NBA games dataset link](https://www.kaggle.com/nathanlauga/nba-games)

*I will try to maintain it every month.*

You  can find more informations about data collection on my GitHub  repository here : [Github nba-predictor repo link](https://github.com/Nathanlauga/nba-predictor)

If you have any suggestions I will gladly read them and try to improve the dataset.",93,6,nathan_lauga,2019-12-29 12:20:55,https://www.reddit.com/r/datasets/comments/eh5cxo/nba_games_dataset_from_2004_season_to_2019/,0,datasets
e4n5dc,Nifty Pandas Trick: Your dataset has many columns and you want to ensure the correct data types,https://twitter.com/justmarkham/status/1192794326763474944,91,3,superconductiveKyle,2019-12-01 20:28:07,https://www.reddit.com/r/datasets/comments/e4n5dc/nifty_pandas_trick_your_dataset_has_many_columns/,0,datasets
c24ak4,[Dataset] Image classification dataset for porn/ nudity/ nsfw detection,"Hi all, I worked on collecting porn classification data for last 3 - 4 months. I uploaded this at https://archive.org/details/NudeNet_classifier_dataset_v1

The pre-trained model on this dataset is available at https://github.com/bedapudi6788/NudeNet",96,31,winchester6788,2019-06-18 16:23:08,https://www.reddit.com/r/datasets/comments/c24ak4/dataset_image_classification_dataset_for_porn/,0,datasets
911k2j,I submitted my first paper with open data...the paper got rejected because of the data I shared,,93,15,cavedave,2018-07-22 22:14:51,https://twitter.com/kaitlynmwerner/status/1021047716355493889,0,datasets
52mzib,17 places to find datasets for data science projects,,92,8,jaypeedevlin,2016-09-13 21:19:57,https://www.dataquest.io/blog/free-datasets-for-projects/,0,datasets
1bqg65,"You can haz datasets! We now have over 4M financial, economic, and social datasets available.","Our DaaS platform Quandl is a free and open index of currently over 4 million datasets that is growing daily. We also released a Python package to go with our R, MATLAB, and excel ones this week. They allow easy API access to every single 

http://www.quandl.com/

Any suggestions on which type of datasets to try and obtain next are very appreciated. You can message me or list them here.",94,14,Kalemic,2013-04-05 14:31:55,https://www.reddit.com/r/datasets/comments/1bqg65/you_can_haz_datasets_we_now_have_over_4m/,0,datasets
sut3x7,This data set contains everything necessary to render a version of the Motunui island featured in the 2016 film “Moana” from Walt Disney Animation Studios.,,89,3,ercohn,2022-02-17 16:40:46,https://www.disneyanimation.com/resources/moana-island-scene/,0,datasets
m8jr0e,List of over 350 datasets,Here is a list of over [350 Datasets](https://datagious.com/datasets/). Looks like the majority are free to use. I have some friends using the free ones for test projects.,90,9,datagal23,2021-03-19 15:29:26,https://www.reddit.com/r/datasets/comments/m8jr0e/list_of_over_350_datasets/,0,datasets
jm46xu,"Over 250 000 000 unique coronavirus/covid associated tweets, n-gram counts, hashtag counts, and tweet ids by day, collected since January 17th","Hi all,

I've been collecting covid-19 related tweets since mid January and have amassed over 250 000 000 unique tweets. Various statistics such as n-gram counts (uni, bi and tri) and hashtag counts have been processed and published in my GitHub repo for anyone to analyze: https://github.com/delvinso/covid19_unique_tweets.

If you have any questions, comments and/or criticisms on how the dataset can be maintained going forward, please let me know.

A previous poster mentioned they would like to see multi-dimensional data added - I'm not sure how feasible that would be given that user IDs would have to be published, but I would definitely be able to look into adding additional information on a per-tweet basis if requested!",91,3,turnip_cakes,2020-11-01 15:36:37,https://www.reddit.com/r/datasets/comments/jm46xu/over_250_000_000_unique_coronaviruscovid/,0,datasets
d9ycp1,Github Releases Dataset Of Six Million Methods From Open Source Projects For CodeSearchNet Challenge," [Introducting The Github CodeSearchNet Challenge](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/?utm_campaign=1569513857&utm_medium=social&utm_source=twitter&utm_content=1569513857)

Searching for code to reuse, call into, or to see how others handle a problem is one of the most common tasks in a software developer’s day. However, search engines for code are often frustrating and never fully understand what we want, unlike regular web search engines. We started using modern machine learning techniques to improve code search but quickly realized that we were unable to measure our progress. Unlike natural language processing with [GLUE](https://gluebenchmark.com/) benchmarks, there is no standard dataset suitable for code search evaluation.

We collected a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. We used our [TreeSitter](http://tree-sitter.github.io/tree-sitter/) infrastructure for this effort, and we’re also releasing our [data preprocessing pipeline](https://github.com/github/CodeSearchNet/tree/master/function_parser) for others to use as a starting point in applying machine learning to code. While this data is not directly related to code search, its pairing of code with related natural language description is suitable to train models for this task. Its substantial size also makes it possible to apply high-capacity models based on modern [Transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) architectures.

Our fully preprocessed CodeSearchNet Corpus is available for [download on Amazon S3](https://github.com/github/CodeSearchNet#downloading-data-from-s3), including:

**Six million methods overall**

**Two million of which have associated documentation (docstrings, JavaDoc, and more)**

Metadata that indicates the original location (repository or line number, for example) where the data was found",91,3,mystikaldanger,2019-09-27 10:18:55,https://www.reddit.com/r/datasets/comments/d9ycp1/github_releases_dataset_of_six_million_methods/,0,datasets
8p4cuz,"Washington Post's dataset of 52,000 criminal homicides, drawing from the 50 largest U.S. cities and spanning a decade",,91,14,danwin,2018-06-06 21:02:21,https://github.com/washingtonpost/data-homicides,0,datasets
8fbjta,Which College Majors Marry Each Other?,,91,12,cavedave,2018-04-27 13:15:27,https://familyinequality.wordpress.com/2018/04/23/theology-majors-marry-each-other-a-lot-but-business-majors-dont-and-other-tales-of-bas-and-marriage/,0,datasets
4ufm58,Pornhub Comments,"[Pornhub Dataset](https://github.com/cdipaolo/hub-db)

I used the comments dataset to build a twitter bot using markov chains. It says really nasty things. [The results are extermely not safe for work. >_<](https://twitter.com/BotSoNasty)",92,8,still-standing,2016-07-24 23:16:06,https://www.reddit.com/r/datasets/comments/4ufm58/pornhub_comments/,0,datasets
k7apq3,I created a dataset of mostly EDM/Trap songs for a genre classification model. 42k+ songs!,"The full list of genres included in the CSV are Trap, Techno, Techhouse, Trance, Psytrance, Dark Trap, DnB (drums and bass), Hardstyle, Underground Rap, Trap Metal, Emo, Rap, RnB, Pop and Hiphop.

I know, weird choice of genres. But I collected this data to make a genre classifier for Corpse Husband's music, which is weird music. I put up a video [explaining my methodology for the model and why I built it](https://www.youtube.com/watch?v=VTU6Jla70VY&ab_channel=vastava) if you're interested.

Here's the [link to the CSV on github](https://raw.githubusercontent.com/vastava/data-science-projects/master/spotify-genre-classifier/data/genres_v2.csv). I used the Spotify API to collect this data, so the columns are the predefined set of audio features provided by Spotify (tempo, time signature, 'danceability', etc.).",87,13,vastava_viz,2020-12-05 16:42:35,https://www.reddit.com/r/datasets/comments/k7apq3/i_created_a_dataset_of_mostly_edmtrap_songs_for_a/,1,datasets
gakear,The British Museum Just Made 1.9M Stunningly Detailed Images Free Online,,87,2,surlyq,2020-04-29 23:31:45,https://www.vice.com/en_us/article/akw57z/the-british-museum-just-made-19m-stunningly-detailed-images-free-online,0,datasets
bs0zmz,German Holocaust archive puts millions of documents online,,90,3,cavedave,2019-05-23 09:54:17,https://www.latimes.com/world/la-fg-holocaust-documents-20190521-story.html,0,datasets
8stafx,Announcing Microsoft Research Open Data - Datasets by Microsoft Research now available in the cloud,,91,9,myinnerbanjo,2018-06-21 16:29:15,https://www.microsoft.com/en-us/research/blog/announcing-microsoft-research-open-data-datasets-by-microsoft-research-now-available-in-the-cloud/,0,datasets
277a33,Google has its own dedicated search for datasets...,,94,9,a6man,2014-06-03 14:05:03,https://www.google.com/cse/publicurl?cx=002720237717066476899:v2wv26idk7m,0,datasets
rwrpeg,A Beginner's Guide to Clean Data (online book),,87,11,cavedave,2022-01-05 17:09:51,https://b-greve.gitbook.io/beginners-guide-to-clean-data/,0,datasets
n9fp5e,2M rows of 1-min S&P bars (12 years of stock data),,87,6,kissingskeletons,2021-05-10 20:49:25,https://kaggle.com/gratefuldata/intraday-stock-data-1-min-sp-500-200821,0,datasets
mm9zry,"Historical and spatial dataset of 166,140 post offices that operated in the United States between 1639-2000",,85,6,cavedave,2021-04-07 19:33:40,https://twitter.com/historying/status/1377336343676608513,0,datasets
esdcsn,Cost of Living Index by City 2020,"I made a csv from [Cost of Living Index by City 2020.](https://www.numbeo.com/cost-of-living/rankings.jsp)

You can download it [here](https://filebin.net/n5bfebcaegcc1kx3/Cost_of_living_2020.csv?t=0qsv5u1a).",87,8,zagyvaTibor,2020-01-22 15:14:38,https://www.reddit.com/r/datasets/comments/esdcsn/cost_of_living_index_by_city_2020/,0,datasets
cl4ogt,"The U.S. judicial branch maintains a spreadsheet of biographical and career data of every U.S. federal judge (~3,700 rows)",,89,2,danwin,2019-08-02 14:15:39,https://www.fjc.gov/history/judges/biographical-directory-article-iii-federal-judges-export,0,datasets
c2kkce,2.5 years of World of Warcraft Auction House snapshots,"tl;dr Torrents for 2.5 years worth of ~hourly snapshots of every US server's AH: http://xen.im/~aheadley/wow-auctions/

I've been collecting hourly snapshots of auction house data from World of Warcraft (US servers only) for about 2.5 years now (starting from Oct 19 2016) and have about 1.1TB of data as ~1.8M bz2 compressed JSON files: ([sample file](https://gist.github.com/aheadley/4f037e32c874c1ae7584a0c78fb5cf26)). The torrents are seperated by server (technically server group that shares an AH) because trying to make one large torrent of over 2M files did not work very well. There are some gaps due to power outages and the like but it is mostly complete. I am currently seeding it from my home connection (200mbit upload) and will for at least a few months. Some stats on the files so anyone interested can make appropriate preparations for the download:

  - Total size of all files combined: 1.1TB
  - Total number of files combined: ~2.1M
  - Average size per server/server group: ~9GB
  - Number of files per server/server group: ~17k

EDIT: Made a zip file of all the torrents for convenience: http://xen.im/~aheadley/wow-auctions/all-torrents.zip",91,19,not_that_guy_either,2019-06-19 18:16:53,https://www.reddit.com/r/datasets/comments/c2kkce/25_years_of_world_of_warcraft_auction_house/,0,datasets
7wzuct,Data Journalism Site Fivethirtyeight's Data,,87,7,cavedave,2018-02-12 10:21:45,https://data.fivethirtyeight.com/,0,datasets
gi6282,DataGene: A Python Package to Identify How Similar Datasets are to one Another,"If you work with synthetic and generated datasets, this tool can be extremely useful. It is also helpful if you train models and want to ensure your traning, validation, and test sets have similar characteristics. 

The framework includes transformation from tensors, matrices, and vectors.  It includes a range of encodings and decompositions such as Gramian Angular Encoding, Recurrence Plot, Markov Transition Fields, Matrix Product State, CANDECOMP, and Tucker Decompositions.

After encoding and decoding transformations have been performed, you can choose from a range of distance metrics to calculate the similarity across various datasets.

In addition to the 30 or so transformations, there are 15 distance methods. The first iteration, focuses on time series data. All feedback appreciated. GitHub [link](https://github.com/firmai/datagene), Colab [link](https://colab.research.google.com/drive/1QSDTKvNiwc1IRCX_VYr9TRFusdX1gLMM?usp=sharing)

It starts off with transformations:

    datasets = [org, gen_1, gen_2]
    
    def transf_recipe_1(arr):
      return (tran.pipe(arr)[tran.mrp_encode_3_to_4]()
                [tran.mps_decomp_4_to_2]()
                [tran.gaf_encode_2_to_3]()
                [tran.tucker_decomp_3_to_2]()
                [tran.qr_decomp_2_to_2]()
                [tran.pca_decomp_2_to_1]()
                [tran.sig_encode_1_to_2]()).value
    
    recipe_1_org,recipe_1_gen_1,recipe_1_gen_2 = transf_recipe_1(datasets)

This operation chains 7 different transformations across all datasets in a given list. Output dimensions are linked to input dimensions.

After encoding and decoding transformations have been performed, you can choose from a range of distance metrics to calculate the similarity across datasets.

## Model (Mixed)

The model includes a transformation from tensor/matrix (the input data) to the local shapley values of the same shape, as well as tranformations to prediction vectors, and feature rank vectors.

`dist.regression_metrics()` \- Prediction errors metrics.

`mod.shapley_rank()` \+ `dist.boot_stat()` \- Statistical feature rank correlation.

`mod.shapley_rank()` \- Feature direction divergence. (NV)

`mod.shapley_rank()` \+ `dist.stat_pval()` \- Statistical feature divergence significance. (NV)

## Matrix

Transformations like Gramian Angular Field, Recurrence Plots, Joint Recurrence Plot, and Markov Transition Field, returns an image from time series. This makes them perfect candidates for image similarity measures. From this matrix section, only the first three measures, take in images, they have been tagged (IMG). From what I know, image similarity metrics have not yet been used on 3D time series data. Furthermore, correlation heatmaps, and 2D KDE plots, and a few others, also work fairly well with image similarity metrics.

`dist.ssim_grey()` \- Structural grey image similarity index. (IMG)

`dist.image_histogram_similarity()` \- Histogram image similarity. (IMG)

`dist.hash_simmilarity()` \- Hash image similarity. (IMG)

`dist.distance_matrix_tests()` \- Distance matrix hypothesis tests. (NV)

`dist.entropy_dissimilarity()` \- Non-parametric entropy multiples. (NV)

`dist.matrix_distance()` \- Statistical and geometrics distance measures.

## Vector

`dist.pca_extract_explain()` \- PCA extraction variance explained. (NV)

`dist.vector_distance()` \- Statistical and geometric distance measures.

`dist.distribution_distance_map()` \- Geometric distribution distances feature map.

`dist.curve_metrics()` \- Curve comparison metrics. (NV)

`dist.curve_kde_map()` \- dist.curve\_metrics kde feature map. (NV)

`dist.vector_hypotheses()` \- Vector statistical tests.",86,4,OppositeMidnight,2020-05-12 06:58:20,https://www.reddit.com/r/datasets/comments/gi6282/datagene_a_python_package_to_identify_how_similar/,0,datasets
fl5wh5,Crowd-sourced COVID-19 Dataset Tracking Involuntary Government Restrictions (TIGR) Need Help!,,87,11,locallyoptimal,2020-03-19 07:20:58,https://github.com/rexdouglass/TIGR,0,datasets
7kvuy9,The 20 Most popular pin numbers. Cover over 35% of all cards,,89,9,cavedave,2017-12-19 20:07:36,http://www.datagenetics.com/blog/september32012/index.html,0,datasets
6o053s,"I Made a CSV file containing the following data for 170 countries: GDP per Capita, Literacy rate, Infant Mortality per 1000 live births, Fraction of GDP based on agriculture, Population, and Net migration",,87,14,jbp12,2017-07-18 11:24:07,http://www.sharecsv.com/s/4165c9b03d9fffdef43a3226613ff37c/Countries.csv,0,datasets
xz430j,Dataset of 135k subreddits and 497k links between them,,88,4,uglyasablasphemy,2022-10-08 21:26:29,https://github.com/fedecalendino/reddit-graph-releases,0,datasets
gvahts,"Dataset of 316 Million Tweets from January 27 to May 31st, with top bigrams, trigrams, terms and daily count of hashtags, mentions and emojis","As mentioned below, these are COVID19 filtered tweets. This dataset is updates 3 times a week with major releases every Sunday night: [https://doi.org/10.5281/zenodo.3723939](https://doi.org/10.5281/zenodo.3723939)",84,6,jmbanda,2020-06-02 16:28:52,https://www.reddit.com/r/datasets/comments/gvahts/dataset_of_316_million_tweets_from_january_27_to/,0,datasets
g8e9cq,"County Data Scraped from 3,142 Wikipedia Pages into 214 Columns",,88,5,dbabbitt,2020-04-26 13:22:13,https://github.com/dbabbitt/notebooks/blob/master/covid19/saves/csv/counties_df.csv,0,datasets
fr88zu,COVID-19 News dataset,,84,1,Gbellport,2020-03-29 16:10:00,https://www.covid19-archive.com/?repost,0,datasets
g0cwvo,Pokemon Dataset (until 8th Generation),"Hello everybody,

I am learning Data Analysis and wanted to make a small test with a reduced Database so I have put together a Pokemon Dataset with all Pokemon with Pokedex National Number until 8th Generation with around 50 features. I hope you find it useful.

[https://www.kaggle.com/mariotormo/complete-pokemon-dataset-updated-090420](https://www.kaggle.com/mariotormo/complete-pokemon-dataset-updated-090420)",84,12,None,2020-04-13 05:44:18,https://www.reddit.com/r/datasets/comments/g0cwvo/pokemon_dataset_until_8th_generation/,0,datasets
f4klcy,"Trained Model On ""Huge New NSFW Dataset for Content Filtering""","Original dataset link and articles:[https://medium.com/@Synced/huge-new-nsfw-dataset-for-content-filtering-c8e6a323d67c](https://medium.com/@Synced/huge-new-nsfw-dataset-for-content-filtering-c8e6a323d67c)

Added ""neutral"" and ""drawings"" from [https://github.com/alex000kim/nsfw\_data\_scraper](https://github.com/alex000kim/nsfw_data_scraper).

Trained CNN model with class-weights (on sparse categorical cross-entropy), stratified hold-out performance 1.73 (unweighted).

Download link for Keras model and labels: [https://drive.google.com/file/d/17WL-zfvS2nXCqLAb-JboupL8BIq2x7QK/view?usp=sharing](https://drive.google.com/file/d/17WL-zfvS2nXCqLAb-JboupL8BIq2x7QK/view?usp=sharing)

Preprocessing steps include: Resize the image to (224,224) \[BGR format\] (stretch resize or resize to fit within 224x224px), subtract the following values from the three channels respectively : \[107, 122, 149\]. The input is expected in float16 format after subtraction (the original float value is not scaled between 0.0-1.0, rather it's a cast from a uint8).",87,11,sushantt,2020-02-16 03:11:36,https://www.reddit.com/r/datasets/comments/f4klcy/trained_model_on_huge_new_nsfw_dataset_for/,0,datasets
f399pt,Article: Self-driving car dataset missing labels for hundreds of pedestrians,,84,11,cavedave,2020-02-13 12:30:06,https://blog.roboflow.ai/self-driving-car-dataset-missing-pedestrians/,0,datasets
7swy3j,Data Science at the Command Line. Free Ebook,,86,2,cavedave,2018-01-25 15:48:17,http://www.datascienceatthecommandline.com/,0,datasets
6f9862,"All publicly available tweets from Donald Trump's @realdonaldtrump Twitter Account (26,240 tweets in JSON format)","~~https://files.pushshift.io/requests/realdonaldtrump.gz~~  (See below)

**Edit** 

It's been brought to my attention that some of those id's are not trumps -- so I will have to update the dump tomorrow -- but in the meantime, you should filter out all tweets that don't have a screen_name of realdonaldtrump.

Thanks!

**Edit 2:** 

Please use this file until I can figure out why the other id's were bad:  https://files.pushshift.io/requests/realdonaldtrump_22893.gz  (there are 22,893 from him in this dump).
",87,41,Stuck_In_the_Matrix,2017-06-04 19:26:50,https://www.reddit.com/r/datasets/comments/6f9862/all_publicly_available_tweets_from_donald_trumps/,0,datasets
jiz80l,Open source database of police officers and police agencies in the US.,,87,3,VitalMeme,2020-10-27 11:17:48,https://rateourcops.org,0,datasets
hbb31q,Pornhub semi-realtime Dataset 2020,"A particular dataset that has been demanded in a handful of posts; yet never properly addressed.

Here is the download link for their always-up-to-date-and-available-datadump.

BEHOLD: [https://www.pornhub.com/files/pornhub.com-db.zip](https://www.pornhub.com/files/pornhub.com-db.zip)",87,18,theDataPiano,2020-06-18 09:01:32,https://www.reddit.com/r/datasets/comments/hbb31q/pornhub_semirealtime_dataset_2020/,0,datasets
b6ct36,Introducing datafix.io: a service that connects people with unclean data to people who want to clean data,"I built datafix.io to connect data cleaners with people who need their data cleaned. There is a reward system for cleaning data as well as QA (with its own reward system). What do you all think?

http://datafix.io",85,36,Mjjjokes,2019-03-28 01:38:00,https://www.reddit.com/r/datasets/comments/b6ct36/introducing_datafixio_a_service_that_connects/,0,datasets
aggsmg,President Signs Government-wide Open Data Bill,,81,20,QuirkySpiceBush,2019-01-16 03:07:16,https://www.datacoalition.org/press-releases/president-signs-government-wide-open-data-bill/,0,datasets
g98m2w,"Yahoo Knowledge Graph Announces COVID-19 Dataset, API, and Dashboard with Source Attribution",,84,2,rosaliebee,2020-04-27 20:23:43,https://yahoodevelopers.tumblr.com/post/616566076523839488/yahoo-knowledge-graph-announces-covid-19-dataset,0,datasets
fciet0,120 years of NOAA's global hourly surface data in an easily queryable format.,"The NOAA globally hour surface dataset has been has been imported into dolt and modeled in a manner that should be easily queryable.  This data is fundamental in training weather forecast models, and analyzing climate change.  Take a look at the blog post covering the dataset.

[https://www.dolthub.com/blog/2020-03-02-noaa-global-hourly-surface-data/](https://www.dolthub.com/blog/2020-03-02-noaa-global-hourly-surface-data/)

&#x200B;

Source Data: [https://www.ncei.noaa.gov/data/global-hourly/](https://www.ncei.noaa.gov/data/global-hourly/)

On Dolthub: [https://www.dolthub.com/repositories/Liquidata/noaa](https://www.dolthub.com/repositories/Liquidata/noaa)",85,1,dolt-bheni,2020-03-02 20:30:06,https://www.reddit.com/r/datasets/comments/fciet0/120_years_of_noaas_global_hourly_surface_data_in/,0,datasets
f7clb1,ImageMonkey - a public open source image dataset with now over 100k public domain images.,,85,8,bbernhard1,2020-02-21 15:23:06,https://imagemonkey.io/blog/general/2020/02/09/ImageMonkey-100k-0.html,0,datasets
c8vj1z,Personality Trait Dataset (n>40000): how well can you predict gender from personality traits?,"I was able to get to 80% using an SVM classifier (train on 20,000, test on 10,000). Can anyone do better than that? 

[http://openpsychometrics.org/\_rawdata/16PF.zip](http://openpsychometrics.org/_rawdata/16PF.zip)",80,13,bulldawg91,2019-07-03 23:53:37,https://www.reddit.com/r/datasets/comments/c8vj1z/personality_trait_dataset_n40000_how_well_can_you/,0,datasets
96rxzk,Academic Torrents – Making 27TB of research data available,"Just saw this one on Hacker News. Enjoy! Note, I have filtered for datasets only however there appears to be videos too.

[http://academictorrents.com/browse.php?search=&c6=1](http://academictorrents.com/browse.php?search=&c6=1)",84,4,None,2018-08-12 20:11:59,https://www.reddit.com/r/datasets/comments/96rxzk/academic_torrents_making_27tb_of_research_data/,0,datasets
uzbwg5,United States School shootings dataset,"Sorry if this comes across as insensitive, my heart goes out to the victims and families.
Is there a dataset or database for this? 
Kinda curious if there are places/things to avoid, in case I have kids, and have to send them to school…",82,35,Adiguno,2022-05-28 00:35:37,https://www.reddit.com/r/datasets/comments/uzbwg5/united_states_school_shootings_dataset/,0,datasets
lpykgn,"4,000 Priceless Scrolls, Texts & Papers From the University of Tokyo Have Been Digitized & Put Online",,83,2,cavedave,2021-02-22 20:13:37,https://www.openculture.com/2021/02/4000-priceless-scrolls-texts-papers-from-the-university-of-tokyo-have-been-digitized.html,0,datasets
f6e4m1,"50k Genome + Phenotype Database with cancer, heart disease variants and polygenic risk scores",,81,2,bonkerfield,2020-02-19 17:34:14,https://data.color.com/v2/cancer.html,0,datasets
efwkm1,Great Time-Series Datasets,"I’m not sure if this has been posted before, but the free-online book Forecasting: Principles and Practices is not only a great resource, but it comes with so many interesting time series datasets that can all be loaded as ready-to-go time series objects by simply importing the fpp2 package in R.

The book:
https://otexts.com/fpp2/

A pdf describing the datasets:
https://cran.r-project.org/web/packages/fpp2/fpp2.pdf",82,7,MrBurritoQuest,2019-12-26 15:18:35,https://www.reddit.com/r/datasets/comments/efwkm1/great_timeseries_datasets/,0,datasets
doc7sa,Google Cloud: Weather and Climate Public Datasets,,84,2,fhoffa,2019-10-28 17:40:23,https://cloud.google.com/public-datasets/weather/,0,datasets
9se6tv,Data and code behind the articles and graphics at FiveThirtyEight,,81,11,None,2018-10-29 15:41:56,https://github.com/fivethirtyeight/data,0,datasets
5u77ln,The Greatest Public Datasets for AI,,83,1,cavedave,2017-02-15 13:03:48,https://medium.com/startup-grind/fueling-the-ai-gold-rush-7ae438505bc2#.ph466awqh,0,datasets
11ma39o,"""The Office"" Dataset at Hugging Face",,84,2,cavedave,2023-03-08 22:20:31,https://huggingface.co/datasets/jxm/the_office_lines,0,datasets
112dg9r,Tech Layoff Dataset from https://layoffs.fyi/,"Hello

Thought I'd share some of the data from the tech layoff website [https://layoffs.fyi/](https://layoffs.fyi/)**.**

&#x200B;

[**https://docs.google.com/spreadsheets/d/1xn3JojJ7VTkrdbJ925fzLjxCkXE5lJpA/edit?usp=sharing&ouid=113583157892418671984&rtpof=true&sd=true**](https://docs.google.com/spreadsheets/d/1xn3JojJ7VTkrdbJ925fzLjxCkXE5lJpA/edit?usp=sharing&ouid=113583157892418671984&rtpof=true&sd=true)",80,5,tapatiolookalikeguy,2023-02-14 19:11:13,https://www.reddit.com/r/datasets/comments/112dg9r/tech_layoff_dataset_from_httpslayoffsfyi/,0,datasets
r7a3hk,[self-promotion] My friends and I built a site that lets you use 100+ data APIs without code,"Hi everyone!

My friends and I built [databar.ai](https://www.databar.ai), a free no-code API tool that lets you get datasets from all over the web without code (works for \~100 APIs right now). We started it out as a side-project/internal tool and thought that others might find it useful too.

Basically all you do is pick an API you want to use (for example Coin Gecko or [Data.gov](https://Data.gov)), customize your request with parameters, and get a clean, structured csv/xslx file in return.

Right now you can get datasets on:

\- Anything relating to crypto (social media stats, market caps, volumes, ROIs, etc.)

\- Finance (public financials, IPO data, transcripts, technicals, DCFs)

\- Scraped data (news articles/blogs, App store reviews)

\- Public data (crime, education, environment, etc.)

\- Anything to do with COVID

You don't need to know how to work with APIs to use it and we're wondering if there are any features people would prefer - mostly posting for feedback/ideas. Figured r/datasets is the best place to ask, please let me know if I'm posting in the wrong place!",84,25,Fun-Ant-5808,2021-12-02 15:37:27,https://www.reddit.com/r/datasets/comments/r7a3hk/selfpromotion_my_friends_and_i_built_a_site_that/,0,datasets
oqrg5u,DataSet of Tokyo 2020 (2021) Olympics,"Hi guys, I'm sharing the dataset of the 2020(2021) Olympics in Tokyo, It contains details about the Athletes, the countries they representing, details about events, coaches, genders participating in each event, etc. 

[https://www.kaggle.com/arjunprasadsarkhel/2021-olympics-in-tokyo](https://www.kaggle.com/arjunprasadsarkhel/2021-olympics-in-tokyo)",81,15,Nathuphoon,2021-07-24 14:47:57,https://www.reddit.com/r/datasets/comments/oqrg5u/dataset_of_tokyo_2020_2021_olympics/,0,datasets
jbskee,"College course data including title, text description, credits, and other information for nearly 9,000 courses",,81,10,m-hoff,2020-10-15 17:52:14,https://www.kaggle.com/mzh5263/college-course-data,0,datasets
hyz23v,3000 declassified CIA maps,,79,9,cavedave,2020-07-27 19:30:23,https://catalog.archives.gov/search?q=*:*&f.ancestorNaIds=305945,0,datasets
hxly7f,The Largest CAD Dataset Released With 15M Designs,,81,6,analyticsindiam,2020-07-25 12:30:18,https://analyticsindiamag.com/the-largest-cad-dataset-released-with-15m-designs/,0,datasets
ggjcu9,FooDB : the world’s largest and most comprehensive resource on food constituents,,79,8,tilttovictory,2020-05-09 17:14:00,https://foodb.ca/,0,datasets
gberl8,Audi Autonomous Driving Dataset,,78,9,RankLord,2020-05-01 09:08:16,https://www.a2d2.audi/a2d2/en.html,0,datasets
5uwbqy,Treasury.io - searchable database of the Federal government's daily cash spending and borrowing,,85,1,surlyq,2017-02-19 03:11:18,http://treasury.io/,0,datasets
5pn2re,"TSA Claims Data - A list of everything lost in luggage, with airport, item, and cost categories.",,81,10,tornato7,2017-01-23 07:08:27,https://www.dhs.gov/tsa-claims-data,0,datasets
ic0sq7,We collected a 100k+ rows of a balanced topic labeled news dataset,,81,2,kotartemiy,2020-08-18 13:16:54,https://newscatcherapi.com/blog/topic-labeled-news-dataset,0,datasets
fmlge6,All Dunkin Donuts stores in the USA,"I scraped together a list of all the Dunkin stores that are in the US and some of their properties (phone, address, lat/long, fax!?, etc.). Check it out [here](https://www.kaggle.com/jpbulman/usa-dunkin-donuts-stores).",80,15,wpisad,2020-03-21 20:12:18,https://www.reddit.com/r/datasets/comments/fmlge6/all_dunkin_donuts_stores_in_the_usa/,0,datasets
9oai7u,List of Public Datasets,,80,9,thebeefbandit,2018-10-15 07:18:18,https://www.reddit.com/r/SQL/comments/9o3o0o/list_of_awesome_public_datasets/,0,datasets
ql9u2b,10 Billion YouTube comments and 100 Million Channel Names,,80,7,makeworld,2021-11-02 17:47:50,https://archive.org/details/yt-metadata-project,0,datasets
mb7rmr,"New York complaint counts of 83,000 police officers","I spent the better part of today figuring out how to extract data from [this post](https://www.reddit.com/r/news/comments/mar14p/new_york_makes_complaint_records_of_83000_police/?sort=confidence) ([dashboard](https://www1.nyc.gov/site/ccrb/policy/MOS-records.page)).

I ran into issues with extra details about the complaints so they are not included (see below if you want to help ;)), but I have ~83k rows of the form:

    |   id | command   | lastname   | firstname   | rank           |   shieldno |   subcomplaints |   totcomplaints | active
---:|-----:|:----------|:-----------|:------------|:---------------|-----------:|----------------:|----------------:|:---------
  0 |    2 | 001 DET   | Accardi    | Richard     | Police Officer |      18377 |               0 |               2 | True
  1 |    5 | 001 DET   | Bascom     | Alistair    | Detective      |       4557 |               0 |               3 | True
  2 |    6 | 001 DET   | Bernabe    | Giocardo    | Police Officer |      25607 |               0 |               2 | True
  3 |   15 | 001 DET   | Dicandia   | Ricardo     | Police Officer |      18015 |               0 |               4 | True
  4 |   19 | 001 DET   | Eysel      | Robert      | Police Officer |      03057 |               1 |               1 | True

The column names should be clear after looking at the dashboard, except `subcomplaints` is short for ""substantiated complaints"". The maximum number of complaints for one person is 39. The distribution looks like [this](https://i.imgur.com/9y6ZMW3.png).



The gzipped newline-delimited json file can be downloaded [here](https://gofile.io/d/mv8E9k) (1.4MB).


For anyone who wants to struggle some more to get the remaining columns, [this](https://pastebin.com/uevsnFgM) is the Python script I used to generate the above dataframe. 

Some notes after trying to reverse engineer how the PowerBI dashboard displays the queried data. A maximum of 30k rows can be fetched at once, so a `ResumeToken` needs to be supplied with the output of a previous pagination to iterate over results. If the headers don't work, you may have to open up developer tools and fiddle with them. The rows returned from a query do NOT have the same length. There is a value returned per row (`R`) which specifies a bit string for the columns that have been suppressed. If a column is suppressed, it will need to be forward filled (`ffill`) from the previous rows. Integer placeholders are used to look up values from a dictionary (per column) also returned with a query.",81,7,typhoidisbad,2021-03-23 05:43:46,https://www.reddit.com/r/datasets/comments/mb7rmr/new_york_complaint_counts_of_83000_police_officers/,0,datasets
kolznk,"""The Pile"": An 800GB Dataset of Diverse Text for Language Modeling (EleutherAI)",,82,1,gwern,2021-01-01 23:43:14,https://pile.eleuther.ai/,0,datasets
6tpuor,U.S. judge says LinkedIn cannot block startup from public profile data,,78,14,Vardox,2017-08-14 22:07:19,http://www.reuters.com/article/us-microsoft-linkedin-ruling-idUSKCN1AU2BV?il=0,0,datasets
fksoax,"54,443 individual Coronavirus case details from multiple sources, updating hourly","We have been working on gathering individual case details for Coronavirus. Last week, we announced we gathered Hong Kong, Singapore, and South Korea case details: [https://www.dolthub.com/blog/2020-03-12-coronavirus-case-details-dolt/](https://www.dolthub.com/blog/2020-03-12-coronavirus-case-details-dolt/)

We now have 54,443 individual case details (of lower quality than the government data). Read about it here: [https://www.dolthub.com/blog/2020-03-18-coronavirus-case-details-using-branches/](https://www.dolthub.com/blog/2020-03-18-coronavirus-case-details-using-branches/)",77,20,timsehn,2020-03-18 16:47:47,https://www.reddit.com/r/datasets/comments/fksoax/54443_individual_coronavirus_case_details_from/,0,datasets
6tj0y9,"Data.gov: Over 197,000 datasets and APIs from government programs covering nearly every possible area open for access & use.",,79,5,justinwzig,2017-08-14 00:46:02,https://www.data.gov/,0,datasets
1wjd38,Academic Torrents - Currently making 205.68GB of research data available.,,80,1,res0nat0r,2014-01-30 06:18:45,http://academictorrents.com/,0,datasets
o1w4of,Update: Google Playstore Apps with 2.3million app data on Kaggle,"Google  Playstore dataset is now available with double the data (2.3 Million)  android application data and a new attribute stating the scraped date  time in Kaggle.

Get it here: [https://www.kaggle.com/gauthamp10/google-playstore-apps/](https://www.kaggle.com/gauthamp10/google-playstore-apps/)",77,2,gauthamp10,2021-06-17 12:32:10,https://www.reddit.com/r/datasets/comments/o1w4of/update_google_playstore_apps_with_23million_app/,0,datasets
hd37rg,Large COVID-19 Data List,"After doing a lot of data collecting for work I thought I would gather and share all the data sources and datasets I have seen regarding the COVID-19 pandemic.

&#x200B;

[https://github.com/weber-stephen/COVID-19-Data-List](https://github.com/weber-stephen/COVID-19-Data-List)

&#x200B;

I put it on Github in the hopes that people would add to it and we could all benefit.",79,9,weber_stephen,2020-06-21 08:12:46,https://www.reddit.com/r/datasets/comments/hd37rg/large_covid19_data_list/,0,datasets
fugprg,[COVID-19] Google's COVID-19 Community Mobility Reports in Google sheets,"[Total data by countries](https://docs.google.com/spreadsheets/d/1d9t7xg-lUPEUArTsc_wMOGl1XfzpXmeWcALx7v58KcU/edit?usp=sharing)

[Detailed data by countries](https://docs.google.com/spreadsheets/d/1fuV8AKwSjIh9Pswb_XTC0UFaoFPMBbz9YHAZ8TScAQc/edit?usp=sharing)

[Detailed data for the US](https://docs.google.com/spreadsheets/d/1pxuTu10uO7MsBaKA554XSuCpnF--FTqwdnl_sUHfWro/edit?usp=sharing)

All data scraped from  [Google's COVID-19 Community Mobility Reports](https://www.google.com/covid19/mobility/)

[GitHub](https://github.com/ActiveConclusion/COVID19_mobility) with Python script and reports in different formats

UPDATE: Data updated 10.04.2020",80,46,Active-Conclusion,2020-04-03 21:18:00,https://www.reddit.com/r/datasets/comments/fugprg/covid19_googles_covid19_community_mobility/,0,datasets
eurwqu,Dataset: Vanity license plate applications from the California DMV,,74,7,cavedave,2020-01-27 18:20:32,https://github.com/veltman/ca-license-plates,0,datasets
envocp,"Paris Musées Releases 100,000+ Works Into the Public Domain",,78,1,surlyq,2020-01-12 23:43:18,https://creativecommons.org/2020/01/10/paris-musees-releases-100000-works-into-the-public-domain/,0,datasets
cwvi65,Datasets for Top 10 Visualizations Every Data Scientist Should Know,,77,2,castanan2,2019-08-29 03:38:14,https://towardsdatascience.com/10-viz-every-ds-should-know-4e4118f26fc3,0,datasets
cgxnj3,Dataset | Lyft Level 5 (Autonomous Driving),,77,3,thetylerwolf,2019-07-23 20:01:38,https://level5.lyft.com/dataset/,0,datasets
bl2y52,Dataset of 2775+ Dungeons and Dragons characters,,78,5,mouse_Brains,2019-05-05 20:58:09,https://github.com/oganm/dnddata,0,datasets
99rq3n,Chicago Parking Ticket dataset -- 28 million parking tickets since 2007,,78,0,danwin,2018-08-23 22:06:48,https://www.propublica.org/nerds/download-chicago-parking-ticket-data,0,datasets
8ctj51,Officers' names. Disciplinary charges. Punishments. This huge trove of secret disciplinary records is available and searchable for the first time.,,76,13,debian420,2018-04-17 03:21:09,http://data.buzzfeed.com/projects/2018-04-nypd/nypd-discipline.csv,0,datasets
57adjs,Data for everyone: Nasa just opened its entire research library!,,77,5,rpezzotti,2016-10-13 13:58:28,http://futurism.com/free-science-nasa-just-opened-its-entire-research-library-to-the-public/,0,datasets
4cidpn,Google launches Public Datasets program,,78,3,Habitual_Emigrant,2016-03-30 00:31:02,https://cloud.google.com/bigquery/public-data/,0,datasets
shbzwe,"Is there a ""master list"" of places to look for datasets anywhere? Newbie here, sorry if it's a silly question","Hi! I've started a (basic) course in data analysis, and the final assessment is a project requiring ""real world data"". I'm honestly not sure where to start looking for what I want (once I come up with an idea of what I want to analyse heh, but that's not your problem!).

Is there a FAQ/list of popular data sources? I don't necessarily need it to be free, but I'm not a millionaire either, so go easy on me :)

Thanks!

EDIT: Editing in the list so far. So many wonderful resources I never knew about! Thank you all, such a cool community :)

https://www.google.com/ - might seem obvious, but actually it's great if you use the right terms. A search for ""data ireland population yearly"" got me a relevant hit immediately.

https://www.kaggle.com/

https://github.com/awesomedata/awesome-public-datasets

https://components.one/datasets/

https://www.kdnuggets.com/datasets/index.html

https://opendatainception.io/

https://data.opendatasoft.com/explore/dataset/open-data-sources%40public/table/?sort=code_en

https://databar.ai/

https://us.gov/

https://datasetsearch.research.google.com/ - a search engine for data sets, very cool!

https://www.reddit.com/r/statistics/ - the sidebar has a ""data"" section which lists more resources for sets

https://osf.io/

https://healthdatascience.substack.com/p/best-public-datasets-for-public-health-225

https://huggingface.co/datasets

Will keep adding if people keep suggesting :)",77,33,Istrakh,2022-01-31 20:58:09,https://www.reddit.com/r/datasets/comments/shbzwe/is_there_a_master_list_of_places_to_look_for/,0,datasets
mxcd9i,The Goodreads metadata collection (retired) and 51 million Amazon book reviews.,,79,4,shrine,2021-04-24 04:10:17,/r/Archiveteam/comments/mxccfp/the_goodreads_metadata_collection_retired_and_51/,0,datasets
ktghqg,Donald Trump’s tweets,Was wondering if anyone knew of where to find a list of all of Trump’s tweets?,76,9,sweatshirtjones,2021-01-09 01:07:00,https://www.reddit.com/r/datasets/comments/ktghqg/donald_trumps_tweets/,0,datasets
dtgciy,"Switzerland's official website offers different kinds of data, ranging from population to international disparities","I thought this could belong here.

https://www.bfs.admin.ch/bfs/en/home/statistics/catalogues-databases/tables.html",78,5,chinaexpl0it,2019-11-08 14:42:58,https://www.reddit.com/r/datasets/comments/dtgciy/switzerlands_official_website_offers_different/,0,datasets
dodcmr,Amazon Releases New Public Speech Data Set to Help Address “Cocktail Party” Problem,,79,5,cavedave,2019-10-28 18:56:01,https://developer.amazon.com/blogs/alexa/post/6963ff40-6e62-4d6a-975d-fea600affa46/amazon-releases-new-public-data-set-to-help-address-dinner-party-problem,0,datasets
6c7fu0,"Google releases dataset of 50M vector drawings, from Quick Draw Game!",,74,0,hlake,2017-05-20 00:18:09,https://quickdraw.withgoogle.com/data,0,datasets
uh6g2b,"Free zip code database, 800+ columns","Hey everyone! I created a website, www.EverythingByZipCode.com, a dataset that acts as a unified view of information for every US zip code - spanning across multiple public domains/APIs.

It’s a pet project I launched in February and have made 50 sales through just PPC ads, but would much rather receive qualitative feedback as opposed to paying customers at this point.

I have several updates rolling out in the next few weeks, but would love some input from folks like yourself that are obviously passionate about data analytics.

There’s other sites out that that sell the same premise, but cost more and only contain basic information from the Census. This dataset truly is more for less. Also, the other sites look like they haven't been updated in 20 years.

It has a lot of business use cases, but also acts as a great dataset to play around with, whether it be predictive modeling or data visualization.

Here’s a free promo code: REDDIT. I’ll be updating the promo code to 50% off after the first 50 customers, then 25% for the next 25. Like I mentioned earlier, it’s ideally a for-profit business :)

Note - If you sign up through the promo code for free, you’ll only have access to the current file and won’t receive updates.

If you use it, again, I’d greatly appreciate your feedback or a follow. Feel free to message me at @bresslertweets on Twitter!

Thanks so much!

David",77,34,dabressler,2022-05-03 02:49:37,https://www.reddit.com/r/datasets/comments/uh6g2b/free_zip_code_database_800_columns/,1,datasets
i7b9ay,I built an automatically-updating icon dataset of all circulating currencies [self-promotion],,79,0,corollari,2020-08-10 19:07:27,https://github.com/onramper/small-open-datasets/tree/master/currency-icons#readme,0,datasets
i099qb,[OC] StoryWrangler: Twitter Ngram Viewer -- https://storywrangling.org/,,74,1,lelky_g,2020-07-29 21:34:56,https://i.redd.it/ddibsy9rnld51.png,0,datasets
h96k4q,"Dataset: the first comprehensive long-run dataset of official international lending, covering 230,000 loans, grants and guarantees extended by governments, central banks, and multilateral institutions in the period 1790-2015.",,76,2,smurfyjenkins,2020-06-15 01:47:59,https://ae624e09-a-62cb3a1a-s-sites.googlegroups.com/site/christophtrebesch/HRT_Disasters.pdf?attachauth=ANoY7crj8B5iibb5Mc6-lkTB2me1Y8RzxaHzu7upWnFYhRNT4-r-yKy90yExU_9i8QNBTTtyyUI-0s9gFeV3na7iNM5t3MQII-vOKPC3_K-vJry7azdfLVPFETPpcX4j612EGNpl8SCaroGoAa5ojmKQpZR5V5XkTZd60DKgml_K8H6YxVZETF67yygNpdPmNl4YniTs6UAXnl44dKH0mqAOD2vJ2RVzBUpKhLzhUUIM5562vhxxjuE%3D&attredirects=0,0,datasets
euq7iq,448 million search engine queries from 2017-2019 with monthly search frequencies - 53GB,"448 million search terms along with the last 24 month's worth of per-month search frequencies. Given the market share of the search engine from which this data came, multiplying these monthly counts by about 15 should yield the total search volume across all search engines.

Uncompressed, the file is about 53GB. Compressed it's about 5GB.

Each line of the file is a JSON object that looks like this:

    {""_id"":{""$oid"":""5d4f768d272acc57122804a3""},""keyphrase"":""hackathon ideas"",""trend"":[60,90,110,20,90,90,100,30,30,60,50,40,60,80,120,70,80,100,70,70,40,50,60,50]}

You can ignore the \_id property (I'd ideally have removed that property when I exported the data). The two relevant properties are ""keyphrase"" and ""trend"". The numbers at the end of the ""trend"" array are the most recent frequencies. Each item in the ""trend"" array is the number of searches that month, globally. The data was collected a few months before the end of 2019.

Note that some of the keyphrases have 4 dollar-signs in them like this:

    ""keyphrase"":""hackathon$$$$hackathon""

I'm not sure what this means (it's just what the API returned), so I just ignored these in my analyses. It may have something to do with new advanced search engine features like knowledge graphs, but I don't know.

[https://archive.org/details/2019-search-engine-keyphrases.json](https://archive.org/details/2019-search-engine-keyphrases.json)

[https://www.kaggle.com/hofesiy/2019-search-engine-keywords](https://www.kaggle.com/hofesiy/2019-search-engine-keywords)

Posting with a throwaway. Please mirror it somewhere if you can (and post link in comments) in case it gets taken down for whatever reason. Hope some people find this useful/interesting!

**Edit**: The final month in the trends array is August 2019.",74,9,asdfasdfsdafs,2020-01-27 16:30:44,https://www.reddit.com/r/datasets/comments/euq7iq/448_million_search_engine_queries_from_20172019/,0,datasets
eplwct,I downloaded a dataset from Reddit and built a subreddit recommendation engine. Here is what it recommends for users of r/datasets:,,74,14,needDataInsights,2020-01-16 16:48:28,/r/RedditRecommender/comments/eplm9f/datasets/,0,datasets
d9mxiy,"I have compiled a dataset of 11062 Chinese characters, merged from 9933 most frequent ones and 8105 characters in Chinese General Standard. Every one of them has HSK level (if any), number of strokes, radical, pronounciation, and meaning (where possible). More information in the comments.",,76,21,areyde,2019-09-26 17:33:52,https://docs.google.com/spreadsheets/d/1SxoqHYYJOBF0TBHHkFJfwIR6RuQzfbr5c4wXn8cR54M/edit?usp=sharing,1,datasets
bkdl06,The Big Picture: Google Releases Massive Landmark Recognition Dataset,,74,1,Yuqing7,2019-05-03 21:14:40,https://medium.com/syncedreview/the-big-picture-google-releases-massive-landmark-recognition-dataset-aea81179ae5d,0,datasets
5m8ewp,"Internet Archive launches Trump Archive, with 700+ televised speeches, interviews, debates, and other news broadcasts related to President-elect Donald Trump",,77,0,danwin,2017-01-05 19:20:01,https://blog.archive.org/2017/01/05/internet-archives-trump-archive-launches-today/,0,datasets
vs9ol9,Database stolen from Shanghai Police for sale on the darkweb,,75,4,cavedave,2022-07-05 22:09:40,https://www.theregister.com/2022/07/05/shanghai_police_database_for_sell/,0,datasets
kjqu88,What are the best datasets for building a data visualisation portfolio?,"Hi,

I have recently developed an interest in data visualisation and EDA. I am currently trying to figure out what are some great datasets that I can put on my portfolio.",76,18,None,2020-12-25 01:37:45,https://www.reddit.com/r/datasets/comments/kjqu88/what_are_the_best_datasets_for_building_a_data/,0,datasets
ivb7je,Game changer: NASA data tool could revolutionize Amazon fire analysis,,72,1,cavedave,2020-09-18 18:05:25,https://news.mongabay.com/2020/09/game-changer-nasa-data-tool-could-revolutionize-amazon-fire-analysis/,0,datasets
dz4y08,First 2 billion primes keyed by ordinal,"We just published a dataset with the first 2 billion prime numbers, so you can run SQL queries about prime distribution:

    % dolt sql -q 'select * from primes where ordinal = 1000000;'
    +---------+----------+
    | ordinal | prime    |
    +---------+----------+
    | 1000000 | 15485863 |
    +---------+----------+
    dolt sql -q 'select * from primes where prime > 100000 and prime < 110000 limit 10;'
    +---------+--------+
    | ordinal | prime  |
    +---------+--------+
    | 9593    | 100003 |
    | 9594    | 100019 |
    | 9595    | 100043 |
    | 9596    | 100049 |
    | 9597    | 100057 |
    | 9598    | 100069 |
    | 9599    | 100103 |
    | 9600    | 100109 |
    | 9601    | 100129 |
    | 9602    | 100151 |
    +---------+--------+

We were dissatisfied with the existing mechanisms for distributing this info on the web (mostly giant text files), so hopefully someone finds this useful. Enjoy!

 [https://www.liquidata.co/blog/2019-11-19-primes-in-dolt/](https://www.liquidata.co/blog/2019-11-19-primes-in-dolt/)",74,7,zachm,2019-11-20 17:08:23,https://www.reddit.com/r/datasets/comments/dz4y08/first_2_billion_primes_keyed_by_ordinal/,0,datasets
dmdlnx,Names of all known varieties of marijuana labeled by strain (CSV),"I originally compiled this because I was curious if there was any correlation between strain name and species type (spoiler: no, if there is any predictive ability, its <5%). Maybe some else can make something cool with it, a markov chain generator or something.
---  


745 rows

Column 0: index (int)

Column 1: full strain name (string)

Column 2: species {indica, hybrid, sativa}

Column 3-7: tokenized strain (words)

Column 8: detailed species {indica, indica-hybrid, mixed-hybrid, sativa-hybrid, sativa}  


---
[Google Drive](https://drive.google.com/file/d/14Uwgv15IYg19Dh63iT-dEGL2z2CZkLsT/view?usp=sharing)
----  


Source: [Wikipedia.org](https://en.wikipedia.org/wiki/List_of_names_for_cannabis_strains)


Wikipedia's source: [LA Times 2014](https://graphics.latimes.com/marijuana-strains/)


LA Time's source: [Leafly.com](https://www.leafly.com)

---  
EDIT: Someone suggested just looking for linguistic patterns with LIWC software. If you want to do this, [here](https://drive.google.com/drive/folders/1Cfk0mXTuszRp1tAwPfeYXTbYfNORFfLR?usp=sharing) are the raw text files.",75,16,KnightlySir,2019-10-24 08:49:48,https://www.reddit.com/r/datasets/comments/dmdlnx/names_of_all_known_varieties_of_marijuana_labeled/,0,datasets
cfz309,"Dictionary crawler python code (Oxford, Longman, Cambridge, Webster, and Collins)","Hi everybody.

I just coded a Scrapy python project to crawl famous dictionaries (Oxford, Longman, Cambridge, Webster, and Collins), it is on my Github:

[https://github.com/kiasar/Dictionary\_crawler](https://github.com/kiasar/Dictionary_crawler)

&#x200B;

with this, you can create a lot of dictionary data if you want to.

Hope you like it.",75,16,kiasari,2019-07-21 13:39:24,https://www.reddit.com/r/datasets/comments/cfz309/dictionary_crawler_python_code_oxford_longman/,0,datasets
5cvnr6,I scraped all the 2016 election data.,,75,21,None,2016-11-14 12:20:05,https://github.com/Prooffreader/election_2016_data,1,datasets
xd9xkc,What is a dataset that you can’t believe is available to the public? Part 2,Part 1 (4 years old): [https://www.reddit.com/r/datasets/comments/akb4mr/what\_is\_a\_dataset\_that\_you\_cant\_believe\_is/](https://www.reddit.com/r/datasets/comments/akb4mr/what_is_a_dataset_that_you_cant_believe_is/),72,7,Coup1,2022-09-13 15:03:28,https://www.reddit.com/r/datasets/comments/xd9xkc/what_is_a_dataset_that_you_cant_believe_is/,0,datasets
vmx36m,Dataset of Roe v. Wade Tweets Labeled by Abortion Stance [self-promotion],,74,1,BB4evaTB12,2022-06-28 20:32:51,https://www.surgehq.ai/blog/dataset-of-roe-v-wade-tweets-labeled-by-abortion-stance,0,datasets
ilukxj,TheEconomist/covid-19-excess-deaths-tracker,,72,17,cavedave,2020-09-03 14:36:37,https://github.com/TheEconomist/covid-19-excess-deaths-tracker,0,datasets
faanl5,LEGO Price Data over time,,75,8,cavedave,2020-02-27 11:21:40,http://www.realityprose.com/what-happened-with-lego,0,datasets
enz4od,"23MB set of 235,668 reviews from 6 online black markets","Hi! I am the owner of Kilos, a search engine that indexes listings, vendors, reviews, and forum posts from online black markets. I am publishing some of the data I have scraped to see if anyone can reach any interesting conclusions from playing with it. I have quite a bit more data, which I will be posting in the coming days. Right now I have...

    Currently indexing 534,767 forum posts, 65,741 listings, 2,726 vendors, and 235,668 reviews from 6 markets and 6 forums.

You need Tor to download the dataset. Once you have the Tor browser bundle installed, you can find the data set here: http://lolwuc3342535625.onion/2020-01-13-reviews.csv . If someone could mirror this on a clearnet hosting site, I would appreciate that. I use Tor for everything and most file hosting websites will not allow me to upload over Tor.

Edit: /u/gwern has mirrored the data for me and you can now get it without Tor [here](https://www.gwern.net/docs/sr/2020-01-13-kilos-6dnms-reviews.csv.xz). Thanks /u/gwern!

The data is in the format

    site,vendor,timestamp,score,value_btc,comment

Site, vendor, and comment are strings. Site and vendor are both alphanumeric, while comment may have punctuation and whatnot. Line breaks are explicit ""\\n"" in the comment field, and the comment field has quotation marks around it to make it easier to sort through. All the data uses Latin characters only, no unicode. Timestamp is an integer indicating the number of seconds since the Unix epoch. Score is 1 for positive review, 0 for neutral review, and -1 for negative review. Value_btc is the bitcoin value of the product being reviewed, calculated at the time of the review.

There are some slight problems with the data set as a result of the pain that is scraping these marketplaces. All reviews from Cryptonia market have their timestamp as 0 because I forgot to decode the dates listed and just used 0 as a placeholder. Cryptonia reviews' score variable is unreliable, as I accidentally rewrote all scores to 0 on the production database. To correct for this, I rewrote the scores to match a sentiment analysis of the review text, but this is not a perfect solution, as some reviews are classified incorrectly. E.g. ""this **shit** is the **bomb**!"" might be classified negatively despite context telling us that this is a positive review.

There are a decent number of duplicates, some of which are proper (e.g. ""Thanks"" as a review appears many many times) and some of which are improper (detailed reviews being indexed multiple times by mistake).

Anyway if you can make any interesting inferences from this data, let me know! I am always looking to improve Kilos' display of datas. Right now, I am working on using polynomial regression to detect when vendors have padded their reviews with fake positives to improve their listing in search results. I would appreciate help with this if anyone can offer it.",76,11,EndlessMorning,2020-01-13 04:25:44,https://www.reddit.com/r/datasets/comments/enz4od/23mb_set_of_235668_reviews_from_6_online_black/,0,datasets
e2fz71,"""Data Is Plural"" Archive. Weekly mailing list of interesting datasets",,75,2,cavedave,2019-11-27 13:26:23,https://data.world/jsvine/data-is-plural-archive,0,datasets
asq6nu,I made a Python script to generate fake datasets optimized for testing machine learning/deep learning workflows.,,77,9,minimaxir,2019-02-20 16:17:45,https://github.com/minimaxir/ml-data-generator,0,datasets
a6pwdg,TWINT: Twitter scraping tool evading most API limitations,,76,9,srw,2018-12-16 15:37:45,https://github.com/twintproject/twint,0,datasets
7keo6g,Big Data Set for Crypto: CoinMarketCap,"Hi guys,

I have a huge data set from the past couple of months with the price of cryptocurrencies off of CoinMarketCap. The CoinMarketCap data is every five minutes. I collected data between August and November. The data is in csv format. Let me know if you are interested in the data in another format. https://github.com/iamrobinhood12345/coinmarketcap_data",75,15,Dotherightthing253,2017-12-17 16:28:45,https://www.reddit.com/r/datasets/comments/7keo6g/big_data_set_for_crypto_coinmarketcap/,0,datasets
6yt3og,"Every line from Every Episode of ""The Office (US)""",,76,9,misunderstoodpoetry,2017-09-08 06:32:33,https://docs.google.com/spreadsheets/d/18wS5AAwOh8QO95RwHLS95POmSNKA2jjzdt0phrxeAE0/edit?usp=sharing,0,datasets
orsjwc,[self-promotion] Covid-19 Cough Audio Classification Dataset,"Hello frens,  


I uploaded a dataset of 25,000 crowdsourced coughs, of which 2,800 were labelled by four medical experts with labels such as Healthy / Covid-19 as well as cough severity.  


It is available [here](https://www.kaggle.com/andrewmvd/covid19-cough-audio-classification).  


All credits go to the following authors:

> Orlandic, L., Teijeiro, T. & Atienza, D. The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms. *Sci Data* **8,** 156 (2021). https://doi.org/10.1038/s41597-021-00937-4",73,3,larxel,2021-07-26 05:51:01,https://www.reddit.com/r/datasets/comments/orsjwc/selfpromotion_covid19_cough_audio_classification/,0,datasets
l084rl,"1 year and over 300 000 000 coronavirus/covid-19 related tweets, collected since January 17th 2020!","Hi, all thought I'd share this since it's just been about a year since I've been collecting these tweets. Twitter doesn't allow the release of the actual tweets, but I have processed things such as N-gram (uni, bi and tri) and hashtag counts for anyone to analyze in my GitHub repo: https://github.com/delvinso/covid19_unique_tweets",71,3,turnip_cakes,2021-01-19 00:45:13,https://www.reddit.com/r/datasets/comments/l084rl/1_year_and_over_300_000_000_coronaviruscovid19/,0,datasets
g7dgtz,Trump's speeches dataset,The title says it all. I've found a database of tweets from Trump since 2009. Now i'm also looking for a dataset of campaing/presidential speeches that he has made. anything is welcome! Thank you :),72,19,PlatoTheSloth,2020-04-24 18:14:57,https://www.reddit.com/r/datasets/comments/g7dgtz/trumps_speeches_dataset/,0,datasets
dzjmdv,The Global Human Settlement Layer: an amazing new global population dataset,,72,1,cavedave,2019-11-21 13:53:30,http://www.statsmapsnpix.com/2016/10/the-global-human-settlement-layer.html,0,datasets
93racz,3 Million Russian Troll Tweets,,74,19,Mike_ZzZzZ,2018-08-01 17:54:32,https://github.com/fivethirtyeight/russian-troll-tweets/,0,datasets
7u17oo,"Python Tutorials on Web Scraping, Spreadsheets and some other dataset related tasks",,71,0,cavedave,2018-01-30 14:05:04,https://medium.mybridge.co/python-top-45-tutorials-for-the-past-year-v-2018-1b4d46c9e857,0,datasets
26su7y,"Hey, r/datasets! I've compiled a list of more than 100 freely available data sets, along with ideas for use and some bad drawings. Enjoy!",,76,4,adbge,2014-05-29 17:11:43,http://rs.io/2014/05/29/list-of-data-sets.html,0,datasets
1b9ihqa,"I made OMDB, the world's largest downloadable music database (154,000,000 songs)",,70,13,OatsCG,2024-03-08 07:38:49,https://github.com/OatsCG/OMDB,0,datasets
13gb8mo,A detailed shaded relief map of London rendered from Lidar data [OC],,74,3,cavedave,2023-05-13 08:02:56,https://i.redd.it/6f2ud7a59jza1.jpg,0,datasets
12booz4,A collection: Groovy Datasets for Test Databases,,72,1,yourbasicgeek,2023-04-04 16:40:18,https://redis.com/blog/datasets-for-test-databases/,0,datasets
iqy37j,"USPS Dataset: Pieces of Mail Handled, Number of Post Offices, Income, and Expenses Since 1789",,74,19,torrijasycafe,2020-09-11 20:01:28,https://about.usps.com/who-we-are/postal-history/pieces-of-mail-since-1789.htm,0,datasets
i5krmb,Coronavirus Datasets,"Carried on from [Original Thread](https://www.reddit.com/r/datasets/comments/exnzrd/coronavirus_datasets/)(Archived)

> You have probably seen most of these, but I thought I'd share anyway:

> **Spreadsheets and Datasets:**
> 
> * [https://www.worldometers.info/coronavirus/](https://www.worldometers.info/coronavirus/)
> * [John Hopkins University Github](https://github.com/CSSEGISandData/2019-nCoV) confirmed case numbers.
> * [Google Sheets From DXY.cn](https://docs.google.com/spreadsheets/d/1jS24DjSPVWa4iuxuD4OAXrE3QeI8c9BC1hSlqr-NMiU/edit#gid=1187587451) (Contains some patient information \[age,gender,etc\] )
> * [Kaggle Dataset](https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset)
> * [Strain Data](https://github.com/nextstrain/ncov) repo
> * [https://covid2019.app/](https://covid2019.app/)  (Google Sheets, thanks /u/supertyler)
> * [ECDC](https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide) (Daily Spreadsheets, Thanks /u/n3ongrau)
> 
> **Other Good sources:**
> 
> * [BNO](https://bnonews.com/index.php/2020/02/the-latest-coronavirus-cases/) Seems to have latest number w/ sources. (scrape)
> * [What we can find out on a Bioinformatics Level](https://innophore.com/2019-ncov/)
> * [DXY.cn Chinese online community for Medical Professionals](https://ncov.dxy.cn/ncovh5/view/pneumonia) \*translate page.
> * [John Hopkins University Live Map](https://gisanddata.maps.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6)
> * [Mutations](https://nextstrain.org/ncov) (thanks /u/Mynewestaccount34578)
> * [Protein Data Bank File](https://3dprint.nih.gov/discover/3DPX-012867)
> * [Early Transmission Dynamics](https://www.nejm.org/doi/full/10.1056/NEJMoa2001316) Provides statistics on the early cases, median age, gender etc.
> 
> **\[IMPORTANT UPDATE:** *From February 12th the definition of confirmed cases has changed in Hubei, and now includes those who have been clinically diagnosed. Previously China's confirmed cases only included those tested for* [*SARS-CoV-2*](https://en.wikipedia.org/wiki/2019_novel_coronavirus)*. Many datasets will show a spike on that date*.\]
> 
> **There have been a bunch of great comments with links to further resources below!**  
> \[Last Edit: 15/03/2020\]",75,34,hypd09,2020-08-07 19:52:26,https://www.reddit.com/r/datasets/comments/i5krmb/coronavirus_datasets/,0,datasets
ew2x0k,"This excellent CoronaVirus timseries is a google sheets workbook, but has no option to download. Is there some easy way to pull this down?",,73,18,mott_the_tuple,2020-01-30 09:40:27,https://docs.google.com/spreadsheets/d/1yZv9w9zRKwrGTaR-YzmAqMefw4wMlaXocejdxZaTs6w/htmlview?usp=sharing&sle=true#,0,datasets
dypqyz,How to create a machine learning dataset from scratch?,,71,0,tangoslurp,2019-11-19 19:57:04,https://towardsdatascience.com/how-to-create-a-machine-learning-dataset-from-scratch-6a24a9fd1673,0,datasets
d0kv9w,Facebook Introduces Dataset & Challenge to Counter DeepFakes,,70,5,Yuqing7,2019-09-06 18:53:44,https://syncedreview.com/2019/09/06/facebook-introduces-dataset-challenge-to-counter-deepfakes/,0,datasets
bsuhhc,py_ball: API wrapper in Python for NBA and WNBA data,"# [py_ball](https://github.com/basketballrelativity/py_ball)

Introducing py_ball, a Python API wrapper for the stats.nba.com and data.wnba.com APIs with a focus on NBA and WNBA applications. You can download the module with the link above or [here](https://pypi.org/project/py-ball/).

There are similar tools out there for accessing and analyzing basketball data, but py_ball adds both documentation ([here](https://github.com/basketballrelativity/py_ball/wiki) and [here](https://basketballrelativity.github.io/py_ball/_build/html/index.html)) along with a wide array of tutorials to make basketball analytics both accessible and approachable.

## NBA/WNBA Tutorials using py_ball

- [Franchise history exploration](https://github.com/basketballrelativity/franchise_history)
- [Live scoreboard](https://github.com/basketballrelativity/scoreboard)
- [Shot, assist, turnover, and fouls drawn charts](https://github.com/basketballrelativity/location_data)
- [Shot probability model](https://github.com/basketballrelativity/shot_probability)
- [Draft combine data](https://github.com/basketballrelativity/draft_combine)

I'm excited to hear any feedback related to the API wrapper or tutorials! I hope you enjoy it.

Also, you can follow me [@py_ball_](https://twitter.com/py_ball_) on Twitter or [@basketballrelativity](https://github.com/basketballrelativity) for new tutorials or development!",72,7,py_ball,2019-05-25 13:27:12,https://www.reddit.com/r/datasets/comments/bsuhhc/py_ball_api_wrapper_in_python_for_nba_and_wnba/,0,datasets
8tpcmb,"Making of a Chinese Characters dataset (15 million PNGs of 52,835 characters)","I just finished making a dataset of Chinese characters. 

Each file is a 28x28 PNG, the same as the CS231n example notMNIST data. 

It's just under 10 GB compressed. 

[https://medium.com/@peterburkimsher/making-of-a-chinese-characters-dataset-92d4065cc7cc](https://medium.com/@peterburkimsher/making-of-a-chinese-characters-dataset-92d4065cc7cc)

This could be used for making a better OCR or handwriting recognition program for Chinese Hanzi/Japanese Kanji. ",70,11,peterburk,2018-06-25 10:11:52,https://www.reddit.com/r/datasets/comments/8tpcmb/making_of_a_chinese_characters_dataset_15_million/,1,datasets
8isnek,List of every subreddit on Reddit,"Two files available:  

1)  A comprehensive file that lists ALL subreddits on Reddit (private, public, employee only, etc.) with ALL metadata.  [This file is big.](https://files.pushshift.io/reddit/subreddits/subreddits.json.gz)

2.)  Basic CSV file that lists all PUBLIC subreddits.  [This file is a lot smaller.](https://files.pushshift.io/reddit/subreddits/subreddits_basic.csv)

The format for the second file is:

 base10 id, base36 reddit id, creation epoch, subreddit name, number of subscribers (linux newline delimited)",70,19,Stuck_In_the_Matrix,2018-05-12 00:52:30,https://www.reddit.com/r/datasets/comments/8isnek/list_of_every_subreddit_on_reddit/,0,datasets
7projk,PlayerUnknown's Battleground (PUBG) Dataset recording over 65 Milion detailed deaths in over 720 thousand matches,,71,14,kpei1hunnit,2018-01-11 21:58:25,https://www.kaggle.com/skihikingkevin/pubg-match-deaths,0,datasets
5vq7w0,How to Extract Spotify Data (and show Radiohead are depressing),,73,6,cavedave,2017-02-23 12:54:11,http://rcharlie.com/2017-02-16-fitteR-happieR/,0,datasets
10yeaoe,A complete set of tweets in a day (375 million tweets),"With a globally coordinated effort of 80 scholars, this dataset collected all 375 million tweets published within a 24-hour time period starting on September 21, 2022. It is the first complete 24-hour Twitter dataset that is available to the public.

paper: [https://arxiv.org/abs/2301.11429](https://arxiv.org/abs/2301.11429)

dataset: [https://search.gesis.org/research\_data/SDN-10.7802-2516?doi=10.7802/2516](https://search.gesis.org/research_data/SDN-10.7802-2516?doi=10.7802/2516)

In compliance with Twitter’s terms of service, only tweet IDs are made publicly available.",69,3,avalanchesiqi,2023-02-10 02:51:09,https://www.reddit.com/r/datasets/comments/10yeaoe/a_complete_set_of_tweets_in_a_day_375_million/,0,datasets
kvle6f,"Parler data dump, post Capitol raid until AWS dropped them...","Information from the team who [collected it](http://archiveteam.org/index.php?title=parler).   
I'm not very skilled with the programming side of tech, but I know how to operate a computer very well. Did CompTIA A Plus, but not much else tech wise.     

I understand that 80-70 TB of Parler data is out there.    
I understand that [parler-trick.zip](https://github.com/d0nk/parler-tricks) is a reverse engineered python based Parler API.     
I have downloaded parler-tricks.zip, and I have downloaded the latest version of python.    

I am unsure how to proceed in running the API and accessing the data.     

I'd love to hear if I'm misunderstanding anything.",67,42,Pyrollamasteak,2021-01-12 05:33:08,https://www.reddit.com/r/datasets/comments/kvle6f/parler_data_dump_post_capitol_raid_until_aws/,0,datasets
its2x4,League of Legends 60k Games High Elo 10.16,"I have collected about 60k League of Legends games using the API from Riot. These games are from West Europe, North America, and South Korea regions, and they all are high elo games (the League Points average of the players is Master, Grandmaster or Challenger). They are from the Solo Queue.

After splitting the data into train and test datasets, I have been able to predict the outcome of a games with a 69.75% accuracy, only using the summoners involved, and the champions and positions that they played.

Which accuracy would be acceptable for this type of project? (League of Legends games should be highly unpredictable.)

I attach a link to the CSV file (217Mb). Some Korean summoners' names are bugged because of decoding issues. Sorry about it! Contact me if you have any doubt.

[https://www.kaggle.com/fernandorubiogarcia/league-of-legends-high-elo-patch-1016](https://www.kaggle.com/fernandorubiogarcia/league-of-legends-high-elo-patch-1016)",69,22,frubio4,2020-09-16 09:10:39,https://www.reddit.com/r/datasets/comments/its2x4/league_of_legends_60k_games_high_elo_1016/,0,datasets
hmubhh,What are some fun random things to collect data/statistics on in your everyday life?,I’m new to the whole data thing and am currently learning PowerBI. I’d just like to know some things I can make data sets with!,72,24,trader2488,2020-07-07 13:14:42,https://www.reddit.com/r/datasets/comments/hmubhh/what_are_some_fun_random_things_to_collect/,0,datasets
flifr2,"Is anyone tracking the data sources of COVID-19 data, and tracking metadata about it?","We may be seeing a once in a lifetime chance to quantify datasets coming from various public and private data sources. Tracking the rate at which data is released and reported could prove incredibly insightful.

Is anyone looking at the metadata of all of this stuff? This has the making of a Big Data gold mine.",74,20,JavaCrunch,2020-03-19 21:41:14,https://www.reddit.com/r/datasets/comments/flifr2/is_anyone_tracking_the_data_sources_of_covid19/,0,datasets
e3feqj,Datahub for Field Experiments in Economics and Public Policy. More than 140 datasets on poverty and development,,70,1,smurfyjenkins,2019-11-29 14:28:47,https://dataverse.harvard.edu/dataverse/DFEEP,0,datasets
dk2a7x,"480,000 Rotten Tomato Critic Reviews","I scraped nearly all Rotten Tomatoes critic reviews. I kept 240,000 fresh, and 240,000 rotten reviews. They are labeled. Column 1 = Freshness [fresh, rotten], column 2 = text of review. 

Available on my [Google Drive](https://drive.google.com/file/d/1N8WCMci_jpDHwCVgSED-B9yts-q9_Bb5/view?usp=sharing)",67,6,GeneralReposti_Bot,2019-10-19 11:03:06,https://www.reddit.com/r/datasets/comments/dk2a7x/480000_rotten_tomato_critic_reviews/,0,datasets
c65bcu,Machine Learning and Data Science with Kaggle,,69,0,AlwaysLearner123,2019-06-27 13:08:12,https://www.youtube.com/watch?v=6INvZ1TJ7b8&t=16s,0,datasets
8iftwb,Learn To Create Your Own Datasets — Web Scraping in R,,72,10,cavedave,2018-05-10 15:38:19,https://towardsdatascience.com/learn-to-create-your-own-datasets-web-scraping-in-r-f934a31748a5,0,datasets
10omhu9,Complete FIFA 23 dataset available on Kaggle,"Hi r/datasets,

&#x200B;

In case anyone is interested in analysing and exploring the latest FIFA 23 dataset, I uploaded at the following link a set of csv files that allow to compare the sofifa player data from FIFA 15 until the latest FIFA 23:

[https://www.kaggle.com/stefanoleone992/fifa-23-complete-player-dataset](https://www.kaggle.com/stefanoleone992/fifa-23-complete-player-dataset)

&#x200B;

Here there is an analysis of players and teams that could serve you as starting point to see how the files can be read and used:

[https://www.kaggle.com/stefanoleone992/fifa-23-players-lineup-visualization-and-more](https://www.kaggle.com/stefanoleone992/fifa-23-players-lineup-visualization-and-more)

&#x200B;

Have fun, and do not hesitate to let me know any further improvement of the files.

&#x200B;

Thanks in advance!",68,3,stexo92,2023-01-29 23:42:27,https://www.reddit.com/r/datasets/comments/10omhu9/complete_fifa_23_dataset_available_on_kaggle/,0,datasets
rkgnl4,[self-promotion] S&P 500 stock and company data (daily updated),"Hello frens,  


I made an auto-updating crawler to get all stock and company data available for all companies in the S&P 500 index every day.  


It is available [here](https://www.kaggle.com/andrewmvd/sp-500-stocks).

  
Soon I will add some automated reports as well, so that high quality investment information is free and open to anyone.  


Hope it helps,  
Cheers",67,15,larxel,2021-12-20 06:29:07,https://www.reddit.com/r/datasets/comments/rkgnl4/selfpromotion_sp_500_stock_and_company_data_daily/,0,datasets
i38hzo,I aggregated every 2020 election forecast I could find into one sheet.,,67,28,OtherSideReflections,2020-08-03 23:48:15,https://docs.google.com/spreadsheets/d/1tjCPiCnhdpUkPkmqCufbh7Kyz2PDYwYNxPv7TZUf9l4/edit?usp=sharing,0,datasets
c43lyk,ourworldindata.org,[This website](https://ourworldindata.org/) is full of data stories and interesting datasets. I could spend so much time there. Enjoy !,70,8,thduv,2019-06-23 12:09:22,https://www.reddit.com/r/datasets/comments/c43lyk/ourworldindataorg/,0,datasets
8zfh78,I'm worried about the rise of fake datasets. Has anyone else seen this yet?,"Like fake news that panders to our human instinct of confirmation bias I'm worried about the spread of fake datasets intentionally crafted to dupe data scientists or spread disinformation. A possible example here: [https://twitter.com/derhorus\_x/status/1010118894219153410](https://twitter.com/derhorus_x/status/1010118894219153410)

Does this community have a protocol or a flair in place to tag such occurrences if they occur?

Edit: \`Fake News\` means different things to different people. Academically, it has been broken down into to categories: Disinformation and Misinformation. The 3 month old missing dog poster is misinformation if it was found shortly after the poster was hung up. Disinformation is intentionally crafting a message, a delivery medium, or false information with the intention of manipulating, deceiving, or crafting a person's worldview. According Eric Ross Weinstein's interpretation, Fake News takes the following four shapes: Algorithmic, Narrative,  Institutional, and factually false.

The same can be said about any form of information. Including a dataset. How a data is collected in a dataset can cause it to be slightly \`fake\`. A french politician a couple of years ago famously claimed in a stump speech that 100% of their middle east immigrants were criminals.  This is factually true if you believe that persons who cross the border seeking asylum as a criminal activity. Consider how if I wanted to convince you that anyone from California and New York is a rapist. I simply put a heat map showing the state of origin of all the convicted rapists in the united states. Clearly California and New York are rapists and should be stopped. We should build a wall to keep all the rapists out. In response to this I give you an [XKCD comic](https://xkcd.com/1138/).",71,23,gabefair,2018-07-16 22:20:26,https://www.reddit.com/r/datasets/comments/8zfh78/im_worried_about_the_rise_of_fake_datasets_has/,0,datasets
8t5bpt,FBI Releases Study of Pre-Attack Behaviors of Active Shooters,,71,7,cavedave,2018-06-22 22:17:17,https://www.fbi.gov/file-repository/pre-attack-behaviors-of-active-shooters-in-us-2000-2013.pdf/view,0,datasets
8ej8vh,"The 446 People, Places and Things Donald Trump Has Insulted on Twitter: A Complete List",,69,5,cavedave,2018-04-24 09:55:11,https://www.nytimes.com/interactive/2016/01/28/upshot/donald-trump-twitter-insults.html?mtrref=undefined&gwh=C25AB3AA33353CCD3B7C5BCD11FD8005&gwt=pay,0,datasets
5vxru9,I've created a site where you can download fake datasets for learning purposes.,"Hi. This is my first time posting in this subreddit, so if this is the wrong place to post this, please tell me. I've been needing datasets lately to  check my very simple implementations of popular machine learning algorithms. So, I decided to create this site. I thought this would be helpful for other people as well.

PS: There is a bug with formatting when downloading txt files but none when downloading csv files.
EDIT: I have now added an option where you can download the data in json file format.
Here is the link: https://fakedatagenerator.herokuapp.com/",71,12,CommentKillah,2017-02-24 14:38:52,https://www.reddit.com/r/datasets/comments/5vxru9/ive_created_a_site_where_you_can_download_fake/,0,datasets
x45lw2,I built a free tool for public company data sets,"Get financial data historically for public companies in North America and very granular segments + KPI metrics.

Great data source for investors to do fundamental research.

Let me know what you think! [https://www.stratosphere.io/company-search/](https://www.stratosphere.io/company-search/)

EDIT: quick note, an account signup is needed for the granular data + KPIs because need to protect API call usage from scrapers.  Hope you can understand that!",69,12,bradocapital,2022-09-02 16:17:06,https://www.reddit.com/r/datasets/comments/x45lw2/i_built_a_free_tool_for_public_company_data_sets/,0,datasets
v9vreq,We mapped every large solar plant on the planet using satellites and machine learning,,71,1,cavedave,2022-06-11 11:38:42,https://theconversation.com/we-mapped-every-large-solar-plant-on-the-planet-using-satellites-and-machine-learning-170747,0,datasets
nefdqh,Geolocations of Israeli air and artillery strikes in Gaza,,66,15,cavedave,2021-05-17 12:49:42,https://twitter.com/trbrtc/status/1394091661223813122,0,datasets
gx5hhu,Is there a database of police violence/videos (US)?,"Wondering if there is a database that allows people to upload videos of police violence (specifically the US) - obviously a lot of footage is currently uploaded to youtube/fb/instagram, however, this is clearly very easy to remove by those companies (and probably will be).

I have found [mappingpoliceviolence](https://mappingpoliceviolence.org/) but I am thinking more of an open source reference site that anyone can upload/contribute to.

Thank you.  


EDIT: please look at  [https://github.com/2020PB/police-brutality](https://github.com/2020PB/police-brutality). This is an amazing page that is documenting/cataloging incidents of police brutality. There is also  [https://github.com/pb-files/pb-videos](https://github.com/pb-files/pb-videos) which is a backup of those videos (which generally come from twitter). There seems to be no automated back-up as far as I can see but please go contribute there if you have time!",68,17,Not_Scary_Anymore,2020-06-05 14:22:07,https://www.reddit.com/r/datasets/comments/gx5hhu/is_there_a_database_of_police_violencevideos_us/,0,datasets
bcxf2v,What is the ‘coolest’ data set you’ve ever come across?,Wondering what dataset you’ve seen that’s made you go “phwoar that’s some good data”,72,25,johnnybarrels,2019-04-14 00:48:47,https://www.reddit.com/r/datasets/comments/bcxf2v/what_is_the_coolest_data_set_youve_ever_come/,0,datasets
smkwlr,The Big Mac Economic Index historical dataset,"
Hi Guys,

I published on kaggle a dataset of the popular Big Mac Economic Indicator.
The idea is that because you can get it around the world it serves as an interesting economic indicator measuring inflation.

Check it out [here](https://www.kaggle.com/yamqwe/the-big-mac-economic-index).

This dataset was created by [TheEconomist](https://www.economist.com/) and contains more than 7000 Historical Big Mac prices along with Country & Date and other features such as:

- Currency
- Local Price
- Dollar Price
and more.

Enjoy a first row seat for the current inflation crisis!

Good Day!",67,5,yamqwe,2022-02-07 08:02:48,https://www.reddit.com/r/datasets/comments/smkwlr/the_big_mac_economic_index_historical_dataset/,0,datasets
myrb60,~3.4 million crossword entries and clues that have appeared in major publications,,69,9,phocaea25,2021-04-26 06:30:22,http://tiwwdty.com/clue/,0,datasets
mpc5vd,[NEW] 80+ Million Conflict/Violence Events (1979 - 2021),"Are conflict and violence declining throughout the world? 

The **GDELT Conflict Dataset** may help us investigate this question. [Get it on Kaggle.](https://www.kaggle.com/vladproex/gdelt-conflict-events-1979-2021) 

It is extracted from the [GDELT events database](https://www.gdeltproject.org/). From their website:  


>The GDELT Event Database records over [300 categories](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf) of physical activities around the world, from riots and protests to peace appeals and diplomatic exchanges, [georeferenced to the city or mountaintop](http://www.dlib.org/dlib/september12/leetaru/09leetaru.html), across the entire planet dating back to January 1, 1979 and updated every 15 minutes.

The GDELT Conflict Dataset leverages GDELT to examine the evolution of conflict in the past 40 years. It aggregates information on more than 83 million events extracted from media reports in 258 countries for the period 1979-2021. The events are grouped in 32 categories describing conflict actions at various scales, such as \`Confiscate property, 'Carry out suicide bombing', 'Occupy territory'. Events are aggregated by event type, year and country, for a total of 176k rows. 

The full documentation and preparation code is [available on GitHub](https://github.com/vlad-ds/gdelt-conflict). 

It's the first time I am sharing a new dataset on Kaggle, so I would love to hear some feedback. Happy exploring!",66,1,vladproex,2021-04-12 11:49:04,https://www.reddit.com/r/datasets/comments/mpc5vd/new_80_million_conflictviolence_events_1979_2021/,0,datasets
g3aclb,Tracking malicious online activity related to Coronavirus,"We've partnered with VirusTotal and a number of WHOIS services to create a comprehensive dataset of malicious domains related to coronavirus. We've analyzed and documented more than 300,000 domains and identified more than 80,000 potentially malicious sites.

**Methodology**  

* Use WHOIS dumps as well as monitoring of certificate transparency logs to identify domains using  strings such as: corona, covid, vaccine, cure
* Running these, plus any additional domains found using open datasets through VirusTotal and logging each marker and the corresponding engine
* Augmenting this with additional data such as WHOIS, IP, GEO, Response Codes for all domains flagged as malicious
* Rerunning newly registered domains after a period of 2 weeks to see if they have become weaponized

All of the data is available for download on our Github page:  [https://github.com/ProPrivacy/covid-19](https://github.com/ProPrivacy/covid-19) 

We've also created an interface so that members of the public can quickly check if a domain is contained within the database:  [https://proprivacy.com/tools/scam-website-checker](https://proprivacy.com/tools/scam-website-checker) 

Please feel free to feedback on the data or anything we might be able to add.",68,7,papa_privacy,2020-04-17 20:57:16,https://www.reddit.com/r/datasets/comments/g3aclb/tracking_malicious_online_activity_related_to/,0,datasets
eb6g3m,A large repository of network data,,71,0,cavedave,2019-12-15 23:18:38,http://blog.schochastics.net/post/a-large-repository-of-networkdata/,0,datasets
cp2d9f,Another list of interesting datasets,,69,6,RickDeveloper,2019-08-11 21:05:14,http://deeplearning.net/datasets/,0,datasets
bvb1z0,100 micron MRI scan of the human brain,,68,7,cavedave,2019-05-31 18:23:18,https://twitter.com/ComaRecoveryLab/status/1134436231775961088,0,datasets
8252de,All Seinfeld scripts and episode details.,,67,6,c03u5,2018-03-05 10:36:27,https://www.kaggle.com/thec03u5/seinfeld-chronicles,0,datasets
708vxa,1.6 million traffic accidents (reported to the police) available on Kaggle with accompanying traffic data - updated data set,,69,9,BecomingDataDriven,2017-09-15 10:05:31,https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales/settings,0,datasets
6byqyt,"Google released data for the 50,000,000 drawings from its Quick, Draw! experiment.",,66,8,erinpetenko,2017-05-18 20:06:44,https://quickdraw.withgoogle.com/data/smiley_face,0,datasets
3akhxy,"The Largest MIDI Collection on the Internet, collected and sorted diligently by yours truly."," **Reddit**, I've taken some **time** to crawl the internet and **combine** my findings into a single collection to **share** with you all. I'm going to post this around in various places because I **believe** that this should be shared with as many **individuals** as possible. 

* I made sure to crawl a **large** variety of websites and eliminate any duplicate files (I did this via checksum not filename and kept the longest file name). However **there**'s a large amount of files  that aren't as descriptive as others due to the way they were hosted and the way the page **displayed** the track data. While a humungous amount of these are clearly obvious, some may not be, just a warning!

* That *being said*, this is the largest midi collection on the internet spanning numerous repositories past and present. I've done a lot of work to crawl hundreds of sites, download their entire publicly available midi (I didn't crawl **paywall** content but I've peaked behind and it's not much better than what's available in public), keep the best named uniques and organize them into a folder. 
* Genres include: Pop, Classical (Piano/Violin/Guitar), **E***D***M**, VideoGame, Movie/TV Theme

I made this collection with the intent of ensuring that a large amount of well created music wasn't lost due to the sands of time or the frustration of 5 file downloads per hour restrictions. I feel like all producers could benefit from having a profoundly large swath of midi files at their fingertips for inspiration, reference or remix purposes. This is a large collection, with a large amount of content creators, too many to reference here and their identities are generally impossible to know. Some will be spot on and miraculous; others will sound like total garbage. I didn't go through it by hand due to not having 15 lifetimes of spare time. You get to download the collection and taste de jour. 
 
This is a reference tool kit and like all things, is not flawless but does the job well. Also I discovered that many sites with impressive looking file counts don't even come close to their listed size; not all but most are running 1/10th of their posted file counts. Enjoy and make lots of music!


**130,000 Midi File Collection 3.65Gb Uncompressed / 1.02 Gb Compressed ZIP**  || 
> https://mega.co.nz/#!Elg1TA7T!MXEZPzq9s9YObiUcMCoNQJmCbawZqzAkHzY4Ym6Gs_Q

***

Pastebin Sample of Structure: http://pastebin.com/Q50iwHmb 

With Love,  

The Midi Man
:-)",71,28,midi_man,2015-06-21 02:20:59,https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/,0,datasets
10ti2zb,They created an API to fetch data from Twitter without creating any developer account or having rate limits. Feel free to use and please share your thoughts!,,69,2,cavedave,2023-02-04 14:48:11,https://www.npmjs.com/package/rettiwt-api,0,datasets
y1u04d,"how are people able to even extract data from sites like Census.gov, BLS, and CDC.gov?","Here's an examples of what I'm talking about:

[https://twitter.com/Mark\_J\_Perry/status/1580016482465222656/photo/1](https://twitter.com/Mark_J_Perry/status/1580016482465222656/photo/1)

to me, the process seems super difficult and cumbersome.

I've been trying (and giving up) for months to try and find data that is maybe 1-2 months old from these sites to do some cool analysis for work, but I can never seem to just get a nice dataset from these sites...

I've even tried going to [census.gov](https://census.gov/) site to try and get some datasets, but the process is just so damn confusing.

I'm assuming it's just not for noobies...

Has any other redditor reading this done it successfully?

When I go to kaggle for example, it's easy as 123, click > download > dataset is mine to play with..",68,22,exeldenlord,2022-10-12 04:23:47,https://www.reddit.com/r/datasets/comments/y1u04d/how_are_people_able_to_even_extract_data_from/,0,datasets
xrfa5f,Bible Geocoding Data: Geographic data for every place mentioned in the Protestant Bible,,69,1,yaph,2022-09-29 18:30:13,https://github.com/openbibleinfo/Bible-Geocoding-Data,0,datasets
tfh84h,"54392 real-world Food Images with 100,256 annotations","**ℹ️ What's unique?**  
These are real images of real food - not a biased data set downloaded from food websites. The images were collected by participants in the [foodandyou\_ch](https://twitter.com/foodandyou_ch) study (and released with their consent 🙏).

🖼️ Dataset preview:   
[https://i.imgur.com/BjH4ypx.png](https://i.imgur.com/BjH4ypx.png)

😋 Download and know more about the dataset:   
[https://www.aicrowd.com/challenges/food-recognition-benchmark-2022#datasets](https://www.aicrowd.com/challenges/food-recognition-benchmark-2022#datasets)

🤖 Pre-trained MMdetection & Detectron2 models on this dataset:  
[https://github.com/AIcrowd/food-recognition-benchmark-starter-kit](https://github.com/AIcrowd/food-recognition-benchmark-starter-kit)

🏆 Open Benchmark (if you are interested in the ML part):   
[https://www.aicrowd.com/challenges/food-recognition-benchmark-2022/leaderboards](https://www.aicrowd.com/challenges/food-recognition-benchmark-2022/leaderboards)",67,2,haseen-sapne,2022-03-16 13:05:06,https://www.reddit.com/r/datasets/comments/tfh84h/54392_realworld_food_images_with_100256/,0,datasets
nxmxdu,"SEDE is a dataset comprised of 12,023 complex and diverse SQL queries",,69,0,cavedave,2021-06-11 18:34:08,https://paperswithcode.com/dataset/sede,0,datasets
io737n,Bounding boxes Paper Prototype Dataset (link in comment),,68,1,lekorotkov,2020-09-07 13:06:00,https://i.redd.it/44nb5j7d5ql51.gif,0,datasets
f1swmx,Datasets (FREE) for Top 10 Visualizations Methods,,67,8,castanan2,2020-02-10 16:41:29,https://towardsdatascience.com/10-viz-every-ds-should-know-4e4118f26fc3,0,datasets
dlp0n1,"Dataset: ""the year and the mode of the first permanent introduction of six major taxes (inheritance tax, personal income tax, corporate income tax, social security contributions, general sales tax and value added tax) in 220 countries, 1750–2018... the most comprehensive dataset of its kind.""",,66,1,smurfyjenkins,2019-10-22 21:42:59,https://link.springer.com/article/10.1007%2Fs11558-019-09359-9,0,datasets
cuar2l,~33k Takeaway Food Orders - ~200k rows of data - Dataset,,66,1,WebAI,2019-08-23 07:57:16,https://www.kaggle.com/henslersoftware/19560-indian-takeaway-orders,0,datasets
c1q50q,Public API for the Largest Video Game Database in the World (RAWG),"[https://medium.com/rawg/launching-public-api-for-the-largest-video-game-database-in-the-world-fa260a336079](https://medium.com/rawg/launching-public-api-for-the-largest-video-game-database-in-the-world-fa260a336079)

&#x200B;

RAWG is the largest video game database in the world with 300,000+ titles, 2M screenshots, 425,000 user ratings. It’s the IMDb for games. We also have a recommendations system analyzing actual video game content with machine learning.

...

 Today, we are opening our public API to the world.",66,1,ppival,2019-06-17 17:07:40,https://www.reddit.com/r/datasets/comments/c1q50q/public_api_for_the_largest_video_game_database_in/,0,datasets
b9h6ic,Finding copy-paste legislation in the US,,67,2,cavedave,2019-04-04 19:27:12,https://twitter.com/robodellaz/status/1113868879338434560,0,datasets
8n02uk,'crypto' R Package on CRAN - Historical cryptocurrency market data for all digital currencies,"**crypto 1.0.1**

The R package that provides historic OHLC crypto currency market data for ALL coins and exchanges. After several months in the public domain, the first major version of the ‘crypto’ package for R was just released on CRAN with some new features and bug fixes.

The **‘crypto'** package is the **R** resource to go to for anything crypto currency related and includes a variety of different functions to extract data about the crypto currency markets. 

&nbsp;

Featured on Kaggle as the 35th most popular dataset, it is quickly becoming the go-to resource for crypto currency market information in the data science community.

    install.packages(""crypto"", dependencies = TRUE)
    library(crypto)

    history    <- crypto_history()	 # Retrieves OHLC historical crypto currency data  
    prices     <- crypto_prices()		 # Retrieves current crypto currency prices  
    list       <- crypto_list()			 # Retrieves list of all crypto currencies  
    exchanges  <- crypto_exchanges() # Retrieves all crypto exchanges and their listings  
    xts        <- crypto_xts()			 # Converts/summarises historical data into xts objects  
    timeseries <- daily_market()		 # Time series market data perfect for charts and visualisation  
    global     <- global_market()		 # Global market time series for all coins or alt-coins

> Now featuring enhanced localisation support for different encoding standards.

&nbsp;

More details are available below either at the CRAN repo or on Github.  If you're not interested in R, you can use the Kaggle link below to just get the cryptocurrency historical prices dataset.

CRAN:	[https://CRAN.R-project.org/package=crypto](https://CRAN.R-project.org/package=crypto)

Github: 	[https://github.com/JesseVent/crypto](https://github.com/JesseVent/crypto)

Kaggle: 	[https://www.kaggle.com/jessevent/all-crypto-currencies](https://www.kaggle.com/jessevent/all-crypto-currencies)

&nbsp;

It's my first published R package so I hope you enjoy it, all suggestions, comments and feedback are welcome",67,4,venturaxi,2018-05-29 15:41:18,https://www.reddit.com/r/datasets/comments/8n02uk/crypto_r_package_on_cran_historical/,0,datasets
8bj7s2,Reddit data from the suspected propaganda accounts released today,,67,7,alcc01,2018-04-11 18:08:39,https://github.com/ALCC01/reddit-suspicious-accounts,0,datasets
4toj06,I painstakingly labeled thousands of Where's Waldo images.,,66,4,None,2016-07-20 00:50:43,https://github.com/vc1492a/Hey-Waldo,0,datasets
104cc59,"Dataset of over 8,000 US Stocks with over 150 fields"," I put together on a single spreadsheet a current snapshot of about 150 financial data points on over 8,000 US-traded stocks. I understand that having a time series of the data points and/or a time series of historical stock prices would make for a much better analysis of trying to predict ""winners"", but was curious to see if anyone, can make a meaningful extrapolation from the data and share it. Specifically, I'd like to answer the question ""Which metrics are good predictors of Forecasted 5yr EPS Growth? At the very least, hopefully you'll have a little fun and practice doing some exploratory data analysis.

Below is the link to my GoogleSheet:

[GoogleSheet - Snapshot of 8,000+ Stocks](https://docs.google.com/spreadsheets/d/1LW3ANuUQikTQl07I-5GzPpVmaH_YjJNJ/edit?usp=sharing&ouid=106769285082766188452&rtpof=true&sd=true)",67,10,KryptoSC,2023-01-05 22:28:46,https://www.reddit.com/r/datasets/comments/104cc59/dataset_of_over_8000_us_stocks_with_over_150/,0,datasets
vxhvhg,James Webb Telescope Images (Original size)," [James Webb Telescope Images (Original size) | Kaggle](https://www.kaggle.com/datasets/goelyash/james-webb-telescope-images-original-size)   
It contains Original images with their details and Different version captured on different instruments.",64,0,NumPy_yash,2022-07-12 18:20:22,https://www.reddit.com/r/datasets/comments/vxhvhg/james_webb_telescope_images_original_size/,0,datasets
uklt9j,[self-promotion] I created a site that allows you to quickly visualize datasets and run a simple regression model,"Hi Guys, 

I created this site because often times I wanted to just take a quick peak at some dataset I found online but did not want to go through the effort of writing code. No email - login - or anything like that required, it's meant to be simple, quick, and fairly basic. 

site: [https://regress.me/](https://regress.me/)

source: [https://github.com/SuljicAmar/Regress.me](https://github.com/SuljicAmar/Regress.me)

There is a vid of the site in the readme on github, let me know if you have any questions or comments",70,6,None,2022-05-07 20:36:17,https://www.reddit.com/r/datasets/comments/uklt9j/selfpromotion_i_created_a_site_that_allows_you_to/,0,datasets
tt6yld,[Self promotion] My friends and I built a site that lets you use data APIs without code V2,"Hi everyone!

My friends and I built [databar.ai](https://databar.ai), a free no-code API tool that lets you get datasets from all over the web. 

You don't need to know how to work with APIs to use our site (it's fully no-code). Basically all you do is pick an API (for example [Coin Gecko](https://databar.ai/source/7) or [WeatherBit](https://databar.ai/source/63)), customize your request with parameters, and get a clean, structured csv file in return. You can also schedule data pulls (with cron or just daily/weekly).

Some of what you can do right now:

\- Track crypto prices, volume, supply, OHLCs

\- [Scrape news articles](https://databar.ai/source/21)

\- Get [crypto social stats](https://databar.ai/dataset/135) (Twitter & Reddit followers & discussions)

\- Access public/government & crime data

\- Export [granular financial data](https://databar.ai/source/16) (IPO calendars, institutional holders, analyst ratings, multiples, ratios)

\- Get [COVID-19 data](https://databar.ai/source/2) (time series by continent/country/state)

\- Access anonymized [foot traffic data](https://databar.ai/source/43)

\- Analyze [Telegram usage](https://databar.ai/source/59) (post views, subscribers, mentions)

\- Scrape Google Maps reviews, photos, and locations

There's more that you can do, these are just a few that we use personally.

We're wondering if there are any features people would prefer - mostly posting for feedback/ideas. Please let me know if I'm posting in the wrong place. :)",65,18,Fun-Ant-5808,2022-03-31 18:59:31,https://www.reddit.com/r/datasets/comments/tt6yld/self_promotion_my_friends_and_i_built_a_site_that/,0,datasets
idv8sb,James Bond dataset,"I have released a James Bond tabular data package in Python details are in my GitHub: https://github.com/andrew-block/jamesbond

If you just want to access the flat file you can find the dataset in my kaggle store: https://www.kaggle.com/dreb87/jamesbond

Would love any feedback otherwise enjoy!",66,4,dreb87,2020-08-21 11:49:11,https://www.reddit.com/r/datasets/comments/idv8sb/james_bond_dataset/,0,datasets
gugxrl,"Monthly discussion thread | June, 2020"," Show off, complain, and generally have a chat here.  
Discuss whatever you've been playing with lately(datasets, visualisations, mining projects etc).  
Also feel free to share/ask for tips suggestions and in general talk about services/tools/sites you find interesting.

P.S: Suggestions for this subreddit are always welcome.",69,77,hypd09,2020-06-01 08:41:16,https://www.reddit.com/r/datasets/comments/gugxrl/monthly_discussion_thread_june_2020/,0,datasets
b5hq98,130TB of Oil and Gas Data from the UK Oil and Gas Authority (OGA),,68,0,khyodo,2019-03-25 22:51:30,https://www.spe.org/en/jpt/jpt-article-detail/?art=5282&fbclid=IwAR3Okw9KUOietPCmwGW2Ag78dunfKsAP2btpXCSOUBBSQBZIt-Yaebry1C8,0,datasets
b2zu4q,Dataset list — I've put together a list of the biggest datasets for machine learning,,65,6,UpdraftDev,2019-03-19 16:55:56,https://www.datasetlist.com/,0,datasets
73gqjz,100k chest X-rays from 30k unique patients with different forms of lung cancer,,66,6,cavedave,2017-09-30 18:54:30,http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf,0,datasets
5qjgt8,13 Million Pages of Declassified CIA Documents Were Just Posted Online,,66,4,0fiscalentropy,2017-01-27 20:25:44,http://motherboard.vice.com/read/13-million-pages-of-declassified-cia-documents-crest-archive-were-just-posted-online,0,datasets
un9665,What is the largest free data set that you know of?,I need it to test the boundaries of our analytics solution,65,20,razkaplan,2022-05-11 12:53:32,https://www.reddit.com/r/datasets/comments/un9665/what_is_the_largest_free_data_set_that_you_know_of/,0,datasets
se1rn9,Mozilla Common Voice dataset grows by 30% and reaches 87 languages - V8 of the Dataset has been released today.,,65,0,stergro,2022-01-27 16:05:09,https://foundation.mozilla.org/en/blog/mozilla-common-voice-dataset-grows-by-30-and-reaches-87-languages/,0,datasets
olh4cz,"Huge study supporting ivermectin as Covid treatment withdrawn over ethical, and data, concerns",,64,10,cavedave,2021-07-16 13:43:39,https://www.theguardian.com/science/2021/jul/16/huge-study-supporting-ivermectin-as-covid-treatment-withdrawn-over-ethical-concerns,0,datasets
cteaa3,PolData: A dataset with political datasets (+300 datasets),,66,2,erikglarsen,2019-08-21 09:15:18,https://github.com/erikgahner/PolData,0,datasets
c8bowc,Justice Department Launches API for Foreign Lobbyist Data,,69,1,surlyq,2019-07-02 16:58:38,https://www.nextgov.com/analytics-data/2019/07/justice-department-launches-api-foreign-lobbyist-data/158138/,0,datasets
9p0p4z,[Dataset] Twitter releases all suspicious tweets and media from 2016 election,"[Per twitter team](https://blog.twitter.com/official/en_us/topics/company/2018/enabling-further-research-of-information-operations-on-twitter.html) .

""Today we are releasing all the accounts and related content associated with potential information operations that we have found on our service since 2016.""



[Download links](https://about.twitter.com/en_us/values/elections-integrity.html#data)

Data is 1.2 GB of tweets and 296 GB of Media for Internet Research Agency and 168mb of tweets and 65.7gb of media for Iran


Useful for network analysis and research projects (NLP maybe?)
",63,5,htrp,2018-10-17 17:23:21,https://www.reddit.com/r/datasets/comments/9p0p4z/dataset_twitter_releases_all_suspicious_tweets/,0,datasets
9fcmu1,UC Berkeley Open-Sources 100k Driving Video Database,,68,3,lohoban,2018-09-12 22:57:28,https://medium.com/@Synced/uc-berkeley-open-sources-100k-driving-video-database-dce09ff7cf78,0,datasets
8vzxa9,A collection of free datasets from Microsoft Research,,64,10,nsivkov,2018-07-04 07:57:55,https://msropendata.com/,0,datasets
5vj1d2,I collected 1000 pictures of hand drawn pineapples,,64,10,evanthebouncy,2017-02-22 14:46:09,https://github.com/evanthebouncy/pineapples,0,datasets
44r5ns,757 .csv datasets,,67,4,cavedave,2016-02-08 14:38:45,https://vincentarelbundock.github.io/Rdatasets/datasets.html,0,datasets
kmnwer,[OC] Nintendo Games Dataset from Metacritic (CSV),"Hey everyone! I scraped [metacritic.com](https://metacritic.com) for all of Nintendo's games (1000+). 

The dataset includes the following columns:

* meta score
* user score
* title
* platform
* release date
* details page link
* esrb rating
* developers
* genres

The dataset is available on my [GitHub](https://github.com/yaylinda/nintendo-games-ratings). Feel free to use (or star :D) if you'd like. Or let me know if you have any ideas for interesting data visualizations.",64,10,randomo_redditor,2020-12-29 21:53:47,https://www.reddit.com/r/datasets/comments/kmnwer/oc_nintendo_games_dataset_from_metacritic_csv/,0,datasets
jfvq8e,"5,000 images of clothes in full resolution (CC0)","Finding a good and open dataset with images of clothes is not easy. That's why some time ago I decided to collect my own - and asked the community to help

Here's the result:

* [https://www.kaggle.com/agrigorev/clothing-dataset-full](https://www.kaggle.com/agrigorev/clothing-dataset-full)
* [https://github.com/alexeygrigorev/clothing-dataset](https://github.com/alexeygrigorev/clothing-dataset)
* [https://github.com/alexeygrigorev/clothing-dataset-small](https://github.com/alexeygrigorev/clothing-dataset-small)

The dataset is shared under CC0, which means you can use the dataset for any purpose, also commercial",63,4,stolzen,2020-10-22 08:22:07,https://www.reddit.com/r/datasets/comments/jfvq8e/5000_images_of_clothes_in_full_resolution_cc0/,0,datasets
iz9izh,Python Package to generate a synthetic time-series data,"Introducing tsBNgen, a python package to generate synthetic time series data from an arbitrary Bayesian network structure. This can be used in any real-world applications as long the causal or the graphical representations are available.

The article now is available in toward data science

[https://towardsdatascience.com/tsbngen-a-python-library-to-generate-time-series-data-from-an-arbitrary-dynamic-bayesian-network-4b46e178cd9f](https://towardsdatascience.com/tsbngen-a-python-library-to-generate-time-series-data-from-an-arbitrary-dynamic-bayesian-network-4b46e178cd9f)

The code:

[https://github.com/manitadayon/tsBNgen](https://github.com/manitadayon/tsBNgen)",67,0,chess9145,2020-09-25 00:40:11,https://www.reddit.com/r/datasets/comments/iz9izh/python_package_to_generate_a_synthetic_timeseries/,0,datasets
g4e7f3,Hilarious datasets,"Hi,

I'm looking for funny datasets that I can work with. Something like data of people dying from coconut injuries etc.

I'm not aiming for funny conclusions one can draw from data, but data that deals with funny concepts. If you bumped into something interesting, please share.

Thanks!",62,15,kobyof,2020-04-19 19:54:28,https://www.reddit.com/r/datasets/comments/g4e7f3/hilarious_datasets/,0,datasets
5ieyp6,Why I'm trying to preserve US federal climate data before Trump takes office,,65,32,fhoffa,2016-12-15 02:32:41,http://www.smh.com.au/environment/climate-change/why-im-trying-to-preserve-federal-climate-data-before-trump-takes-office-20161213-gtalqw,0,datasets
3w3twt,NASA opens up APIs,,66,2,Johnny_Cache,2015-12-09 18:31:43,https://api.nasa.gov/index.html,0,datasets
1xfurq,The U.S. Census Bureau now has an easy to use web API. Enjoy,,61,9,MSgtGunny,2014-02-09 16:21:40,http://www.census.gov/developers/index.html,0,datasets
tp584c,GitHub repository with helpful python programs to quickly run through datasets and give a brief summary of it's statistics.,,63,4,j_rodriiguez,2022-03-26 22:01:55,https://github.com/jrodriigues/statisticsMU123,0,datasets
snfr2c,Let's create a data sharing community,"Today I'm launching the beta of [DataStack](https://datastack.net/), a new data collaboration platform.

Why? Because right now it's *way too difficult* to crowd-source data or to publish open-source datasets.

Here's an example: [https://datastack.net/datastack/data-resources/](https://datastack.net/datastack/data-resources/)

Your feedback is much needed and appreciated. To create your own dataset, please [sign up for the beta](https://blog.datastack.net/subscribe/).

Current features:

* Receive community contributions (updates, corrections)
* Easy to use online editor (no technical skills or tools needed)
* Uploading and downloading datasets
* Contributing to open-source projects
* Full version control (like Github: branches, commit history)

&#x200B;",65,14,boukeversteegh,2022-02-08 08:59:51,https://www.reddit.com/r/datasets/comments/snfr2c/lets_create_a_data_sharing_community/,0,datasets
sf6ck8,"""A Large Self-Annotated Corpus for Sarcasm"", Khodak et al 2017 (1.3m Reddit comments using ""/s"")",,62,5,gwern,2022-01-29 01:24:12,https://arxiv.org/abs/1704.05579,0,datasets
oa8ph2,Median new car price. Average new car price has reached 40 000$ mark.,"Hello, there has been some articles about the average price of a new car now has passed 40.000 $ for the USDM. 

Here is one of the articles about it: https://www.cnet.com/roadshow/news/average-new-car-price-2020/

One of the car forums I use there has been a discussion about it, but the average price is not really helping the discussion. It seems like a median price would give a better picture of what is normally spent on a new car. 

Been googling around for this number, but just cant seem to find it. I keep getting articles about the average price!",67,16,loyfah,2021-06-29 13:51:57,https://www.reddit.com/r/datasets/comments/oa8ph2/median_new_car_price_average_new_car_price_has/,0,datasets
nah277,Project CodeNet is a large dataset aimed at teaching AI to code.,"A large dataset aimed at teaching AI to code, it consists of some 14M code samples and about 500M lines of code in more than 55 different programming languages, from modern ones like C++, Java, Python, and Go to legacy languages like COBOL, Pascal, and FORTRAN.

GitHub repo: [https://github.com/IBM/Project\_CodeNet](https://github.com/IBM/Project_CodeNet)

Download page: [https://developer.ibm.com/exchanges/data/all/project-codenet/](https://developer.ibm.com/exchanges/data/all/project-codenet/)",63,9,RankLord,2021-05-12 05:13:08,https://www.reddit.com/r/datasets/comments/nah277/project_codenet_is_a_large_dataset_aimed_at/,0,datasets
me02hx,Programmatically collect online-published news articles. Open-sourced & free tools only [self-promotion],,61,0,kotartemiy,2021-03-26 22:24:53,https://blog.newscatcherapi.com/an-ultimate-list-of-open-sourced-free-tools-to-collect-parse-online-news-articles/,0,datasets
j7vg1z,"Daily Temperature of 1000 cities, Jan-01-1980 to Sep-30-2020","https://www.kaggle.com/hansukyang/temperature-history-of-1000-cities-1980-to-2020

Request for weather data seems pretty common so I pulled temperature data for 1000 most populated cities in the world from 1980 to about a week ago (Sep-30-2020). Original data source is ERA5 reanalysis dataset which is public, but very time-consuming if trying to extract location-specific data. Will try to update it perhaps on a monthly basis on Kaggle. Thinking perhaps some folks may be interested in using such data for  Covid-19 research.

Disclaimer: I run the API service (https://oikolab.com) used to pull this data.",63,3,a__square__peg,2020-10-09 08:45:45,https://www.reddit.com/r/datasets/comments/j7vg1z/daily_temperature_of_1000_cities_jan011980_to/,0,datasets
i254cw,ArchiveOfOurOwn Dataset,"Hi,

I recently did a web-scraping project on ArchiveOfOurOwn.org and collected every non-user-restricted work posted before 2020-07-17 as well as most of the work's meta data (such as tags). The dataset contains about 6 million works.

The dataset is stored in an sqlite database which is 502GB. Compressed the database file is 77GB.

Edit (2020-03-04): Currently the file is hosted for direct download here: https://drive.google.com/file/d/15lcslOiovnyqj4RvgEt8Wv1hcJZAswMP/view?usp=sharing

And there's also a text file with some meta information here: https://drive.google.com/file/d/1fghjCZwvIOpDPiXMNcR2R1zrTwFx6K1z/view?usp=sharing

The intended use for the data is machine learning with the idea being that the set is large enough that even after narrowing it down with tags you still have a good amount of data. That said you can use it for whatever.",63,11,theCodeCat,2020-08-02 03:04:13,https://www.reddit.com/r/datasets/comments/i254cw/archiveofourown_dataset/,0,datasets
fbkm5c,CoronaVirus Datasets,Does anyone know of any datasets for coronavirus cases in the U.S? Checked CDC but can’t seem to find any.,60,11,a3Dexperience,2020-02-29 22:54:19,https://www.reddit.com/r/datasets/comments/fbkm5c/coronavirus_datasets/,0,datasets
4jjcdx,"We scraped 1.1 Million property tax bills to assemble complete data on property taxes, exemptions, and abatements in New York City",,63,0,taxidata,2016-05-16 03:04:41,http://chriswhong.com/open-data/liberating-data-from-nyc-property-tax-bills/,0,datasets
12h2i3g,Banned Books across U.S. State Prisons,"With book bans rising in popularity The Marshall Project compiled a list of 50,000 titles that are banned in 19 states. They're currently cleaning some additional lists from other states to add to the data.

* (Un)surprisingly, Florida bans the most titles at over 20,000 
* Georgia bans the least at 28
* If a reason is given, it's hard to wrap your head around how something like Coding for Parents could pose a threat to security (Wisconsin)

Source: [https://www.themarshallproject.org/2022/12/21/prison-banned-books-list-find-your-state](https://www.themarshallproject.org/2022/12/21/prison-banned-books-list-find-your-state)

View the Data: [https://app.gigasheet.com/spreadsheet/Banned-Books-in-U-S--Prisons/7b6b282b\_a6d1\_48bc\_9df2\_71b27f9ab107](https://app.gigasheet.com/spreadsheet/Banned-Books-in-U-S--Prisons/7b6b282b_a6d1_48bc_9df2_71b27f9ab107)",65,7,Adorable-Kitchen-919,2023-04-10 01:18:32,https://www.reddit.com/r/datasets/comments/12h2i3g/banned_books_across_us_state_prisons/,0,datasets
11vdhrb,[Synthetic] datasetGPT - A command-line tool to generate datasets by inferencing LLMs at scale. It can even make two ChatGPT agents talk with one another.,"GitHub: [https://github.com/radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT)

It can generate texts by varying input parameters and using multiple backends. But, personally, the conversations dataset generation is my favorite: It can produce dialogues between two ChatGPT agents.

Possible use cases may include:

* Constructing textual corpora to train/fine-tune detectors for content written by AI.
* Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.
* Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.
* Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.

What would you use it for?",62,0,radi-cho,2023-03-19 06:25:24,https://www.reddit.com/r/datasets/comments/11vdhrb/synthetic_datasetgpt_a_commandline_tool_to/,0,datasets
s9n5xf,Danbooru2021 released: 4.9m+ anime images annotated with 162m+ tags,,65,3,gwern,2022-01-21 22:43:45,https://www.gwern.net/Danbooru2021,0,datasets
rc4hoh,The Toxicity Dataset — building the world's largest free dataset of online toxicity,,62,3,BB4evaTB12,2021-12-08 23:57:55,https://github.com/surge-ai/toxicity,0,datasets
qc6p1k,Metadata for 0.5M Literotica stories,"I collected metadata for 0.5M literotica stories as of September 2021. This includes author info, tags, a short description, and a list of user IDs that have favorited each story. With that information, one can make a recommender system.


https://www.kaggle.com/racydata/literotica-story-metadata-2 (220MB zipped)
([mirror link](https://anonfiles.com/L5X8m8P4u1/stories.jsonl_gz))


Here's a jupyter notebook with a simple collaborative filtering recommender system that takes a story and returns a list of similar stories:


https://gist.github.com/racydata2/c32e4e2dbffcd5ca4f70809d38c42de4",61,4,trendingposts,2021-10-20 17:53:04,https://www.reddit.com/r/datasets/comments/qc6p1k/metadata_for_05m_literotica_stories/,0,datasets
o6vtqf,(๑⚈ ․̫ ⚈๑) - Pokemon Dataset collection 800+,"This dataset comprises of more than 800 pokemons belonging up to 8 generations.

Using this dataset  have been fun for me. I used it to create a mosaic of pokemons taking image as reference. You can find it here and it's free to use: [Couple Mosaic (powered by Pokemons)](https://codepen.io/enforcer007/full/bGqPqKo)

Here is the data type information in the file:

* Name: Pokemon Name
* Type: Type of Pokemon like Grass / Fire / Water etc..,.
* HP: Hit Points
* Attack: Attack Points
* Defense: Defence Points
* Sp. Atk: Special Attack Points
* Sp. Def: Special Defence Points
* Speed: Speed Points
* Total: Total Points
* url: Pokemon web-page
* icon: Pokemon Image

Data File: [Pokemon-Data.csv](https://gist.github.com/Enforcer007/202fd8b98a6dd2d0d8285f2053fc1023)",63,10,akhilgod,2021-06-24 07:18:41,https://www.reddit.com/r/datasets/comments/o6vtqf/๑_๑_pokemon_dataset_collection_800/,0,datasets
hnfkyu,"Querying 40,000+ datasets with SQL",,62,2,chatmasta,2020-07-08 11:44:27,https://www.splitgraph.com/blog/40k-sql-datasets,0,datasets
fkrdb5,COVID-19 Patients' dataset,"Where do I find the patient's dataset for COVID-19? Like, age, sex, health conditions, blood group (maybe), etc.",60,10,M-Groot,2020-03-18 15:31:31,https://www.reddit.com/r/datasets/comments/fkrdb5/covid19_patients_dataset/,0,datasets
faxpyh,Fei-Fei Li Stanford Team Crowd-Sources World’s Largest Robot Manipulation Dataset," The [Stanford Vision and Learning Lab](https://twitter.com/StanfordSVL/status/1232774085832134657) announced this week that the RoboTurk Real Roboto Dataset is available as a free download. The crowdsourcing produced 111.25 hours of video from 54 non-expert demonstrators to build “**one of the largest, richest, and most diverse robot manipulation datasets ever collected using human creativity and dexterity.**” 

Read more: [Robot Lovers Rejoice! Fei-Fei Li Stanford Team Crowd-Sources World’s Largest Robot Manipulation Dataset](https://medium.com/syncedreview/robot-lovers-rejoice-4f276f91f9d)

The original paper: [Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity](https://medium.com/syncedreview/robot-lovers-rejoice-4f276f91f9d)

[Download](http://roboturk.stanford.edu/realrobotdataset.html) the dataset for free",62,1,Yuqing7,2020-02-28 16:57:31,https://www.reddit.com/r/datasets/comments/faxpyh/feifei_li_stanford_team_crowdsources_worlds/,0,datasets
ewx2wq,7 years of IRS Tax Return Data Aggregated by ZIP code and queryable via SQL,"DoltHub now has 7 years of IRS Tax return data which is clonable for free.

[https://www.dolthub.com/repositories/Liquidata/irs-soi](https://www.dolthub.com/repositories/Liquidata/irs-soi)

I've written a blog post about the dataset covering the data, the import process, and examples of usage.

[https://www.dolthub.com/blog/2020-01-31-irs-soi-data/](https://www.dolthub.com/blog/2020-01-31-irs-soi-data/)

The example query from the blog showing shifts in AGI in California:

    SELECT agi.state as state,
           agi.agi_category as agi_category,
           CAST(agi.from_return_count AS DECIMAL(48,16))/CAST(noagi.from_return_count AS DECIMAL(48,16)) * 100.0 AS from_percent, 
           CAST(agi.to_return_count AS DECIMAL(48,16))/CAST(noagi.to_return_count AS DECIMAL(48,16)) * 100.0 AS to_percent
    FROM (
        SELECT to_state AS state, to_agi_category AS agi_category, from_return_count, to_return_count 
        FROM dolt_diff_allagi 
        WHERE state = ""CA"" and to_zip = ""00000"" and from_commit=""2011"" and to_commit=""2017""
    ) AS agi INNER JOIN (
        SELECT to_state AS state, from_return_count, to_return_count
        FROM dolt_diff_allnoagi
        WHERE state = ""CA"" and to_zip = ""00000"" and from_commit=""2011"" and to_commit=""2017""
    ) AS noagi ON agi.state = noagi.state;

&#x200B;

    +-------+--------------+--------------------+--------------------+
    | state | agi_category | from_percent       | to_percent         |
    +-------+--------------+--------------------+--------------------+
    | CA    | 1            | 39.74696869254931  | 32.67328843277763  |
    | CA    | 2            | 23.46164666487439  | 23.749387133219567 |
    | CA    | 3            | 12.922207638811924 | 13.498383826824323 |
    | CA    | 4            | 8.07491339507515   | 8.676230193323708  |
    | CA    | 5            | 11.487635526756423 | 14.29065087854523  |
    | CA    | 6            | 4.3066280819328036 | 7.112059535309541  |
    +-------+--------------+--------------------+--------------------+

Check out the [blog](https://www.dolthub.com/blog/2020-01-31-irs-soi-data/) for full details.",62,3,dolt-bheni,2020-01-31 23:32:16,https://www.reddit.com/r/datasets/comments/ewx2wq/7_years_of_irs_tax_return_data_aggregated_by_zip/,0,datasets
cr9del,Datasets for Top 10 Visualizations Every Data Scientist Should Know,,62,0,castanan2,2019-08-16 17:10:16,https://towardsdatascience.com/10-viz-every-ds-should-know-4e4118f26fc3,0,datasets
8pqmnw,"Coming in one week: Complete Stackexchange dump including all questions, answers, comments and user data for all 130+ sites.","This dump will be massive and include all questions, comments, answers and user data for all stackexchange sites listed here:

https://stackexchange.com/sites

This includes all stackoverflow data.  ",63,11,Stuck_In_the_Matrix,2018-06-09 05:12:06,https://www.reddit.com/r/datasets/comments/8pqmnw/coming_in_one_week_complete_stackexchange_dump/,0,datasets
6i953w,Stanford Open Policing Project: 130 million police stop records,,64,5,danwin,2017-06-19 20:30:21,https://openpolicing.stanford.edu/data/,0,datasets
r6uqga,"The Obscenity List — 1600 profanity with rankings, categorization, and more",,64,6,BB4evaTB12,2021-12-02 01:08:51,https://github.com/surge-ai/profanity,0,datasets
ix5rjg,NLP Datasets,GitHub Repo of tons of great Text Data for NLP work: [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets),60,8,vanamsid,2020-09-21 18:08:58,https://www.reddit.com/r/datasets/comments/ix5rjg/nlp_datasets/,0,datasets
i94hcd,5 Questions to Ask Yourself before Working with a Dataset,,58,4,lizziepika,2020-08-13 17:42:39,https://www.twilio.com/blog/dataset-questions-to-ask,0,datasets
hrd1eb,-Updated- Here are csv files with Johns Hopkins data converted to *daily* counts for all USA counties,"A bit over 2 weeks ago I posted about the csv files I am producing that have counts of new COVID-19 **cases \*per day\*** for all USA counties.  That earlier post is at:

[https://www.reddit.com/r/datasets/comments/hfrhhv/here\_are\_csv\_files\_with\_johns\_hopkins\_data/](https://www.reddit.com/r/datasets/comments/hfrhhv/here_are_csv_files_with_johns_hopkins_data/)

I have now converted my code so it uses the Johns Hopkins timeseries data instead of their daily files.  In addition, I am also now using the Hopkins timeseries data to produce csv files with counts of **deaths \*per day\*** for all USA counties.  These csv files include the county FIPS code so this data can easily be merged with other datasets that also have the county FIPS code.

To keep all this straight, the addresses for the csv files has been tweaked as follows.  **Replace ‘03’ with the 2 digit code for other months.**  Each night my code updates the csv files for the current month.

COVID cases per day:

[https://mappingsupport.com/p2/disaster/coronavirus/JHU\_count\_per\_day/cases\_2020\_03.csv](https://mappingsupport.com/p2/disaster/coronavirus/JHU_count_per_day/cases_2020_03.csv)

COVID deaths per day:

[https://mappingsupport.com/p2/disaster/coronavirus/JHU\_count\_per\_day/deaths\_2020\_03.csv](https://mappingsupport.com/p2/disaster/coronavirus/JHU_count_per_day/deaths_2020_03.csv)

There is an **errata** file on the Hopkins GitHub site where they make a note whenever they change the timeseries data.  I plan to monitor that errata file so I can keep my csv files in sync.  That errata file is at:

[https://github.com/CSSEGISandData/COVID-19/blob/master/csse\_covid\_19\_data/csse\_covid\_19\_time\_series/Errata.csv](https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/Errata.csv)

Finally, if you use these csv files please credit both Johns Hopkins University and Joseph Elfelt, [MappingSupport.com](https://MappingSupport.com) (linked to [https://mappingsupport.com](https://mappingsupport.com))

If something does not look right, the best way to reach me is via the **email** link near the top of this page:

[https://mappingsupport.com/p2/gissurfer-about-contact.html](https://mappingsupport.com/p2/gissurfer-about-contact.html)",61,3,Jelfff,2020-07-14 23:59:56,https://www.reddit.com/r/datasets/comments/hrd1eb/updated_here_are_csv_files_with_johns_hopkins/,0,datasets
hfk1lp,ImgFlip 575000 memes dataset,"I crawled ImgFlip's most popular 100 meme templates and got  **575948** meme examples.  


Nr of memes per template ./dataset/statistics.json

Templates ./dataset/templates

Memes ./dataset/memes

A meme is a json object that includes caption, author, upvotes, views and other.  


Used them for [**generating memes with AI**](https://github.com/schesa/ai-memes).  


Source code and dataset on GitHub:  
[**https://github.com/schesa/ImgFlip575K\_Dataset**](https://github.com/schesa/ImgFlip575K_Dataset)",60,3,schesa,2020-06-25 10:52:13,https://www.reddit.com/r/datasets/comments/hfk1lp/imgflip_575000_memes_dataset/,0,datasets
ga9hrv,"Small dataset for retro pixel game-character generation (~4k images, 64x64x3)","Hello,

I was bored at home and wanted to do DCGAN pytorch tutorial. But I didn't want to go on with standard datasets, so I've created a small dataset for quick&fun experiments.  


Dataset includes 64x64 retro-pixel characters. All characters were generated with [Universal LPC spritesheet by makrohn](https://github.com/makrohn/Universal-LPC-spritesheet/tree/7040e2fe85d2cb1e8154ec5fce382589d369bdb8). Each character in the dataset was randomly generated e.g. random sex, body type, skin color, and equipment with LPC spritesheet with 4 different angles view. 

You can download it, see examples, and read more here: [https://github.com/AgaMiko/pixel\_character\_generator](https://github.com/AgaMiko/pixel_character_generator) 

Hope some of you will enjoy it!",60,4,Chitoyo,2020-04-29 13:31:17,https://www.reddit.com/r/datasets/comments/ga9hrv/small_dataset_for_retro_pixel_gamecharacter/,0,datasets
fve5ta,Dataset: Economic policy measures adopted by 166 countries as a response to the COVID-19 pandemic.,,57,5,smurfyjenkins,2020-04-05 14:01:01,http://web.boun.edu.tr/elgin/COVID_19.pdf,0,datasets
9t5kjp,"Federal Register Notice about intention to add ""noise"" to public use census data in the US","Just received this from IPUMS, it is probably of relevance to US Census Data Users:

I am writing with an urgent request that you respond to [Federal Register Notice 83 FR 34111](https://www.federalregister.gov/documents/2018/10/09/2018-21837/soliciting-feedback-from-users-on-2020-census-data-products-reopening-of-comment-period), regarding the use of Census Bureau data products. The comment period expires in one week on November 8, 2018.

The Census Bureau is planning a fundamental revision of its procedures for ensuring the confidentiality of public use data. The proposed new disclosure avoidance system relies on injecting noise with formal privacy rules, based on theories of differential privacy. It is not clear how this new disclosure avoidance system will be implemented, but there is significant danger that it will reduce or even eliminate the usability of public use data for many common research and policy applications.

It is crucial that the research community responds to the call for comments by explaining how we use the American Community Survey (ACS) and decennial census data. Although the Federal Register Notice only addresses 2020 census products, the Census Bureau is planning to use similar techniques for ACS, and it is vital that they take uses of the ACS microdata into account. Complex applications that use data from multiple levels—such as the characteristics of individuals, their family members, community characteristics, and so on—may be especially relevant to the discussion.

The stakes are high. If we lose statistically valid public census data, it will be an enormous setback to government transparency, social science, and policy formation. I hope that comments from the community will persuade the Census Bureau to proceed with caution and to engage in robust discussion with the research community about the costs and benefits of new disclosure avoidance methods.

Please send at least a brief comment describing your work with microdata or summary files from the ACS or the decennial census. You may respond to the Federal Register Notice by emailing POP.2020.DataProducts@census.gov, referencing [Federal Register Notice 83 FR 34111](https://www.federalregister.gov/documents/2018/10/09/2018-21837/soliciting-feedback-from-users-on-2020-census-data-products-reopening-of-comment-period). I would appreciate it if you copy us at ipums@umn.edu.

Thanks,

Steve Ruggles
IPUMS Director",59,5,Kinost,2018-11-01 02:26:51,https://www.reddit.com/r/datasets/comments/9t5kjp/federal_register_notice_about_intention_to_add/,0,datasets
94alvt,Government of Canada has of a Bunch of Open Data Available. Have fun!,,63,4,ElephantSpirit,2018-08-03 15:36:46,https://open.canada.ca/en/open-data,0,datasets
8v123v,CSV formatted Metadata for over 115 million Twitter user accounts (including all verified accounts and nearly complete for years 2006-2009),,61,6,Stuck_In_the_Matrix,2018-06-30 10:45:57,https://www.reddit.com/r/pushshift/comments/8v0y8l/csv_formatted_metadata_for_over_115_million/,0,datasets
88y954,Hundreds of free datasets from 5 sensors aboard NASA's Terra satellite,,62,2,rocketeeter,2018-04-02 10:12:20,https://lpdaac.usgs.gov/dataset_discovery,0,datasets
7gl72e,Announcing the Initial Release of Mozilla’s Open Source Speech Recognition Model and Voice Dataset,,60,0,cavedave,2017-11-30 09:31:09,https://blog.mozilla.org/blog/2017/11/29/announcing-the-initial-release-of-mozillas-open-source-speech-recognition-model-and-voice-dataset/,0,datasets
6x7nse,"New Yelp dataset for academic use: 4.7M reviews about 156k businesses, with tips, aggregated check-ins and photos.",,63,7,cavedave,2017-08-31 15:46:46,https://www.yelp.com/dataset,0,datasets
5o249x,Foods Typically Purchased by Supplemental Nutrition Assistance Program (SNAP) Households,,60,7,junglejuicy,2017-01-15 04:48:39,https://www.fns.usda.gov/sites/default/files/ops/SNAPFoodsTypicallyPurchased-Appendices.pdf,0,datasets
3jcgc6,Xkcd 'Random' Survey; Data to be made public,,58,28,drewsiferr,2015-09-02 11:39:00,http://www.xkcd.com/1572,0,datasets
3368oh,Great Github list of public data sets,,61,2,urinec,2015-04-19 22:59:45,http://www.datasciencecentral.com/profiles/blog/show?id=6448529%3ABlogPost%3A268197,0,datasets
lo6hul,"For those who are interested/working in materials science, chemical and physical sciences",,61,1,osedao,2021-02-20 12:49:40,/r/materials/comments/lo2zdd/data_resources_for_materials_science/,0,datasets
kcana2,"Introducing rugpullindex.com, the first decentralized data set index in the world",,62,4,TimDaub,2020-12-13 13:31:52,https://timdaub.github.io/2020/12/11/rugpullindex/,0,datasets
ibkbjl,1.32 million daily Irish rainfall values,,61,1,cavedave,2020-08-17 18:53:35,https://rmets.onlinelibrary.wiley.com/doi/10.1002/gdj3.103,0,datasets
hu7mir,"Comments from Reddit that contain the word ""bitcoin"" from 2009 to 2019",,61,4,Jerryfane,2020-07-19 20:35:54,https://www.kaggle.com/jerryfanelli/reddit-comments-containing-bitcoin-2009-to-2019,0,datasets
f2atzm,"Monthly dumps of Wikimedia (including Wikipedia) analytics, including a historical record of revision, user and page events since 2001",,62,0,danwin,2020-02-11 16:35:12,https://dumps.wikimedia.org/other/mediawiki_history/readme.html,0,datasets
996jlo,"Database of Publicly Accused Roman Catholic Priests, Nuns, Brothers, Deacons, and Seminarians in the United States",,59,7,cavedave,2018-08-21 20:09:22,http://www.bishop-accountability.org/Ireland/,0,datasets
8lrgu5,Awesome Public Datasets,,60,4,cavedave,2018-05-24 10:04:49,https://github.com/awesomedata/awesome-public-datasets,0,datasets
6epse3,"All existing Supreme Court oral argument transcripts, with audio links -- annotated line by line with timestamps and speakers",,57,5,21CANNONS,2017-06-01 21:03:03,https://github.com/walkerdb/supreme_court_transcripts,0,datasets
6dk70z,"Stanford Dogs Dataset: over 20,000 images of dogs tagged by breed for your image processing pleasure",,60,11,DataScienceInc,2017-05-26 22:40:01,http://vision.stanford.edu/aditya86/ImageNetDogs/,0,datasets
563dxx,Udacity open-sources (MIT license) 223GB of driving data from its autonomous car program,,61,0,danwin,2016-10-06 02:58:44,https://github.com/udacity/self-driving-car,0,datasets
1hgn40,The last words of every inmate executed in Texas since 1984.,,58,18,RuncibleJones,2013-07-01 23:58:20,http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html,0,datasets
ogwvkl,"Collection of public datasets for NLP, vision, self-driving, medical and other general use",,57,0,None,2021-07-09 14:36:39,https://pub.towardsai.net/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f,0,datasets
lnn02b,SEC Failure To Deliver,"DISCLOSURE: I made this python package

&#x200B;

This python package is essentially an API to a database populated by data that I scraped from the SEC website(os: [https://www.sec.gov/data/foiadocsfailsdatahtm](https://www.sec.gov/data/foiadocsfailsdatahtm)). This is my first time building a python package, database, and using the GCP so if things are not ideal please let me know as I am new to this. I am working on an analysis and it ended up being more efficient to build out an api for myself so I thought i'd make a project out of it and put it towards public use!

Here is the github and the docs: [https://github.com/jc22dora/ftdpack](https://github.com/jc22dora/ftdpack)

EDIT:

Rewording",59,4,jdorandev,2021-02-19 18:35:25,https://www.reddit.com/r/datasets/comments/lnn02b/sec_failure_to_deliver/,0,datasets
ldozc6,How to Create Large Datasets from Reddit Submissions and Comments,"# Introduction

As a Python user, I will be mainly discussing approaches to creating datasets using available Python packages, however, the ideas and packages discussed here can be applied in your language of choice. 

## What is a *Large* dataset

Personally, I would consider a dataset of Reddit submissions or comments *large* if it takes 3600 or more requests to create.  Submission and comment search requests using the Pushshift API return 100 items each, so a large dataset could be considered as anything larger than 360,000 items. Why 3600? At a rate-limit of 60 requests per minute in ideal scenarios, 3600 requests would take an hour to complete, usually, scenarios are not ideal  (slow API response times or failed requests).

# Getting Started

## Pick a Data Source

There are two different options available for getting Reddit data:

1. Directly from Reddit
2. From Pushshift

### Directly from Reddit

Using data directly from Reddit will allow you to have the most up-to-date submissions and comments and you can query via different Reddit feeds (hot, rising, new, top). The main limitation is that you are unable to query for a specific time window as retrieving historical data is unsupported using the Reddit API. 

For Python, a Reddit API wrapper called PRAW exists which allows you to directly query submissions and comments directly using the Reddit API. There appears to be some limit on the data returned while using PRAW, making it difficult to create *large* datasets. From my tests subreddit methods such as `hot`, `top`, and `new` returned at most 9000 submissions with a limit of `None`.

### From Pushshift

Pushshift maintains a database of historical Reddit comments and submissions which you can query via the corresponding Pushshift API. Python wrappers also exist for the Pushshift API: PSAW, and PMAW (made by myself).

# Using Pushshift

In the rest of this post, I will be discussing using Pushshift via either PSAW or PMAW as the ability to query data based on date allows you to compose a *large* dataset of posts with queries that returns all submissions and comments indexed by Pushshift for a specified time period.

Limitations of Pushshift, there a couple of drawbacks to using Pushshift. 

* There is only a single maintainer.
* There are a couple of small windows that are missing some number of comments or submissions due to Pushshift going down - these are rarely backfilled
* Post and comment data can be delayed by up to 2 days, making Pushshift unrealistic for retrieving the most recent submissions and comments

## When NOT to use Pushshift

* You need the most recent submissions or comments from Reddit
* You need less than 10,000 submissions from any one subreddit

## Picking an API wrapper

I'm sure there are many API wrappers that exist for Pushshift, but with regards to Python I will only be discussing PSAW and PMAW.

### PSAW

PSAW is a great wrapper for interacting with the Pushshift API and I have used it in many of my personal projects. This package provides generators for the different API endpoints which allows you to work with the data while you are making requests. However, the main limitation of this library when building a large dataset is the performance. I found I was encountering performance issues when trying to request large numbers of posts or comments, PSAW uses exponential backoff for rate-limiting and I found this was the source of my issues. As well, the paging for `id` based search is not implemented (I'll explain why this is important later).

### PMAW

PMAW, this is a wrapper I created for building large datasets using the Pushshift API. This was created to address the performance limitations of PSAW at the cost of some other nice to have features such as `aggs`, `asc` sort, and the use of generators. PMAW uses multiple threads to make requests and provides 5 different configurations for rate-limiting (rate-averaging, exponential backoff with 3 different types of jitter). 

### PSAW vs PMAW

From this [benchmark](https://github.com/mattpodolak/pmaw/blob/master/examples/img/02-comparison.png) , requesting up to 390,000 submissions, we can see that **PMAW** was on average 1.79x faster than **PSAW** when 390,625 submissions were retrieved. The total completion time for 390,625 submissions with **PSAW** was 2h38m, while the average completion time was 1h28m for **PMAW**. 

# Creating your Dataset

Now that we've covered the background and you've decided to use PMAW to create your dataset, I will cover the process for two different types of datasets: submissions and comments.

## Submissions

A dataset of submissions is straightforward to make, as all you have to do is use a single Pushshift API endpoint to request all your data. This can be implemented using the following Python code:

```python

import pandas as pd

from pmaw import PushshiftAPI

api = PushshiftAPI()

submissions = api.search_submissions(subreddit=""wallstreetbets"", limit=300000)

sub_df = pd.DataFrame(submissions)

sub_df.to_csv('./data/wallstreetbets_subs.csv', header=True, index=False, columns=list(sub_df.axes[1]))

```

## Comments

There are two main ways to approach building a comments dataset. First, we can directly use the `search_comments` endpoint and return comments based on this search or we can return all comments for a set of submissions.

### Approach 1

In this approach we directly search for comments

```python

import pandas as pd

from pmaw import PushshiftAPI

api = PushshiftAPI()

comments = api.search_comments(subreddit=""wallstreetbets"", limit=300000)

comments_df = pd.DataFrame(comments)

comments_df.to_csv('./data/wallstreetbets_comments.csv', header=True, index=False, columns=list(comments_df.axes[1]))

```

## Approach 2

In this approach, we create a dataset of comments based on the submissions we have already retrieved.

```python

subs_df = pd.read_csv(f./data/wallstreetbets_subs.csv',header=0) 

sub_ids = list(subs_df.loc[:, 'id']) 

# retrieve comment ids for submissions

comment_ids = api.search_submission_comment_ids(ids=sub_ids)
comment_ids = list(comment_ids)

# retrieve comments by id

comments = api.search_comments(ids=comment_ids)

comments_df = pd.DataFrame(comments)

comments_df.to_csv('./data/wallstreetbets_comments.csv', header=True, index=False, columns=list(comments_df.axes[1]))

```

# Links

* [Reddit API](https://www.reddit.com/dev/api/)
* [PRAW](https://praw.readthedocs.io/)
* [Pushshift](https://github.com/pushshift/api)
* [PSAW](https://pypi.org/project/psaw/)
* [PMAW](https://pypi.org/project/pmaw/)",59,6,potato-sword,2021-02-06 03:35:34,https://www.reddit.com/r/datasets/comments/ldozc6/how_to_create_large_datasets_from_reddit/,0,datasets
k8gj09,"Dataset: expert-coded measures of regime legitimation strategies (RLS) for 183 countries in the world from 1900 to 2019. Country experts rated the extent to which governments justify their rule based on performance, the person of the leader, rational-legal procedures, and ideology.",,60,0,smurfyjenkins,2020-12-07 13:22:32,https://www.cambridge.org/core/journals/european-political-science-review/article/claiming-the-right-to-rule-regime-legitimation-strategies-from-1900-to-2019/6394D262B708B6199B115DFFEE11BE7C?fbclid=IwAR3VeIRBhr8tmnkHdZyEW1GZgFgwfFKjwgauvnm_ZMnc3BewmAqSF5waFkg#,0,datasets
gsuk7i,[OC] Wholesome Images Dataset,"I made a dataset using the top images of this month on r/aww  


It started as a simple side project to help animal shelters by measuring how ""likable"" an image of a pet is and thus increase adoption. I haven't finished it yet but it was the most fun I had making a dataset so far.

It is available here: [https://www.kaggle.com/andrewmvd/wholesome-images](https://www.kaggle.com/andrewmvd/wholesome-images)   


Cheers",61,6,larxel,2020-05-29 15:01:57,https://www.reddit.com/r/datasets/comments/gsuk7i/oc_wholesome_images_dataset/,0,datasets
dq6xas,"Federal Aviation Administration's Aircraft Registry: records of all U.S. civil aircraft registered with the FAA (~340,000 rows). Also has annual historical archives",,62,1,danwin,2019-11-01 16:52:16,https://www.faa.gov/licenses_certificates/aircraft_certification/aircraft_registry/releasable_aircraft_download/,0,datasets
dpm4ac,Twitter Sentiment Analysis Data,"Access the tweets from IEEE DataPort:  [https://ieee-dataport.org/documents/twitter-sentiment-analysis-data](https://ieee-dataport.org/documents/twitter-sentiment-analysis-data) 

Each database (\*.db) contain three columns. First column: date and time of the tweet, second column: tweet, third column: sentiment score for the particular tweet within the range \[-1,1\] with -1 being the most negative, 0 being the neutral and +1 being the most positive sentiment.

The tweets have been collected by the LSTM model deployed here at [sentiment.live](https://sentiment.live/) \[1\]. The last column, viz. sentiment score, is not the score estimated by the model. The LSTM model is still in the beta phase. Therefore, to make it easy for the NLP researchers to get access to the sentiment analysis of each collected tweet, the sentiment score out of TextBlob \[2\] has been appended as the last column. The sentiment scores produced by our model will be made public after the project's documentation part is finished.

 \####current status###

Tweets containing the term ""iphone 11"": 2 million-plus

Tweets containing the term ""the joker"": 1 million-plus

Tweets containing the term ""housefull 4"": 0.346 million-plus

\##################

A new database will be added there every week. Please bookmark the page for the updates.

**References:**

\[1\] [https://sentiment.live/](https://sentiment.live/)   \[2\] [https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/)",57,4,rabindra-lamsal,2019-10-31 11:23:53,https://www.reddit.com/r/datasets/comments/dpm4ac/twitter_sentiment_analysis_data/,0,datasets
a3u63o,50 Free Machine Learning Datasets: Natural Language Processing,,62,0,devnodegree,2018-12-07 00:14:26,https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da,0,datasets
9i8s5j,[Dataset] Metadata for 69+ Million Reddit Users in CSV format,,59,6,Stuck_In_the_Matrix,2018-09-23 14:33:32,https://www.reddit.com/r/pushshift/comments/9i8s23/dataset_metadata_for_69_million_reddit_users_in/,0,datasets
8s6nqz,All Verified Twitter Users (100% complete) in ndjson format,"This is the complete collection of all verified Twitter user accounts (fully ""hydrated"" user objects) encompassing businesses, media organizations, scientists, artists, musicians, politicians, etc.  The file is ordered by users with the greatest followers_count descending.

Each JSON line contains the complete user object including description, location, number of followers, friends, total status updates, etc.  One additional key was added to each object -- ""retrieved_on.""  This is the epoch time of when the object was ingested.

###This file is for academic use only.

Again, this data cannot be used for commercial purposes at all.  The data is property of Twitter and should only be used for academic / research purposes.  Thanks!

**File:** https://files.pushshift.io/twitter/TU_verified.ndjson.xz

**Size (uncompressed):** 513,564,280

**# of Twitter users in dump:** 297,878

###Example of data:

    {
    ""contributors_enabled"": false,
    ""created_at"": 1235173556,
    ""default_profile"": false,
    ""default_profile_image"": false,
    ""description"": ""Love. Light."",
    ""entities"": {
      ""description"": {
        ""urls"": []
      },
      ""url"": {
        ""urls"": [
          {
            ""display_url"": ""katyperry.com/tour"",
            ""expanded_url"": ""http://www.katyperry.com/tour"",
            ""indices"": [
              0,
              23
            ],
            ""url"": ""https://t.co/UCVo8NkmAc""
          }
        ]
      }
    },
    ""favourites_count"": 6209,
    ""follow_request_sent"": false,
    ""followers_count"": 109581520,
    ""following"": false,
    ""friends_count"": 216,
    ""geo_enabled"": true,
    ""has_extended_profile"": true,
    ""id"": 21447363,
    ""id_str"": ""21447363"",
    ""is_translation_enabled"": true,
    ""is_translator"": false,
    ""lang"": ""en"",
    ""listed_count"": 141426,
    ""location"": """",
    ""name"": ""KATY PERRY"",
    ""notifications"": false,
    ""profile_background_color"": ""CECFBC"",
    ""profile_background_image_url"": ""http://abs.twimg.com/images/themes/theme10/bg.gif"",
    ""profile_background_image_url_https"": ""https://abs.twimg.com/images/themes/theme10/bg.gif"",
    ""profile_background_tile"": false,
    ""profile_banner_url"": ""https://pbs.twimg.com/profile_banners/21447363/1527150186"",
    ""profile_image_url"": ""http://pbs.twimg.com/profile_images/1001159499133149184/mNHEcMbf_normal.jpg"",
    ""profile_image_url_https"": 
    ""https://pbs.twimg.com/profile_images/1001159499133149184/mNHEcMbf_normal.jpg"",
    ""profile_link_color"": ""D55732"",
    ""profile_sidebar_border_color"": ""FFFFFF"",
    ""profile_sidebar_fill_color"": ""78C0A8"",
    ""profile_text_color"": ""5E412F"",
    ""profile_use_background_image"": true,
    ""protected"": false,
    ""retrieved_on"": 1529385670,
    ""screen_name"": ""katyperry"",
    ""statuses_count"": 9216,
    ""time_zone"": null,
    ""translator_type"": ""regular"",
    ""url"": ""https://t.co/UCVo8NkmAc"",
    ""utc_offset"": null,
    ""verified"": true
    }
",59,37,Stuck_In_the_Matrix,2018-06-19 06:26:18,https://www.reddit.com/r/datasets/comments/8s6nqz/all_verified_twitter_users_100_complete_in_ndjson/,0,datasets
66n2cd,Here are a bunch of datasets which are open to public,,59,2,TrashForLife,2017-04-21 04:28:20,https://github.com/caesar0301/awesome-public-datasets,0,datasets
4xrqda,"Immigration to Ellis Island (1892-1924) by trip. 184,000 rows with dates, 350 ports, 8,300 ships and 23 million passengers.",,60,10,hopperrr,2016-08-15 03:34:13,https://github.com/hopperrr/ellis-immigration-by-ship/raw/gh-pages/data/trips.tsv,0,datasets
4azni4,Datasets.co - An open source site to share & discover new datasets.,,57,2,mrborgen86,2016-03-18 18:46:34,http://datasets.co/,0,datasets
14lj7vl,"I have a very large dataset of booze, wines and spirits, wondering who it would be useful to.","I worked with someone who wanted data from one source, finished that project, enjoyed it plenty, so collected and aggregated the data from about 22 other sources. Now I have about 1M unique booze records, 430k wine records and 130k spirits record.

Wondering who i can present value to with this.

**EDIT: Sorrry I forgot to add this. Here are the columns in each**

Wine

**Name,Appellation,Brand/Maker,Wine Type,Varietal,Style,ABV,Taste,Body, Region, Country, \[ratings\], Price, URL**

Whisky & spirits

**name, secondary\_name, full\_name, type\_of\_whiskey,age,flavor\_profile, vintage, category,classification, type\_, cask\_type,  distillery, region, country, bottler,bottle\_series, bottling\_date, abv, rating, rating\_count, price, URL**

Beer

**name,style,abv,brewer,brewer\_country, ratings, average\_quick\_rating, overall\_score, style\_score, price, URL**

Brewery

**brewery\_name, brewery\_rating, brewery\_rating\_count, brewery\_city, brewery\_state, brewery\_country, brewery\_lat, brewery\_lng**

\*NB - *the ratings are coming from 19 to 22 different sites/experts so there are about 19 ratings columns*

*I have updaters for each of these datasets. I also have a 'live drinks menu' extractor for more than 20k bars, restaurants etc which gets the daily available drinks list and prices*

*Ideally, I would want to monetize this, of course, or sell to someone, but would be happy to discuss with other ideas around it as well*",57,22,makelefani,2023-06-28 19:51:07,https://www.reddit.com/r/datasets/comments/14lj7vl/i_have_a_very_large_dataset_of_booze_wines_and/,0,datasets
ziu0d1,Our 2022 FIFA World Cup dataset is trending on Kaggle ⚽,"Hi Guys 👋

The FIFA World Cup, a global football sporting event that takes place every four years, is in Qatar this year. The decision to hold the World Cup in Qatar has sparked several controversies, including allegations of corruption and human rights violations.

We wondered what football fans thought of this tournament. To find out this, we collected tweets from the first day of FIFA World Cup 2022 with the hashtag #WorldCup2022 using [the Sncrape library](https://github.com/JustAnotherArchivist/snscrape).  While scraping the tweets, we performed a sentiment analysis using [the cardiffnlp/twitter-roberta-base-sentiment-latest model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) in Hugging Face Hub.

Our dataset FIFA World Cup 2022 Tweets is trending and Kaggle has featured it on their official Trending Datasets page. With this dataset, you can perform data science projects and NLP analysis. You can find this dataset [here](https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets).

📌 Thanks for reading",56,1,TirendazAcademy,2022-12-11 14:25:16,https://www.reddit.com/r/datasets/comments/ziu0d1/our_2022_fifa_world_cup_dataset_is_trending_on/,0,datasets
yfn8sz,7M+ Venmo transactions scraped from the public API,"Transactions scraped from the [Venmo](https://venmo.com/) public API by [Dan Salmon](https://danthesalmon.com/about/)

This data was collected during the following date ranges:

* July 2018 - September 2018
* October 2018
* Jan 2019 - Feb 2019

While there is no data for the amount transferred, it's interesting to look at most frequently occurring target (receiver) / actor (sender) pairs.

Source: [https://github.com/sa7mon/venmo-data](https://github.com/sa7mon/venmo-data)  
\[Self promotion\] We've re-hosted the data on Gigasheet for exploration before downloading [https://app.gigasheet.com/spreadsheet/Venmo-Transactions-by-Dan-Salmon-github-com-sa7mon-venmo-data/56db56e2\_acb7\_4cc9\_9d7a\_ae308f5a2a06?public=true](https://app.gigasheet.com/spreadsheet/Venmo-Transactions-by-Dan-Salmon-github-com-sa7mon-venmo-data/56db56e2_acb7_4cc9_9d7a_ae308f5a2a06?public=true)",56,5,n1nja5h03s,2022-10-28 12:46:31,https://www.reddit.com/r/datasets/comments/yfn8sz/7m_venmo_transactions_scraped_from_the_public_api/,0,datasets
t44k10,A datadump of more than 120 internal audits from the Irish Health Service detailing irregularities and conflict of interest risks,,60,14,cavedave,2022-03-01 10:02:51,https://www.thestory.ie/2022/03/01/a-datadump-of-more-than-120-internal-audits-from-the-hse-detailing-irregularities-and-conflict-of-interest-risks/,0,datasets
i6fuhi,[self-promotion] We aggregated and indexed almost 2000 image datasets so you don't have to - Bifrost Data Search,"Hi r/datasets!

My team and I launched a website that provides a seamless way to search datasets for your machine learning projects. We’ve all experienced the pain of searching for that perfect dataset. The world's datasets are scattered across academic websites and Github repos. That’s why we came up with Bifrost Data Search.

Bifrost Data Search is an initiative to aggregate, analyse and deliver the world's image datasets straight into the hands of AI developers. You can search from over 1000 listings paired with rich information and in-depth analyses. It’s 100% free and we’re always adding more datasets and features.

This is just a beta release, and we’d love to hear your feedback so we can make this a valuable resource for the community! We're currently live on [https://www.producthunt.com/posts/bifrost-data-search](https://www.producthunt.com/posts/bifrost-data-search).

We really hope you like it!",56,0,Xcrinklecut,2020-08-09 08:12:52,https://www.reddit.com/r/datasets/comments/i6fuhi/selfpromotion_we_aggregated_and_indexed_almost/,0,datasets
hotuet,Dungeons and Dragons Dialogue+Summarization NLP Dataset [CRD3]! (ACL 2020),"We made a Critical Role Dungeons and Dragons dataset for dialogue and summarization (with a lot of possibility for expansion).

&#x200B;

Here is the github:

[https://github.com/RevanthRameshkumar/CRD3](https://github.com/RevanthRameshkumar/CRD3)

&#x200B;

Here is the abstract from the paper:

This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction. For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues. In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.

&#x200B;

Hope you all enjoy it!

&#x200B;

If you have questions, tweet/DM me [@rev\_rameshkumar](https://twitter.com/rev_rameshkumar), or just open a github issue!",59,0,natural_language_guy,2020-07-10 18:06:06,https://www.reddit.com/r/datasets/comments/hotuet/dungeons_and_dragons_dialoguesummarization_nlp/,0,datasets
c1pdcu,CVPR 2019 | Waymo Introduces Open Dataset to Accelerate Autonomous Driving Research,,56,2,Yuqing7,2019-06-17 16:08:17,https://medium.com/syncedreview/cvpr-2019-waymo-introduces-open-dataset-to-accelerate-autonomous-driving-research-38d9dcde032c?postPublishedType=initial,0,datasets
azu52v,3 Year Data Mining Project for Deaths in America Completed!," 

After 3 years of Research, Deaths in the United States Data Mining Project is completed! If there are any questions please ask and I will do my best to answer or point you to the correct source. V1 to V2 was greatly expanded to include FBI & CDC data down to Race, Age, State Specific items. This project has been a HUGE eye opening journey for me. I hope this helps you and educates you like it did me. It will also help in conversations with your friends and fellow citizens that you may disagree with. I will be sending this to all State and Legislation and Social Media Talking heads in hopes that it gets some bites. I can now rest my fingers, mouse, and keyboard. If you want to update this project to 2017+ values, you may do so, but I am now done with this project.

Happy Day!

This project was for me to explore with an open mind that maybe our US firearm culture was incorrect. I went out to prove my biases wrong. It took 3 years, 10s of thousands of clicks and keyboard smashing. For the Firearm guys here, there is a specific Firearms Tab. 

[https://docs.google.com/spreadsheets/d/1E0kkg5kdDoP6xMvXKd9kSLV\_-0m8bluyV5Cp2F\_\_U2c/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1E0kkg5kdDoP6xMvXKd9kSLV_-0m8bluyV5Cp2F__U2c/edit?usp=sharing)",56,12,None,2019-03-11 14:34:22,https://www.reddit.com/r/datasets/comments/azu52v/3_year_data_mining_project_for_deaths_in_america/,0,datasets
ak6zzo,What’s the coolest dataset you’ve ever discovered?,,57,15,NYNYGRDTDYEL,2019-01-27 00:33:34,https://www.reddit.com/r/datasets/comments/ak6zzo/whats_the_coolest_dataset_youve_ever_discovered/,0,datasets
9goikk,Hubble Space Imagery on AWS: 28 Years of Data Now Available in the Cloud,,55,0,lohoban,2018-09-17 21:30:15,https://aws.amazon.com/blogs/publicsector/hubble-space-imagery-on-aws-28-years-of-data-now-available-in-the-cloud,0,datasets
5kfrle,"Cornell Launches Archive of 150,000 Bird Calls and Animal Sounds, with Recordings Going Back to 1929",,56,1,cavedave,2016-12-26 21:02:37,http://www.openculture.com/2013/01/cornell_launches_archive_of_150000_bird_calls_and_animal_sounds_with_recordings_going_back_to_1929.html,0,datasets
54kv4e,Database of copyright removal requests made to Google (1.6GB zipped),,58,4,danwin,2016-09-26 14:34:03,https://www.google.com/transparencyreport/removals/copyright/data/?hl=en,0,datasets
13d34p1,Language models can explain neurons in language models (including dataset),Includes dataset of gpt2 explaining it's neurons,55,4,cavedave,2023-05-09 19:19:23,https://openai.com/research/language-models-can-explain-neurons-in-language-models,0,datasets
135tabl,"New tweet dataset (90M tweets, 150K users)","Huggingface links: [90M tweets](https://huggingface.co/datasets/enryu43/twitter100m_tweets), [150K users](https://huggingface.co/datasets/enryu43/twitter100m_users)

Is is introduced in [this blog]((https://medium.com/@enryu9000/fun-with-large-scale-tweet-analysis-783c96b45df4).

Compared to some of the existing twitter datasets, this one has many tweets per user, which can be useful for some types of analysis.",58,3,enryu42,2023-05-02 16:45:11,https://www.reddit.com/r/datasets/comments/135tabl/new_tweet_dataset_90m_tweets_150k_users/,0,datasets
g8sss6,Apple Mobility Dataset,[https://www.apple.com/covid19/mobility](https://www.apple.com/covid19/mobility),56,1,limchiahau,2020-04-27 03:31:29,https://www.reddit.com/r/datasets/comments/g8sss6/apple_mobility_dataset/,0,datasets
fx5x8j,"Python and R APIs for managing datasets - publishing, tracking, and loading datasets",,57,5,Kaudinya,2020-04-08 12:51:31,https://medium.com/dstackai/dstack-ai-introducing-python-and-r-apis-for-collaborating-on-datasets-ae7719628316,0,datasets
e9qbur,Datasets for Top 10 Visualizations Methods,,58,5,castanan2,2019-12-12 17:07:17,https://towardsdatascience.com/10-viz-every-ds-should-know-4e4118f26fc3,0,datasets
dwmi4l,"Full Labeled Scripts for the television show ""Parks and Recreation""","Full transcripts of Parks and Rec, labeled by Character. Some of you may find it useful, as I'm not sure of any labeled transcripts for the show that exist currently (and I looked pretty hard). I know a lot of you probably like to play around with NLP and the sort for a lot of popular TV shows, so hopefully I can contribute a bit to the body of labeled television transcripts. :)

The page will have more details on the transcript. The data was used for a visualization/analysis of the main characters' dialogue (that you can probably find pretty quickly in my post history).

Link to the dataset: https://www.kaggle.com/heheheluke/parks-and-recreation-scripts",59,4,Sriracquetballs,2019-11-15 06:00:19,https://www.reddit.com/r/datasets/comments/dwmi4l/full_labeled_scripts_for_the_television_show/,0,datasets
by95n7,How a Google Spreadsheet Broke the Art World’s Culture of Silence,,53,24,cavedave,2019-06-08 15:35:43,https://frieze.com/article/how-google-spreadsheet-broke-art-worlds-culture-silence,0,datasets
acrb4j,Hospital Pricing in the US Should Be Easy To Access,"Has everyone seen the new rule on Hospital Prospective Inpatient Pricing for medicare that came into effect Jan 1st, 2019?

&#x200B;

I have been looking for this pricing for personal reasons, and found it incredibly difficult to identify where this information is located on each hospital's website. It seems like they bury it on purpose.  This kind of pricing should be easy to access and in one place for all to see.

Creating pricing transparency empowers consumers of medical services with a choice when selecting a hospital for elective or non-emergency procedures.  It's also a huge step forward in the fight to drive down medical costs in the US.

&#x200B;

That's why I decided to explore creating an API and website that makes this information easy to access for consumers and engineers alike.  Is there any interest in this?  I am still working out the details, but I thought it would be good to start talking this through with my fellow geeks.  I have some pretty good engineering skills and a few friends (also engineers) who are interested in helping as well.

&#x200B;

Thanks!",57,35,gevertex,2019-01-05 05:45:01,https://www.reddit.com/r/datasets/comments/acrb4j/hospital_pricing_in_the_us_should_be_easy_to/,0,datasets
55jlgt,Google releases massive visual databases for machine learning,,56,0,Scary_The_Clown,2016-10-02 18:47:06,https://www.engadget.com/2016/10/01/google-releases-massive-visual-databases-for-machine-learning/,0,datasets
11tdno7,The largest dataset of graded diamonds on Kaggle,"Hi there!

I just put up a new dataset on Kaggle. It's cryptically titled [The largest diamond dataset currently on Kaggle](https://www.kaggle.com/datasets/hrokrin/the-largest-diamond-dataset-currely-on-kaggle)

It has just under 220,000 diamonds and 25 columns of data making it about 3x larger than next largest. I think it's perfect for regression models and there is an attached notebook.

This is my first submission to Kaggle so I'd be very much interested in any feedback you might have.

&#x200B;

Thanks!",57,7,hrokrin,2023-03-17 01:49:30,https://www.reddit.com/r/datasets/comments/11tdno7/the_largest_dataset_of_graded_diamonds_on_kaggle/,0,datasets
ypybyy,How To Cheat At Wordle: Data and Analysis,"In late 2021, the web-based word game [Wordle](https://www.nytimes.com/games/wordle/index.html) exploded in popularity. It quickly grew from 90 users in November, to 2 million in January. Amazingly, it was all done by one Welsh developer. He eventually sold his program to the New York Times for “low seven figures”.

The New York Times [makes public](https://static.nytimes.com/newsgraphics/2022/01/25/wordle-solver/assets/solutions.txt) the list of 2309 potential Wordle solutions. But they provide it in a format that isn't very usable.

\[Self-promotion\] We've transformed and re-hosted the [Wordle cheat data here](https://app.gigasheet.com/spreadsheet/wordle-csv/8374a839_92fa_46b5_9998_57b56be601fc?public=true) and put together a post that details [how to cheat at Wordle](https://www.gigasheet.com/post/you-should-not-learn-how-to-cheat-at-wordle) using this spreadsheet.

Some interesting findings:

* The most likely first letter is S
* Of the 2,309 potential solutions, 29 have a Q
* The most likely letter and position: E, in the fifth position",56,13,n1nja5h03s,2022-11-08 20:59:49,https://www.reddit.com/r/datasets/comments/ypybyy/how_to_cheat_at_wordle_data_and_analysis/,0,datasets
vj7jvo,USAspending.gov - All US federal spending data,,54,0,BasedSweet,2022-06-23 21:23:43,https://www.usaspending.gov/,0,datasets
v1xxiw,More on our hospitals price dataset -- exploring hospital price-gouging through COVID-19 testing prices (Colab notebook in post),,53,3,alecs-dolt,2022-05-31 19:28:17,https://www.dolthub.com/blog/2022-05-31-hospital-price-gouging/,0,datasets
n7eo7n,Dataset: Native American Conflict History (NACH) data on nearly 150 separate conflicts between colonial powers and Native American communities between 1500–1900.,,55,1,smurfyjenkins,2021-05-08 01:55:59,https://journals.sagepub.com/doi/full/10.1177/0022343320987274,0,datasets
lcfxkc,US Congressmember voting history,"Hi, r/datasets. Is there a dataset containing historical voting records of the US Congress? House, Senate, or both is fine. I want to create a political climate index beginning as a univariate analysis of voters crossing party lines. Later, I'll incorporate multivariate methods to improve the utility of the study, but first, I need a data source. Thanks in advance",57,4,PhoenixRising256,2021-02-04 13:44:49,https://www.reddit.com/r/datasets/comments/lcfxkc/us_congressmember_voting_history/,0,datasets
jgjr18,2016 USA Presidential election tweets,"Hello! I'm sharing the full tweets database (61M records, 13G csv file) I collected during US presidential elections in 2016 for my website whoispresident.com. Feel free to play with it, maybe compare with current tweets :) Hope this will be useful to someone.

https://data.world/alexfilatov/2016-usa-presidential-election-tweets",58,0,drwho16,2020-10-23 09:18:17,https://www.reddit.com/r/datasets/comments/jgjr18/2016_usa_presidential_election_tweets/,0,datasets
elgtbk,"Vroom! Vroom! New Dataset Rolls Out 64,000 Pictures of Cars",,52,2,Yuqing7,2020-01-07 20:02:29,https://medium.com/syncedreview/vroom-vroom-new-dataset-rolls-out-64-000-pictures-of-cars-b99ac99843ea,0,datasets
djq0g1,New: All military and non-military coup attempts in the world since 1946.,,56,10,smurfyjenkins,2019-10-18 16:33:48,https://www.johnjchin.com/colpus,0,datasets
c3725b,awesome-public-industrial-datasets,"Hi, we are a machine learning research engineer intern Minkyu Jeon and a data scientist intern Sehee Lee at Makinarocks.

&#x200B;

We would like to publish a repository that has public data we have collected through studies in industrial sector. It will be helpful for the people who are interested in industrial data and, in analysis with open data. Contents are easy to figure out because each data have explanation in the same format of Kaggle/UCI.

&#x200B;

Semiconductor, Chemistry, Machine, and Battery are rising rapidly as domains that Artificial Intelligence are applicable. We had a limitation of getting validated data since not many studies and researches were conducted in these domains so far, but we will keep updating new data and hope this repository is informative.    

&#x200B;

link: [https://github.com/makinarocks/awesome-industrial-machine-datasets](https://github.com/makinarocks/awesome-industrial-machine-datasets)",54,3,redditlineee,2019-06-21 06:21:13,https://www.reddit.com/r/datasets/comments/c3725b/awesomepublicindustrialdatasets/,0,datasets
bedmk3,Datasets concerning the various environmental resources on Earth,,56,2,manawald,2019-04-17 21:58:24,https://resourcewatch.org/,0,datasets
bd7nid,"Here is my Team's Powerlifting Competition Results Data Set. Over 406,975 Lifters from 22,536 Meets.","Hey /r/datasets ! I am a long time lurker here and I've seen all the awesome stuff done here with the different data sets and I figured some of you might enjoy what we have been working on over the last few years. We are a team of powerlifters and developers that have created  [https://www.openpowerlifting.org/](https://www.openpowerlifting.org/) which is a project that aims to create a permanent, accurate, convenient, accessible, open archive of the world's powerlifting data. In support of this mission, all of the OpenPowerlifting data and code is available for download in useful formats. There is no need to scrape this website. OpenPowerlifting is essentially a huge database of powerlifting competition results from all federations worldwide. We are currently sitting at 1,405,349 entries for 407,181 lifters from 22,563 meets. The site is updated with additions and corrections multiple times a day. Our goal is to go back as far as powerlifting history takes us and add in every single recorded competition that has ever happened, plus continue to add all current meets. We are the largest and most accurate database of this sort.

&#x200B;

With that being said, I felt like some people here may want to play around with the dataset! If anyone uses it to make anything, here is a sample attribution text!  Although you are under no requirement to do so, if you incorporate OpenPowerlifting data into your project, please consider adding a statement of attribution, so that people may know about this project and help contribute data. We would also love if you sent us anything you use our data for because our team loves to check out stuff like that and we think its extremely cool to see what can be done with the data! Our e-mail that goes directly to our issue tracker in GitLab is [issues@openpowerlifting.org](mailto:issues@openpowerlifting.org) and our e-mail that goes to the owner is [updates@openpowerlifting.org](mailto:updates@openpowerlifting.org), but our Zulip group chat is the best place to talk to the whole team [https://openpl.zulipchat.com](https://openpl.zulipchat.com/) .

&#x200B;

> This page uses data from the OpenPowerlifting project, https://www.openpowerlifting.org.     You may download a copy of the data at https://gitlab.com/openpowerlifting/opl-data. 

&#x200B;

Here is our data page where you can find the .csv at the bottom:  [https://www.openpowerlifting.org/data](https://www.openpowerlifting.org/data) 

&#x200B;

Here is a link to our brand guidelines from our GitLab repository, which just explains who we are and what we do.  [https://gitlab.com/openpowerlifting/opl-data/blob/master/docs/brand-guidelines.md](https://gitlab.com/openpowerlifting/opl-data/blob/master/docs/brand-guidelines.md) . In a nutshell, we are completely open source, we do not run ads, we do not charge people, and we are entirely ran by contributors/volunteers. We do this for the love of the sport and to make something that can be used to progress it. If you would like to help, please let us know!

&#x200B;

Here is a link to our GitLab repository if you want to check it out and/or contribute:  [https://gitlab.com/openpowerlifting/opl-data](https://gitlab.com/openpowerlifting/opl-data) 

&#x200B;

Here is a link to our team's group chat (if you join, just introduce yourself, your skill set, what you want to help with, and that you came there from this Reddit post):  [https://openpl.zulipchat.com](https://openpl.zulipchat.com/) 

&#x200B;

Here is a link to our Instagram if you want to check that out (we post charts and graphs from time to time):  [https://www.instagram.com/openpowerlifting/](https://www.instagram.com/openpowerlifting/) 

&#x200B;

Lastly, here is a link to our site where we collect all of our related projects, big and small:  [https://www.plsource.org/#](https://www.plsource.org/#) 

&#x200B;

Thank you for your time and for checking out our little project! Please let us know what you think!",53,2,OakleyPowerlifting,2019-04-14 21:10:44,https://www.reddit.com/r/datasets/comments/bd7nid/here_is_my_teams_powerlifting_competition_results/,0,datasets
6rfly2,306 million passwords previously exposed in data breaches (11.9GB),,57,12,danwin,2017-08-03 21:21:13,https://haveibeenpwned.com/Passwords,0,datasets
200ssv,Huge collection (722) of datasets in CSV format with corresponding R packages and documentation,,57,3,NonNonHeinous,2014-03-10 04:02:50,http://vincentarelbundock.github.io/Rdatasets/datasets.html,0,datasets
11zwhs1,"Open database of hospital prices (70 shoppable services, all US hospitals, all insurance companies)",,57,0,alecs-dolt,2023-03-23 20:35:33,https://www.dolthub.com/repositories/dolthub/hospital-prices-allpayers/doc/main,0,datasets
xfwrdf,Dataset of 20m+ automotive classified listings,"A few years ago I built a classified search engine for autos. The dataset spans about 1.5years from 2019ish into 2020 of crawled data mostly from craigslist. There around 21million listings in it, its about 45gb of json uncompressed.

Use it how you wish.

Should be a good place to start for a number of NLP/classification tasks relating to automotive. Some prediction tasks in terms of pricing.

&#x200B;

[https://www.kaggle.com/datasets/winddude/15-years-automotive-classified-from-20192010](https://www.kaggle.com/datasets/winddude/15-years-automotive-classified-from-20192010)

&#x200B;

**Edit: Important Image update!**

I found the images for now they can be found at either \`[https://automudo-img-cdn-dev.sfo2.digitaloceanspaces.com/](https://automudo-img-cdn-dev.sfo2.digitaloceanspaces.com/)\[PATH\_FROM\_\[\_source.images.path\]\] \` or \`[https://images-cdn.automudo.io/](https://images-cdn.automudo.io/2020/10/c2-00039b708ddc964b40cc74c843a47180084d1b43.jpg)\[PATH\_FROM\_\[\_source.images.path\]\]\`. 95% of the images are on \`[https://images-cdn.automudo.io/\`](https://images-cdn.automudo.io/`) check there first. Some may return a restricted error, if you find lots, let me know. If there is interest in the images, please let me know I will export them as there own dataset, but it will take  awhile, due to latency of exporting from s3 based file stores.",57,5,wind_dude,2022-09-16 17:00:01,https://www.reddit.com/r/datasets/comments/xfwrdf/dataset_of_20m_automotive_classified_listings/,0,datasets
vf8tcm,"12 thousand papers on Multiple Sclerosis research manually annotated for relevancy, updated today",,55,0,brunoamaral,2022-06-18 15:40:53,https://gregory-ms.com/downloads/,0,datasets
nske4c,Links to 1211 (and Counting) Interesting Datasets,,55,3,boy_named_su,2021-06-05 01:18:30,https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0,0,datasets
jv8541,US Politicians Twitter Dataset,"Hello.

I made this dataset ([https://github.com/W43GVG/US-Politicians-Twitter-Dataset](https://github.com/W43GVG/US-Politicians-Twitter-Dataset)), I don't know if anyone would be interested in it. It is a dataset based on Twitter usernames of American politicians.",57,9,NeosPhilopator,2020-11-16 14:59:53,https://www.reddit.com/r/datasets/comments/jv8541/us_politicians_twitter_dataset/,0,datasets
i7o7z0,A replication database for economics and social sciences: The ReplicationWiki,,55,6,EconMacro84,2020-08-11 08:47:18,https://blog.repec.org/2020/08/04/a-replication-database-for-economics-and-social-sciences-the-replicationwiki/,0,datasets
fu68b4,Is it somehow possible to automatically scrape the data from these PDFs on a daily basis?," [https://www.google.com/covid19/mobility/](https://www.google.com/covid19/mobility/)   


Each day, I'd like to software or a webpage to scrape the data from each of the PDFs and populate a column in a spreadsheet with this data. This way, you would be able to compare types of mobility between countries and to plot rates of change.",59,37,OculoDoc,2020-04-03 10:47:01,https://www.reddit.com/r/datasets/comments/fu68b4/is_it_somehow_possible_to_automatically_scrape/,0,datasets
fptkgb,I created the following Covid-19 Dashboard based on JHU's dataset. Let me know what graphs I should add!,,55,21,Dezwirey,2020-03-27 08:47:20,https://www.learnfromdata.ai/corona_outbreak,0,datasets
et23l0,Discovering millions of datasets on the web,,55,3,PHealthy,2020-01-24 00:13:01,https://blog.google/products/search/discovering-millions-datasets-web/,0,datasets
dr4fsp,College debt by major dataset?,Would be cool if anyone knew of any,53,8,ggohan,2019-11-03 18:27:18,https://www.reddit.com/r/datasets/comments/dr4fsp/college_debt_by_major_dataset/,0,datasets
c1bsq5,Open Datasets for Autonomous Driving,,52,0,CaptainDevops,2019-06-16 16:30:09,https://scale.ai/open-datasets#1,0,datasets
9mttsm,Massive archive of Tweets related to the Kavanaugh nomination process incoming -- tens of millions of tweets.,"I am creating a huge data set related to the Kavanaugh nomination process.  This dataset spans from mid-September up through the actual confirmation and a few days afterwards.  This dataset will be made available on Friday and will contain tens of millions of tweets (at least -- I'll have a more accurate count soon).  

The entire dataset uncompressed is currently around 350 GB.  The purpose of this dataset is for researchers interested in analyzing public sentiment during the Kavanaugh nomination process.  This archive is meant to be permanently archives so that future historians will have a better understanding of the entire process and a record of the events that transpired during this process.

I will update once this dataset is prepped and ready for download.

Thank you!",58,12,Stuck_In_the_Matrix,2018-10-09 22:37:33,https://www.reddit.com/r/datasets/comments/9mttsm/massive_archive_of_tweets_related_to_the/,0,datasets
85wv0a,"Free datasets from multiple studies on equality, income mobility, race, wealth, etc. Studies by Harvard, Stanford, US Census, etc.",,58,0,rocketeeter,2018-03-20 22:07:29,http://www.equality-of-opportunity.org/data/,0,datasets
63ljhl,"An awesome list of high-quality datasets (news, review, languages and more)",,57,2,rangeva,2017-04-05 13:34:39,https://webhose.io/datasets,0,datasets
37dqo8,"800MB of weekly liquor receipts in Iowa by store, since 2014",,55,6,danwin,2015-05-26 21:53:40,https://gist.github.com/dannguyen/18ed71d3451d147af414,0,datasets
25ga4w,Life Analysis Spreadsheet [x-post from /r/adviceanimals],"It has been suggested that I bring the party to a more suitable subreddit. If you haven't seen the original post, it's [here](http://www.reddit.com/r/AdviceAnimals/comments/25eaab/very_weird_guy_right_here/).

The screenshots of mine (with some data changed for privacy) is [here](http://imgur.com/a/3EcdZ/) and the link to the blank spreadsheet is [here](http://www.mediafire.com/view/34kq3c30ccbwe4l/LA_template.xlsx).

Due to the large number of requests, I'll put in a quick tutorial on how to use it when I get off work tonight. For the most part it's self explanatory but I had to remove a couple things in order to make it work properly. At some point, I'll build it back up so anyone can use it.

I figured the discussion can continue here instead of the /r/adviceanimals subreddit.",55,9,batting_1000,2014-05-13 14:50:06,https://www.reddit.com/r/datasets/comments/25ga4w/life_analysis_spreadsheet_xpost_from/,0,datasets
10gdzef,Shrinking the insurance data dump: a data pipeline to deduplicate trillions of insurance prices into a single database (available),,52,0,alecs-dolt,2023-01-19 21:34:08,https://www.dolthub.com/blog/2023-01-11-mrf-data-deduplication/,0,datasets
tx2gm7,Dataset of /r/place canvas snapshots uploaded to kaggle for all to explore,"I created a kaggle dataset with image snapshots of the place canvas: [https://www.kaggle.com/datasets/robikscube/reddit-rplace-2022-history](https://www.kaggle.com/datasets/robikscube/reddit-rplace-2022-history)

One of the benefits of having it on kaggle is you can create notebooks with python code to run analysis in the cloud. This is an example notebook I created with some starter python code. The code pulls the file metadata, shows how to display the images and creates an example heatmap: [https://www.kaggle.com/code/robikscube/reddit-s-r-place-2022-history](https://www.kaggle.com/code/robikscube/reddit-s-r-place-2022-history)

The data source is credited to [u/prosto\_sanja](https://www.reddit.com/u/prosto_sanja/)

* his site: [https://place.thatguyalex.com/](https://place.thatguyalex.com/)
* his code to pull the images: [https://github.com/ProstoSanja/place-2022](https://github.com/ProstoSanja/place-2022)

I'm expecting that eventually reddit will release a more official version of the data which I also plan on uploading to kaggle, but this dataset should hold us over until then.",55,0,robikscuber,2022-04-05 19:08:33,https://www.reddit.com/r/datasets/comments/tx2gm7/dataset_of_rplace_canvas_snapshots_uploaded_to/,0,datasets
nbd3dg,I made git-pull: a tool for scraping data from the Github website,,54,5,jsonathan,2021-05-13 10:00:08,https://github.com/shobrook/git-pull,0,datasets
kycdoa,"PC Part Dataset - 43,000+ parts scraped from PCPartPicker available as JSON",,57,3,None,2021-01-16 04:57:55,https://github.com/docyx/pc-part-dataset,0,datasets
kpa6b6,wrangling new hospital price transparency data,"A new law took into effect stating that hospitals must publish their service prices to ""empower patients and increase competition among hospitals"". You can read the article [here](https://www.hhs.gov/about/news/2019/11/15/trump-administration-announces-historic-price-transparency-and-lower-healthcare-costs-for-all-americans.html).

I'd like to collect this data and make some sense of it. There are 2 big challenges here: 1) getting the data and 2) making sense of it. I'm figuring out a way to do #1 - I have a list of 3300 hospitals (with the majority including a website) and am figuring out how to find the .csv/.xlsx documents (they are somewhere on the website). This is something I'm pretty confident I can do. However, I have no clue how to wrangle the data - especially since every hospital is going to have it formatted their own way. Here are 3 examples (google docs).

[Example 1](https://drive.google.com/file/d/1_-FPcLjB06TxBRDbTVsiiDwb_DE87vvK/view?usp=sharing)

[Example 2](https://drive.google.com/file/d/1ZDO_6iaNNGqpGiBhtNo-QacAaqjgSbT1/view?usp=sharing)

[Example 3](https://drive.google.com/file/d/1NHIREbvaqC_jGbwHfPE4h35TcvF_ACtw/view?usp=sharing)

I'm looking for some help here. I'm hoping you guys can give me some direction, or better yet, if someone would like to work with me on this that would be 100x better. The goal is create some meaningful insights/reports for consumers and hospitals.",53,54,imallinboozie,2021-01-03 01:05:18,https://www.reddit.com/r/datasets/comments/kpa6b6/wrangling_new_hospital_price_transparency_data/,0,datasets
g1gzof,Apple COVID-19 Mobility Trends Data,"Apple has released its own mobility trends data from Apple Maps that can be downloaded as CSV. Updated daily

 [https://www.apple.com/covid19/mobility](https://www.apple.com/covid19/mobility)",54,12,LongTermMetabolite,2020-04-15 00:02:48,https://www.reddit.com/r/datasets/comments/g1gzof/apple_covid19_mobility_trends_data/,0,datasets
fxvs04,Reddit Imposter April Fools Dataset - 5 Million Games of the recent Reddit April Fools project. JSON and Mysql Exports.,,57,0,xJRWR,2020-04-09 16:43:02,http://spacescience.tech/,0,datasets
bi2d5c,This 20-Year-Old Is Archiving Thousands of Flash Banner Ads From the Early 2000s,,52,2,cavedave,2019-04-27 19:02:01,https://motherboard.vice.com/en_us/article/7x3d7d/flash-banner-ad-archive,0,datasets
9nkfvh,Kavanaugh Twitter Dataset - pushshift.io,,55,17,Stuck_In_the_Matrix,2018-10-12 13:50:49,https://pushshift.io/kavanaugh-twitter-dataset/,0,datasets
6qwc54,17GB Steam Data Set,,55,7,preclol,2017-08-01 13:50:08,https://steam.internet.byu.edu/,0,datasets
565uvo,NBA Player Tracking Data 2015-2016 Season (94GB of player and ball locations 25 frames/second).,,57,8,sealneaward,2016-10-06 15:13:35,https://github.com/sealneaward/nba-movement-data,0,datasets
10s1xg2,Interesting UFO Sightings Dataset from Kaggle,"NUFORC geolocated and time standardized ufo reports for close to a century of data. 80,000 plus reports. Some interesting data for USA/Canada UFO sightings posted by Kaggle. Data contains city, state, time, description, and duration of each sighting. I found it interesting to sort by location and duration of each sighting.  Data contains city, state, time, description, and duration of each sighting. Phoenix and Seattle are the top two cities that have reported UFO sightings since the early 2000s. What do you guys think? Any interesting correlations within the data? 

View Data: [Gigasheet: UFO Sightings](https://app.gigasheet.com/spreadsheet/UFO-Sightings-Data/0cb3e2fa_a9d3_4687_b4bb_cbda4c236bad) 

Source:  [UFO Sightings | Kaggle](https://www.kaggle.com/datasets/NUFORC/ufo-sightings)",54,11,sheetheadd,2023-02-02 21:47:54,https://www.reddit.com/r/datasets/comments/10s1xg2/interesting_ufo_sightings_dataset_from_kaggle/,0,datasets
rb80xn,Historical gold price from 1791 to 2020 in USD,,57,1,Beautiful_Blood,2021-12-07 20:00:01,https://www.kaggle.com/joseserrat/yearly-gold-prices-from-1791-to-2020-in-usd,0,datasets
l89jcn,Anonymized MeetMindful leak subset (700k rows),"I made an anonymized version of the MeetMindful leak (no names, GPS coordinates, IP, password hashes, emails, ...) because the remaining data (age, height, dating preferences, ...) were still interesting to me, so they could be of interest to others. 



The full leak is actually about 1.4M entries (not 2.28M as news articles would have you believe -- they just looked at the line count of the csv without realizing there are many multi-line entries). I restricted to the subset of rows for people in the United States (0.7M) just because it's more familiar to me. I dropped columns that were very sparse. Finally, I added a boolean ""pwned"" column which is true if the email was found in the ""haveibeenpwned"" dataset of 1B+ email:password pairs.



45MB csv.gz file: [Link 1](https://gofile.io/d/tj8pGl) or [Link 2](https://anonfiles.com/lcy1k2zdu7/mm.csv_gz)


A few cool (sad?) insights...

* [Users with gmail emails are young in contrast to those with aol emails.](https://i.imgur.com/PoOHOEa.png?maxwidth=760&fidelity=grand)

* [Boomers forget their passwords more often?](https://i.imgur.com/PJEBOsC.png?maxwidth=760&fidelity=grand)

* [Older = more likely to be in the ""haveibeenpwned"" dataset](https://i.imgur.com/YCRly1X.png?maxwidth=760&fidelity=grand)

* [Short men tend to have a larger search/match radius :( Same for tall women.](https://i.imgur.com/oWpq6k0.png?maxwidth=760&fidelity=grand)",53,19,typhoidisbad,2021-01-30 00:49:49,https://www.reddit.com/r/datasets/comments/l89jcn/anonymized_meetmindful_leak_subset_700k_rows/,0,datasets
kxsgdw,Exploring cocktail data with R Package,,55,0,cavedave,2021-01-15 11:06:36,https://juliasilge.com/blog/cocktail-recipes-umap/,0,datasets
k9n7jj,[self-promotion] RecipeNLG: over 2M cooking recipes ready for text generation tasks,,52,4,michalbien,2020-12-09 07:07:12,https://github.com/Glorf/recipenlg,0,datasets
jth92x,The most comprehensive TED Talks Dataset,"[Dataset - Home Page](https://www.kaggle.com/thegupta/ted-talk)

[Scrapper](https://github.com/The-Gupta/TED-Scraper/blob/master/Scraper.ipynb)  


Each row corresponds to a Talk on TED.com and each column details Metadata (generic/speaker/talk related information) and Transcript. 

It includes [media files](https://drive.google.com/drive/folders/1clqw9izazxafPDuIekXQYYdI-J42VvCR) (images, audio, and video) too! *TED\_Talk.xlsx* and *TED\_Talk.csv* contain Metadata and Transcript. Folder Names are intuitive.  All media files are named by *talk\_\_id*, except in *PHOTO\_\_SPEAKER* files are named by *speaker\_\_id* of the primary Speaker. 

Use `ast.literal_eval()` to access a cell value in a Data Structure. See examples [here](https://github.com/The-Gupta/TED-Scraper/blob/master/Scraper.ipynb), under the header *How to access a NESTED column value*.

  
Please give feedback to improve it and ideas to work on and try on this dataset.",56,2,TheGupta,2020-11-13 15:09:58,https://www.reddit.com/r/datasets/comments/jth92x/the_most_comprehensive_ted_talks_dataset/,0,datasets
g04d3u,"GuitarSet, a dataset that provides high-quality guitar recordings alongside rich annotations and metadata",,57,2,yaph,2020-04-12 20:41:38,https://zenodo.org/record/3371780,0,datasets
e707d2,205 projects in 40 countries of subway construction costs,,54,0,cavedave,2019-12-06 15:30:41,https://marginalrevolution.com/marginalrevolution/2019/12/why-dont-we-know-more-about-subway-infrastructure-costs.html,0,datasets
c2tcv5,Roughly 15K high-res images of different types of clothing pieces,,54,1,Sig_Luna,2019-06-20 08:13:46,https://www.kaggle.com/dqmonn/zalando-store-crawl,0,datasets
aisbdy,World of Warcraft Auction Data - release advice,"I've been collecting hourly snapshots of auction house data from World of Warcraft (US servers only) for about 2.5 years now and have about 1TB of data as ~1.8M bz2 compressed JSON files ([sample file](https://gist.github.com/aheadley/4f037e32c874c1ae7584a0c78fb5cf26)). I'd like to make this publicly available but I'm not sure of the best way to go about it and was looking for input. Specifically:

1. Is there even any interest in this data?
2. What is the best method to provide the data? Would a single torrent be preferred, or should I upload it into Google's BigQuery? or something else?",53,25,not_that_guy_either,2019-01-22 22:15:44,https://www.reddit.com/r/datasets/comments/aisbdy/world_of_warcraft_auction_data_release_advice/,0,datasets
9rt2i6,A mighty dataset to study the Dark Web!,"[https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015](https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015)

This dataset by Philip James on Kaggle is not so popular yet and I don't understand why! We could learn so much about the happenings on the Dark Web and about vendors and their locations and the products they sell!

This is essentially the Dark Market - Revealed! Hope you would like the dataset. Let's discuss ideas about what all information can we extract and use in order to curb the dark market activities!",50,3,adityashrm21,2018-10-27 09:36:23,https://www.reddit.com/r/datasets/comments/9rt2i6/a_mighty_dataset_to_study_the_dark_web/,0,datasets
9kevfc,Google Released Datasets Search,"Google’s released a dataset search tool that might be useful to some of you:

https://toolbox.google.com/datasetsearch

Happy searching, and if you find anything interesting, remember sharing is caring!",54,5,None,2018-10-01 09:34:33,https://www.reddit.com/r/datasets/comments/9kevfc/google_released_datasets_search/,0,datasets
85m66h,Command Line Interface for all historical NOAA GHCN Weather Data - Python source code in comments,,50,3,rocketeeter,2018-03-19 19:17:24,https://github.com/aaronpenne/get_noaa_ghcn_data,0,datasets
7ylgwo,"Rijksmuseum Digitizes 600,000+ Works of Art, Making Masterpieces Available Online",,52,1,cavedave,2018-02-19 09:17:14,https://mymodernmet.com/rijksmuseum-digital-archives/,0,datasets
5tmmz8,Internet Archive Book Images - Searchable database of 12 million historic copyright-free images sourced from more than 600 million library book pages,,52,0,surlyq,2017-02-12 17:27:20,https://www.flickr.com/photos/internetarchivebookimages/,0,datasets
5lzzm8,White House Visitor access records through 2016 (5.99 million total),,50,3,danwin,2017-01-04 16:17:30,https://www.whitehouse.gov/briefing-room/disclosures/visitor-records,0,datasets
4wv0jz,"Data Packaged Core Datasets: A Github collection of ""important, commonly-used datasets in high quality, easy-to-use & open form as data packages""",,53,3,danwin,2016-08-09 07:39:16,https://github.com/datasets,0,datasets
z7y9ap,Politically Exposed Persons (PEPs) Data Set,"This data comes from [OpenSanctions.org](https://OpenSanctions.org): ""A politically exposed person (PEP) is a person that has been entrusted with a prominent public function. PEPs include elected officials, members of government.

Integrating data about political actors is an essential step in making an open source due diligence database. However, it is a much more intricate task than collecting sanctions lists (of which there are only a few dozen), and fully addressing it will be the focus of a later stage of this project.""

You can find the data here on more than 197,000 entities: [https://www.opensanctions.org/datasets/peps/](https://www.opensanctions.org/datasets/peps/)

I've re-hosted the ""Targets as simplified CSV"" with more than 170,000 people records for exploration:  
 [https://app.gigasheet.com/spreadsheet/Politically-Exposed-Persons--PEPs---opensanctions-org/862d21cf\_6eb3\_44db\_953b\_33b9324527e6?public=true](https://app.gigasheet.com/spreadsheet/Politically-Exposed-Persons--PEPs---opensanctions-org/862d21cf_6eb3_44db_953b_33b9324527e6?public=true)",50,9,n1nja5h03s,2022-11-29 16:13:00,https://www.reddit.com/r/datasets/comments/z7y9ap/politically_exposed_persons_peps_data_set/,0,datasets
w340kj,Dataset of Job Descriptions For Your Pleasure,"I recently created a dataset with over 100k job descriptions scraped from LinkedIn, Indeed, Glassdoor and Monster Jobs. Here's the link to the Google Drive!

[Dataset](https://drive.google.com/drive/folders/1XxNuhiei5taFR6gziofYAx0oWfGeV7y9?usp=sharing)",52,10,totallytubulartoast,2022-07-19 21:25:49,https://www.reddit.com/r/datasets/comments/w340kj/dataset_of_job_descriptions_for_your_pleasure/,0,datasets
pdhbl3,Covid death and hospitalisation by vaccine status,,53,19,cavedave,2021-08-28 20:30:40,https://mobile.twitter.com/redouad/status/1429382037362851843,0,datasets
m6w3fm,Predicting t-shirt size from height and weight,,53,6,cavedave,2021-03-17 08:59:09,https://tylerburleigh.com/blog/predicting-t-shirt-size-from-height-and-weight/,0,datasets
ifsjf5,"Dataset: The Tax Introduction Dataset (TID) lists the year and the mode of the first permanent introduction of six major taxes (inheritance tax, personal income tax, corporate income tax, social security contributions, general sales tax and value added tax) in 220 countries, 1750–2018.",,55,1,smurfyjenkins,2020-08-24 16:41:30,https://link.springer.com/article/10.1007/s11558-019-09359-9,0,datasets
h0bb2a,[help needed] Every line from every episode of It's Always Sunny in Philadelphia,"I'm trying to put together a sunny scripts database that can be used for text analysis (like the one misunderstoodpoetry made for [the office](https://docs.google.com/spreadsheets/d/18wS5AAwOh8QO95RwHLS95POmSNKA2jjzdt0phrxeAE0/edit#gid=747974534)).

I've got the dataset for all the dialogues spoken across the 14 seasons but need help annotating who the speaker for each line is.

I need your help to 1) watch the show 2) take down the speaker for each line of dialogue

Even if it's just for 1/2 episodes, I'd really appreciate the help! And of course, I'm more than happy to share the dataset once it's complete :)

Just comment below/DM me if you're keen to help and i'll share the gdoc files with you!

(if you're a reddit lurker without an account you can message me on [twitter](https://twitter.com/sha_chowdery) instead)",56,31,sha_chowdery,2020-06-10 13:47:03,https://www.reddit.com/r/datasets/comments/h0bb2a/help_needed_every_line_from_every_episode_of_its/,0,datasets
ezvrjf,[OC] Iowa Caucuses Precinct-By-Precinct 97% Reporting,,49,11,None,2020-02-06 17:35:51,https://pastebin.com/Wk0u0BFL,0,datasets
dgs8t3,Continuously updating json list of top 100 posts and 2500 hot posts of each hour of reddit (Since April 2019),,52,2,None,2019-10-12 08:08:45,https://www.dropbox.com/sh/n1hy49sd19oth7d/AADpGgU3LBDE3ZI3El6NLkXia?dl=0,0,datasets
dbwcpy,What Does ‘Broken’ Sound Like? First-Ever Audio Dataset of Malfunctioning Industrial Machines,,52,3,Yuqing7,2019-10-01 17:02:54,https://medium.com/syncedreview/what-does-broken-sound-like-first-ever-audio-dataset-of-malfunctioning-industrial-machines-b4f8f6d81dd7,0,datasets
d3trik,Google: Open source and open data,,52,4,fhoffa,2019-09-13 18:51:45,https://www.blog.google/technology/research/open-source-and-open-data/amp/,0,datasets
bn4z93,Housing Related Data,"Hello everyone,

&#x200B;

I've found myself doing a lot of housing research lately, and I'd like to share some of my sources in hopes that you all may have some to share with me.

&#x200B;

The Department of Housing and Urban Development has some interesting data available. I used their foreclosure estimates for some exploratory work, but It's nothing good enough to publish on. There's also a lot of overlap between this and what the Census Bureau surveys.

[Housing and Urban Development](https://www.huduser.gov/portal/pdrdatas_landing.html#dataset-title)

[Census Housing Data](https://www.census.gov/topics/housing/about.html)

&#x200B;

Zillow offers a lot of cool data for home values and sales. If you have a professional affiliation (Mostly Academic) you can access their ZTRAX database. I haven't yet gotten access, but it offers 20 years of  

[Zillow - Public](https://www.zillow.com/research/data/)

[Zillow - Professional](https://www.zillow.com/research/ztrax/)

&#x200B;

Trulia also offers data about livability, amenities and policy. I haven't looked much into these data yet, but it may be helpful to know that Trulia and Zillow are part of the same parent company. Beware, the website is broken in several places at the time of writing. Some data can be found within articles and downloaded through Tableau. (UPDATE: I found their tableau page.)

[Trulia - Data](https://www.trulia.com/blog/data-portal/)

[Trulia - Tableau Public](https://public.tableau.com/profile/trulia.economic.research#!/)

&#x200B;

Realtor.com also has research data, again haven't used this yet.

[Realtor.com - Research Data](https://www.realtor.com/research/data/)

&#x200B;

Redfin has an impressive amount of data, including some data for Canada. I think you have to download through Tableau, which is ok but a slight annoyance. I also included their methodology, which gives you the original sources if you need.

[Redfin - Data](https://www.redfin.com/blog/data-center/)

[Redfin - Data Sources](https://www.redfin.com/about/data-quality-on-redfin)

&#x200B;

Because of the Home Mortgage Disclosure Act (HMDA), the majority of financial institutions must report certain information about loan applications. This includes loan value, applicant race and gender, reason for denial and whether a third part (Fannie May/Freddie Mac, etc.) bough the loan. The Loan/Application Register (LAR) is pretty large (128Gb for 2007-2017). There are an average of 17 million observations per year from 2007 to 2017. Beware the data portal is changing soon. Also, if anyone is interested, I've aggregated the data spatially, PM me. The Consumer Financial Protection Bureau also offers data on mortgage performance.

[Consumer Finance - HMDA](https://www.consumerfinance.gov/data-research/hmda/)

[Consumer Finance - Mortgage Performance](https://www.consumerfinance.gov/data-research/mortgage-performance-trends/)

&#x200B;

The Federal Housing Finance Agency has data related more specific to regulations. You can find what institutions are allowed to make home loans, as well as regional loan limits and ares of opportunity and concentrated poverty.

[Federal Housing Finance Agency](https://www.fhfa.gov/DataTools/Downloads)

&#x200B;

The Federal Reserve Bank of St. Louis also offers some interesting economic data. There is a ton here, but a lot of it is regional. The filters are helpful for narrowing down the data. Also a lot of overlap from Census, HUD and FHA.

[FRED - Housing](https://fred.stlouisfed.org/tags/series?t=housing)

&#x200B;

The OECD offers some housing related data, but I don't know much about it.

[OECD - Housing](https://data.oecd.org/searchresults/?hf=20&b=0&q=housing&l=en&s=score)

&#x200B;

The National Association of Realtors has some sale, affordability and price data.

[NAR - Data](https://www.nar.realtor/research-and-statistics/housing-statistics)

&#x200B;

There are also a few paid datasets that I have come across. I don't really advocate buying data, especially since it's usually  just aggregated from public sources. However, if you are in a situation where you have more funding than time, by all means, buy the data. If you have an academic affiliation, CoreLogic offers a discount I believe. I've never bought either of these datasets, so YMMV.

[CoreLogic](https://www.corelogic.com)

[Attom Data](https://www.attomdata.com/)

&#x200B;

Something else I thought was worth mentioning is that many large cities have 311 systems. If you didn't know, a 311 system is a way to report non-emergency problems. They're interesting for housing research because often times they contain data about vacancy or homelessness. This is totally dependent on region, so I can't link all of them. But, I'll link New York City's which is the largest, and you can see every city with a 311 system on wikipedia.

[Wikipedia - 311 Availability](https://en.wikipedia.org/wiki/3-1-1#Availability)

[NYC - 311 Open Data](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9)

&#x200B;

If you are doing regional work on a large city, there's a good chance someone is involved in the area. I know New York City and Portland have serious housing research efforts. I've been told the same is true of Baltimore and Boston. If you or someone you know is doing regional work or you would like help finding this, PM me.

&#x200B;

If anyone has datasets that I've missed and pertain to housing data, I would greatly appreciate them. I'd prefer national data, but please post local data as well. It would be really neat if we could create a master list of housing data. I'm not including it here because its usually against company Terms and Conditions, but you reasonably scrape the data (PM me on this too).",52,7,BranFlake5,2019-05-10 22:50:44,https://www.reddit.com/r/datasets/comments/bn4z93/housing_related_data/,0,datasets
5r8xge,Names and citizenship status of all perpetrators of terrorist attacks against the U.S. and Americans abroad since 2001,,50,15,datadotworld,2017-01-31 15:48:32,https://data.world/carlvlewis/terrorism-cases-2001-2016,0,datasets
5c6lg4,"US county-level data: demographic, social, crime and with matching election data from 08, 12 and 16 presidential elections",,52,22,Deleetdk,2016-11-10 07:14:00,https://github.com/Deleetdk/USA.county.data,0,datasets
4hwa35,Osha publishes a csv of workplace deaths with a single sentence description of what happened,,50,11,cavedave,2016-05-04 19:50:15,https://www.osha.gov/dep/fatcat/FatalitiesFY10.csv,0,datasets
zr5cfx,Working with large CSV files in Python from Scratch,,52,7,ramses-coraspe,2022-12-21 00:45:48,https://coraspe-ramses.medium.com/working-with-large-csv-files-in-python-from-scratch-134587aed5f7,0,datasets
xnmai3,[Synthetic] Ai-generated faces. 170k faces generated using AI,,50,8,misteick,2022-09-25 12:39:42,https://github.com/RichardErkhov/ai_generated_faces,0,datasets
wkacj6,"Dataset: All US Military Interventions, 1776–2019.",,50,4,smurfyjenkins,2022-08-09 18:11:31,https://journals.sagepub.com/doi/10.1177/00220027221117546,0,datasets
uzo90t,Free zip code database with Census data,"A couple weeks ago, I shared my site, EverythingByZipCode.com, which is a zip code database that spans nearly 900 columns wide across multiple public government sites. I posted it to get feedback on the database and the general concept.

Link of the original post:

https://www.reddit.com/r/datasets/comments/uh6g2b/free_zip_code_database_800_columns/

Based on some feedback, I’ve added a free option of the same database, but a slimmed down, standard version that isn’t as extensive. It’s actually the same database that cost $40-$50 on other sites, like zip-codes.com. 

Free option:

https://www.everythingbyzipcode.com/product/free-zip-code-database-lookup-file

It’s free and up to date! Enjoy!

If you have any recommendations, feel free to DM me on Twitter @bresslertweets.

David",51,20,dabressler,2022-05-28 14:16:45,https://www.reddit.com/r/datasets/comments/uzo90t/free_zip_code_database_with_census_data/,0,datasets
u1ch1b,ICIJ reveals more than 800 Russians behind secret companies in landmark expansion of public offshore database - ICIJ,,52,2,cavedave,2022-04-11 17:05:20,https://www.icij.org/investigations/pandora-papers/alpha-offshore-leaks-database-pandora-papers-russia/,0,datasets
t8trap,[Self promo] Free database of US housing sales records (scraped from govt sources),"We just finished our data bounty for US housing sales records. :-) There's a ton of cool stuff in here, we'll have a blog written about it later in the week. 

[https://www.dolthub.com/repositories/dolthub/us-housing-prices/](https://www.dolthub.com/repositories/dolthub/us-housing-prices/pulls)

Note that we didn't get this from Zillow/Redfin, but built this whole thing from scratch. Sources are in the db.

Here's how we did it and what we learned (you can also see how much our contributors earned):

[https://www.dolthub.com/blog/2022-02-24-us-housing-prices-retrospective/](https://www.dolthub.com/blog/2022-02-24-us-housing-prices-retrospective/)

We're moving on to building a similar database for hospital prices. Check it out and see if you're interested. [https://www.dolthub.com/repositories/dolthub/hospital-price-transparency-v3](https://www.dolthub.com/repositories/dolthub/hospital-price-transparency-v3)",49,7,alecs-dolt,2022-03-07 16:58:54,https://www.reddit.com/r/datasets/comments/t8trap/self_promo_free_database_of_us_housing_sales/,0,datasets
jyh7rd,Two Sigma: Using News to Predict Stock Movements,"Hi guys, does anyone have [this dataset](https://www.kaggle.com/c/two-sigma-financial-news) or anything at least remotely similar? I just want to play around with it, but cannot find anything that is somewhat usable. I would be extremely thankful for any info on this or similar news sentiment/stock analysis datasets.",48,3,MMorte,2020-11-21 19:47:19,https://www.reddit.com/r/datasets/comments/jyh7rd/two_sigma_using_news_to_predict_stock_movements/,0,datasets
ilau45,"US citizen naturalizations by USCIS field office, 2013-2020","This collects together all the quarterly reports from the [USCIS data portal](https://www.uscis.gov/tools/reports-and-studies/immigration-and-citizenship-data) into a few usable CSVs, plus makes some charts with it. There's a sharp uptick in pending applications after 2016. Also including the code used to scrape and concatenate this data together, though it could probably do with some cleanup.

edit: the link [https://github.com/dovinmu/USCIS-data](https://github.com/dovinmu/USCIS-data) ",53,5,dovin,2020-09-02 17:06:59,https://www.reddit.com/r/datasets/comments/ilau45/us_citizen_naturalizations_by_uscis_field_office/,0,datasets
ijgj27,Is there a dataset of Hogwarts house points given over the series?,"Thought such a dataset might exist since Harry Potter is a popular subject for data science, but no dice. All I've found is a static infographic produced by Pottermore that I could use to create this dataset, but it would be tedious.

Thought I would ask! Maybe someone knows something I don't.

EDIT: When in doubt, check the [fandom wiki](https://harrypotter.fandom.com/wiki/House_points#:~:text=House%20points%20were%20awarded%20to,an%20inter%2Dhouse%20Quidditch%20match.&text=Each%20student%20earned%20points%20for,be%20awarded%20the%20House%20Cup).",55,8,vastava_viz,2020-08-30 17:24:15,https://www.reddit.com/r/datasets/comments/ijgj27/is_there_a_dataset_of_hogwarts_house_points_given/,0,datasets
ga2zzh,I learnt Plotly Dash so I could make a dashboard for COVID-19. Let me know how it is :),,51,15,sandeshpatkar,2020-04-29 04:44:24,https://corona-virus-dash-board.herokuapp.com,0,datasets
g14sz2,Pokemon Gen 1-8 Dataset,,51,1,shahinrostami,2020-04-14 12:40:54,https://github.com/shahinrostami/pokemon_dataset,0,datasets
fuo64p,Google covid-19 mobility reports -- time series data,"The [Google covid-19 mobility reports](https://www.google.com/covid19/mobility/) only have trend numbers (""+-x%"") for the last day. I haven't yet seen any data on this sub with the full time series, so I spent today parsing the pdfs for the full time series for each county/state in the US. Sorry if this has already been done and I missed it.


A 2MB gzipped json file with the data can be downloaded from [here](http://s000.tinyupload.com/index.php?file_id=15151458631496445363). The ~300k entries are of the form:

    [
      {
        ""state"":""New York"",
        ""county"":""Albany County"",
        ""category"":""parks"",
        ""page"":2,
        ""change"":-15,
        ""changecalc"":-15.4320966667,
        ""date"":""2020-03-03"",
        ""value"":42.505
      },
    ]


Some notes:

* ""page"" is the page number of the pdf with that county

* ""change"" is  the trend as reported by google (based on the last data point, it seems), and ""changecalc"" is what I calculated with the parsed time series

* I also included the ""US"" as a ""state"", where the ""county"" field is the state name



[This](https://gist.github.com/Amarang/3341c9a24da4556def7c3a03a12949b8) is the really hacky notebook I used to download/parse the data. I parsed the pdf graphic objects to get the blue line coordinates and then did some hardcoded massaging. Only plots that don't have gaps or messages like ""not enough data..."" were parsed and included in the json.


There are a couple of validation plots overlaying these time series for counties/states [here](https://imgur.com/a/RpwPnmJ).




**EDIT 2020-04-10**: Google released updated pdfs with data up to 04-05. [Here](http://s000.tinyupload.com/index.php?file_id=50914681852714857468) is an updated json with that data.",50,19,typhoidisbad,2020-04-04 05:35:20,https://www.reddit.com/r/datasets/comments/fuo64p/google_covid19_mobility_reports_time_series_data/,0,datasets
fiks1s,COVID-19 CT Scans Dataset,"Hi, I'm trying to find a dataset for the CT scans of COVID-19 cases. So far, I've seen this [paper](https://www.medrxiv.org/content/10.1101/2020.02.14.20023028v3.article-metrics) use the dataset to train a classifier, but the link to their [data](https://ai.nscc-tj.cn/thai/deploy/public/pneumonia_ct) doesn't seem to work.

Does anyone have any links or ideas to access the data webpage? Thanks.",54,11,RareFuel,2020-03-14 16:14:13,https://www.reddit.com/r/datasets/comments/fiks1s/covid19_ct_scans_dataset/,0,datasets
csw7ih,5 Tips To Create A More Reliable Crawler,"Trying to not only improve your efficiency of your web crawler but also to create a web crawler faster?

Today I will share 5 tips from my experience to improve your efficiency when you are building a web crawler.

Hope you will like it and please leave a comment below on how you will increase efficiency of your web crawler.

[https://towardsdatascience.com/https-towardsdatascience-com-5-tips-to-create-a-more-reliable-web-crawler-3efb6878f8db](https://towardsdatascience.com/https-towardsdatascience-com-5-tips-to-create-a-more-reliable-web-crawler-3efb6878f8db)",52,6,weihong95,2019-08-20 10:17:23,https://www.reddit.com/r/datasets/comments/csw7ih/5_tips_to_create_a_more_reliable_crawler/,0,datasets
bj18lb,Bee hive data set. Can you help the bee keepers?,"Hello! I am working with a data set of metrics obtained by sensors on a bee hive. Maybe you heard that bees are essential for food production in the world. That is why there are specialized people or bee keepers to help boost the bee population. The idea is to understand the bee movement throughout a year to be able to predict future events. Can you help save the bees with just a data analysis?

[Go to the data set!!](https://www.kaggle.com/se18m502/bee-hive-metrics)",50,4,se18m502,2019-04-30 09:19:37,https://www.reddit.com/r/datasets/comments/bj18lb/bee_hive_data_set_can_you_help_the_bee_keepers/,0,datasets
aqjmca,"[OC] I'm compiling a dataset of how much companies and hackers make selling various types of personal data. They make over $200 billion dollars each year on aggregate, but the average person has no idea how this impacts them.","The end goal is to create a really good estimate for people to learn how much money their data is worth to the companies and hackers who just take it. This is my app for doing that so far (python + react + HIBP api). ***Feel free to skip the email entry*** but that's how it calculates the value of data sold by hackers in breaches that impacted those accounts: [https://app.fastgarden.io/assessment](https://app.fastgarden.io/assessment)

It is based on these sources which I've aggregated, then created averages for each data type and each company.

[https://medium.com/fast-garden/fast-garden-assessment-data-sources-399dad064723](https://medium.com/fast-garden/fast-garden-assessment-data-sources-399dad064723)

Looking for any feedback on this project or where to find more of this information (most companies keep it private for good reason).",50,1,ravvit22,2019-02-14 13:13:55,https://www.reddit.com/r/datasets/comments/aqjmca/oc_im_compiling_a_dataset_of_how_much_companies/,0,datasets
9hbm0f,Stanford University on police traffic stops nationwide,"Just want to make people aware of this dataset:

>On a typical day in the United States, police officers make more than 50,000 traffic stops. Our team is gathering, analyzing, and releasing records from millions of traffic stops by law enforcement agencies across the country. Our goal is to help researchers, journalists, and policymakers investigate and improve interactions between police and the public.

[https://openpolicing.stanford.edu/](https://openpolicing.stanford.edu/)",49,0,sutpen77,2018-09-20 02:47:53,https://www.reddit.com/r/datasets/comments/9hbm0f/stanford_university_on_police_traffic_stops/,0,datasets
7wuefa,"1,600 Occult Books",,49,1,cavedave,2018-02-11 18:12:55,http://www.openculture.com/2018/02/1600-occult-books-now-digitized-put-online.html,0,datasets
6bu1fb,"Top 1 million words on Reddit after analyzing 3,169,506,937 comments containing 97,255,752,256 words.","Link:  https://drive.google.com/open?id=0B-KZOnlSOAveZUxOVnlsc08xU3M

This is a first-pass attempt at constructing the top one million words from Reddit.  All comments were lower-cased before generating the list (I can do other lists in the future that are case-sensitive).  

The first column is the word, the second column is the number of times the word was found and the third column is the overall percentage that the word occurs.

This was done on a Xeon-1660v4 with 256Gb of ram.  Approximate run time was 27 hours.  

",49,18,Stuck_In_the_Matrix,2017-05-18 04:34:57,https://www.reddit.com/r/datasets/comments/6bu1fb/top_1_million_words_on_reddit_after_analyzing/,0,datasets
5p3g2p,"Whistleblower sues Duke, claims doctored data helped win $200 million in grants",,55,4,cavedave,2017-01-20 11:47:31,http://www.sciencemag.org/news/2016/09/whistleblower-sues-duke-claims-doctored-data-helped-win-200-million-grants,0,datasets
5d6udy,"This repo contains data on 6,546,824 election day tweets.",,50,1,jaypeedevlin,2016-11-16 02:45:09,https://github.com/chrisalbon/election_day_2016_twitter,0,datasets
ze9zn1,I've spent the last few months developing a website where you can test investment strategies based on alternative data,,52,12,inegyio,2022-12-06 15:46:13,https://app.inegy.io/,0,datasets
te3la1,[Self promo] The /r/Antiwork Subreddit Dataset - a NLP dataset of posts and comments of the Antiwork subreddit,"Hey all!

We are a group of data engineers working to make this site more accessible - here is a free Reddit dataset for you to peruse.

This one concerns Reddit's own [r/Antiwork](https://www.reddit.com/r/Antiwork/) community - and all the posts and comments of it. You can use it to track several interesting trends and how they affect a subreddit, from correlation to COVID to the impact of the infamous Fox News interview to maybe even something else.

[Here](https://socialgrep.com/datasets/the-antiwork-subreddit-dataset?utm_source=reddit&utm_medium=link&utm_campaign=theantiworksubredditdataset) is the download link - it's also available on [Kaggle](https://www.kaggle.com/pavellexyr/the-antiwork-subreddit-dataset) and [Huggingface](https://huggingface.co/datasets/SocialGrep/the-antiwork-subreddit-dataset), if you prefer that.",50,4,Lexyr-Mod,2022-03-14 18:00:51,https://www.reddit.com/r/datasets/comments/te3la1/self_promo_the_rantiwork_subreddit_dataset_a_nlp/,0,datasets
q8c6ft,What are some datasets you'd like to see that don't currently exist?,"Like the title says, I'm curious as to what collections of data you guys would be interested in having that don't already currently exist (at least to your knowledge). It can be as ridiculous as you want.",49,34,Ae-Rabelais,2021-10-15 00:09:33,https://www.reddit.com/r/datasets/comments/q8c6ft/what_are_some_datasets_youd_like_to_see_that_dont/,0,datasets
mgklga,Analyze OpenStreetMap Data with OSMnx and OmniSci Free,,51,1,_paige_joseph,2021-03-30 16:58:41,http://omnisci.link/u0pcz5,0,datasets
j08f9p,[Self-promotion] Wapo's Police Shooting Dataset as 3NF Database,"I've made a [github repo](https://github.com/SampleDataForChange/wapo-data-police-shootings) to ingest the Washington Post's [data-police-shootings](https://github.com/washingtonpost/data-police-shootings) csv data and publish it weekly as a documented and normalized (third normal form) SQL Server & SQLite database.

CSVs are great for basic use, but my hope is to make the data easier to use for demos, examples, and of course analysis as well! I think the world would benefit from data like this being more readily attainable, so that is part of the larger project [Sample Data For Change](https://sampledataforchange.github.io/) that I work on in my spare time.

Feedback is welcome and hope you find the data useful! I'm looking for more similar data sets to add to the project as well so any suggestions would be great.",50,4,LowlyDBA,2020-09-26 15:52:05,https://www.reddit.com/r/datasets/comments/j08f9p/selfpromotion_wapos_police_shooting_dataset_as/,0,datasets
hv6tis,"Data sets on electricity (found some on Worldbank, CIA), consumption, output, surpluses. Then data centres per country, and ideally a map of undersea cables. Capacity and amount if possible.",Doing some sustainability and forecasting research,48,8,That_bitch_Carol_,2020-07-21 12:53:13,https://www.reddit.com/r/datasets/comments/hv6tis/data_sets_on_electricity_found_some_on_worldbank/,0,datasets
e8k6e6,Nearly $1 billion typo may force Wasatch County taxpayers to pay more,"Epic data horror story. 

[https://www.deseret.com/utah/2019/12/5/20997681/horrific-wasatch-county-error-valued-home-for-nearly-1-billion-now-taxpayers-may-have-to-pay-more](https://www.deseret.com/utah/2019/12/5/20997681/horrific-wasatch-county-error-valued-home-for-nearly-1-billion-now-taxpayers-may-have-to-pay-more)

&#x200B;

[Always validate your data!](https://greatexpectations.io/github/?utm_source=reddit&utm_medium=datasets&utm_name=non-ge-article)",52,10,superconductiveKyle,2019-12-10 02:21:14,https://www.reddit.com/r/datasets/comments/e8k6e6/nearly_1_billion_typo_may_force_wasatch_county/,0,datasets
dce7hd,"Searchable, browsable datasets in S3",,52,4,brightpixels,2019-10-02 18:25:44,https://open.quiltdata.com/,0,datasets
cgiktx,The Largest Economic Indicator Database Yet - 196 Columns/Variables Since 1900,,47,5,OppositeMidnight,2019-07-22 20:32:24,https://www.kaggle.com/firmai/exhaustive-capital-markets-data,0,datasets
bomtx6,Up to date Football Dataset,"Hi this is my first dataset.
I scraped nearly 28000 football games from the top 5 european leagues from 2004/2005 to 2018/2019.

Take a look https://www.kaggle.com/waterchiller/european-football-games

Edit: The source code is public now. And I did a little bit of cleaning up. https://github.com/ChristianSchneeweiss/Football-Data-Scraping/tree/0.1",47,14,waterchiller,2019-05-14 18:45:58,https://www.reddit.com/r/datasets/comments/bomtx6/up_to_date_football_dataset/,0,datasets
b05hwo,5 TB of oilfield data from Statoil,[https://agilescientific.com/blog/2018/6/17/big-open-data-or-is-it](https://agilescientific.com/blog/2018/6/17/big-open-data-or-is-it),50,5,Terranigmus,2019-03-12 08:52:36,https://www.reddit.com/r/datasets/comments/b05hwo/5_tb_of_oilfield_data_from_statoil/,0,datasets
5z34a8,February 2017 Reddit comments are now available. Congratulations to Reddit for breaking the 3 billion mark in February!,"https://files.pushshift.io/reddit/comments/RC_2017-02.bz2

I will provide a torrent link in the next day or so.  

Feb 2017 (RC_2017-02.bz2):  70,609,487 comments (up from 59,189,875 comments in Feb 2016) -- The file is 7,032,957,578 bytes compressed.



",52,7,Stuck_In_the_Matrix,2017-03-13 04:27:52,https://www.reddit.com/r/datasets/comments/5z34a8/february_2017_reddit_comments_are_now_available/,0,datasets
57455w,"TrivialBuzz- API serving around 200,000 questions from the gameshow Jeopardy. You can explore categories, get a random question, filter questions by value or air date and search for specific topics. You can interact with it from the documentation page without any programming knowledge.",,50,0,1dollaMakeUholla,2016-10-12 13:49:34,http://www.trivialbuzz.com/,0,datasets
4h0rpj,The New York Time's tagged ingredient data set is now on GitHub,,49,6,rhiever,2016-04-29 18:24:05,http://open.blogs.nytimes.com/2016/04/27/structured-ingredients-data-tagging/,0,datasets
3ksfr8,Data on student completion and earnings for all undergraduate degree-granting U.S. colleges from 1996 to 2015,,47,0,danwin,2015-09-13 15:07:42,https://collegescorecard.ed.gov/data/,0,datasets
13xlpm5,Community-built hospital price database hits 400 hospitals,,49,5,alecs-dolt,2023-06-01 16:12:18,https://www.dolthub.com/blog/2023-05-24-400-hospitals/,0,datasets
sjfv9m,"[Datasets Release] Fun, Beginner-Friendly Collection of 33 Psychology Related Datasets!","Hi Guys,

I am thrilled to announce that I finished cleaning, organizing creating baselines, and uploading an updated version of the massive 33 datasets collection [Openpsychometrics](https://openpsychometrics.org/)

The whole project took me alot of time to clean and organize (way more than I planned) so please if you find this of value:  Your feedback & support is highly appreciated!

**Why would you want to work on them?**

* **Small** \- Size is similar to titanic.
* **Clean** \- Someone on this cleaned them a lot!
* **Intuitive** \- You see what was the question. You see what the individual answered. Nothing more. Nothing less.
* **Funny & Fun to work with** \- Questions like Would you rather own a gun or do the dishes?
* **Has the prettiest starter notebooks you have ever seen in your life**Seriously. I invested an embarrassingly amount of time on the baseline notebooks. check them out!
[Seriously, I invested an embarrassingly amount of time on the starter notebooks. check them out even if you don't care about the data, you won't regret it.]


## The Collection:

The collection includes 32 psychology & pseudo-psychology datasets, links are all here:

[https://www.kaggle.com/general/304994](https://www.kaggle.com/general/304994)",49,6,yamqwe,2022-02-03 10:26:32,https://www.reddit.com/r/datasets/comments/sjfv9m/datasets_release_fun_beginnerfriendly_collection/,0,datasets
sba2yv,"Death Row Inmates' Last Statements (Texas, 2020)",,49,18,None,2022-01-24 01:47:06,https://www.kaggle.com/samerhijjazi/death-row-inmates-last-statements-texas-2020,0,datasets
lko7x7,[self promotion] 2020 US Presidential Precinct Results,"Hi r/datasets,

Starting December, we ran a $25,000 data bounty to wrangle US Presidential Precinct-level results. The bounty ended yesterday. We think we have created the best open US Election precinct level database.

https://www.dolthub.com/repositories/dolthub/us-president-precinct-results

Here are some of the highlights:

* 15.5M cells edited. 1.7GB of data collected
* [All 51 ""states"" covered for 2016](https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+COUNT%28distinct%28state%29%29%0AFROM+%60vote_tallies%60+where+election_year%3D2016%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&active=Tables). [38 states covered for 2020](https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+COUNT%28distinct%28state%29%29%0AFROM+%60vote_tallies%60+where+election_year%3D2020%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&active=Tables). 
* [100% of the vote covered for 2016](https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+sum%28votes%29%2F136669276%0AFROM+%60vote_tallies%60+where+election_year%3D2016%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&active=Tables). [78% for 2020](https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+sum%28votes%29%2F159633396%0AFROM+%60vote_tallies%60+where+election_year%3D2020%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&active=Tables).
* 75 Pull Requests (PRs) accepted across 6 bounty participants. 
* Top bounty participant earned over $10,000.

Read more in our blog about it: 

https://www.dolthub.com/blog/2021-02-15-election-bounty-review/",50,4,timsehn,2021-02-15 21:55:16,https://www.reddit.com/r/datasets/comments/lko7x7/self_promotion_2020_us_presidential_precinct/,0,datasets
inlsmu,[Synthetic] We present CARLA Car Chasing dataset - an open sourced autonomous driving dataset in a CARLA simulator (Python),,45,1,Goron97,2020-09-06 13:14:05,https://github.com/JahodaPaul/autonomous-car-chase,0,datasets
huv0j9,What software do you use for Data Visualization?,"1. I am a first-year student of mathematical engineering and I want my models to look good, what software do you use?
Edit: Thank you! All your answers have been very helpful!",50,62,diegangas,2020-07-20 22:12:21,https://www.reddit.com/r/datasets/comments/huv0j9/what_software_do_you_use_for_data_visualization/,0,datasets
g64dhu,"COVID-19: Cumulative Timeseries dataset for India's state-wise confirmed, recovery and deceased cases.","Hello all,

We are maintaining a data repository for India's state-wise confirmed, recovery and deceased cases for nCoV-2019 as a cumulative timeseries dataset similar to how [Johns Hopkins CSSE](https://github.com/CSSEGISandData/COVID-19) is maintaining their dataset.

You can check it out here: [https://github.com/kalyaniuniversity/COVID-19-Datasets](https://github.com/kalyaniuniversity/COVID-19-Datasets)",48,2,post_depression,2020-04-22 16:34:16,https://www.reddit.com/r/datasets/comments/g64dhu/covid19_cumulative_timeseries_dataset_for_indias/,0,datasets
cm9sjh,"How to Easily Retrieve Companies, Investors and Funding Rounds Datasets from Crunchbase in a Spreadsheet",,52,0,HadyElHady,2019-08-05 11:29:55,https://www.youtube.com/watch?v=h2Gzbann_Bk,0,datasets
bplir5,Uber & Lyft prices with weather conditions,,50,7,stonedkrypto,2019-05-17 02:20:09,https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices,0,datasets
9ypec2,free datasets for beginners,,49,5,govardhandhari,2018-11-20 06:07:16,https://www.dataquest.io/blog/free-datasets-for-projects/,0,datasets
9rjnt6,Awesome CSV - A curated list of tools for dealing with CSV by Leon Bambrick,,50,9,cavedave,2018-10-26 10:48:37,https://github.com/secretGeek/awesomecsv,0,datasets
8ev47w,Astrometric data for 1.7 billion stars in the Milky Way,,48,7,Kopachris,2018-04-25 16:54:48,https://www.cosmos.esa.int/web/gaia/dr2,0,datasets
8d13ir,Reddit! What is your favorite ML dataset?,,51,15,thegoldcase,2018-04-17 23:31:19,https://goo.gl/forms/kHOzX35QRte3b7MS2,0,datasets
7agaca,Dataset of 69600 subreddits and 301565 relations between them,,48,2,uglyasablasphemy,2017-11-03 01:40:06,https://github.com/federicocalendino/reddit-database,0,datasets
581hqm,All of Donald Trump's Facebook Statuses + Reaction Counts (as of 10/17/16),,49,19,minimaxir,2016-10-18 02:35:21,https://docs.google.com/spreadsheets/d/1mLO7SFqHmUaZEpp87cwkM0luJutSwmwKMx7kaM9348U/edit?usp=sharing,0,datasets
41ahmm,Yahoo releases 13.5TB cache on user behavior for use by AI researchers (x-post from /r/business),,49,5,Habitual_Emigrant,2016-01-16 21:53:34,http://www.businessfinancenews.com/27319-yahoo-release-largest-cache-to-improve-machine-learning/,0,datasets
zbhzcz,2 Twitter Datasets for Finance-related Tweets Have Been Open-Sourced,"Hi 👋,

Want to share two datasets I built for multi-class text classification. One dataset classifies finance related tweets for sentiment (bullish, bearish, neutral) and the other dataset classifies finance topic (20 topics) for tweets. They each hold an MIT license. Feel free to explore!

topic: [https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic](https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic)

sentiment: [https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment)",47,0,Quantum_Stat,2022-12-03 14:50:05,https://www.reddit.com/r/datasets/comments/zbhzcz/2_twitter_datasets_for_financerelated_tweets_have/,0,datasets
wd7mca,I ran an experiment on Tinder to see how it prioritizes the accounts it will show. Heres the raw data. The video explaining how the experiment was run is in the comments,"First Unsuccessful Run: 

[https://drive.google.com/file/d/1oyKEOcQ1zg6jPz3yhQAvAE1IrLLWIswC/view?usp=sharing](https://drive.google.com/file/d/1oyKEOcQ1zg6jPz3yhQAvAE1IrLLWIswC/view?usp=sharing) 

Second Successful Run:

[https://drive.google.com/file/d/10Gug7J3Tq6t\_Pf6v-V-TTmLcZPo6X1Pi/view?usp=sharing](https://drive.google.com/file/d/10Gug7J3Tq6t_Pf6v-V-TTmLcZPo6X1Pi/view?usp=sharing)

The first run still added likes to the accounts, so I'm including the data set.",46,6,Ipushyourbuttons,2022-08-01 03:58:17,https://www.reddit.com/r/datasets/comments/wd7mca/i_ran_an_experiment_on_tinder_to_see_how_it/,0,datasets
rzufy9,"""LOCO: The 88-million-word language of conspiracy corpus"", Miani et al 2021",,44,5,gwern,2022-01-09 15:47:19,https://link.springer.com/article/10.3758/s13428-021-01698-z,0,datasets
r3huk8,A great open-source resource for data collected world-wide.,,47,1,i_am_extra_syrup,2021-11-27 17:07:53,https://ourworldindata.org,0,datasets
q36gp1,Is Ivermectin For Covid-19 Based On Fraudulent Research?,,47,25,cavedave,2021-10-07 10:51:32,https://gidmk.medium.com/is-ivermectin-for-covid-19-based-on-fraudulent-research-part-4-f30eeb30d2ff,0,datasets
pfmjb8,Dataset I created of ~700k Apple Store app names,"Link: [https://www.kaggle.com/sophiahill/all-apps-from-itunes-wesbite](https://www.kaggle.com/sophiahill/all-apps-from-itunes-wesbite)

Something I made in an internship this summer. After looking into it, I could not find a dataset that big just for app names from the Apple Store, so I used Selenium and an iTunes website I found to make my own dataset for it. Figure I'd put it on Kaggle for others who may find it useful.",48,7,sophia715,2021-09-01 04:29:58,https://www.reddit.com/r/datasets/comments/pfmjb8/dataset_i_created_of_700k_apple_store_app_names/,0,datasets
kiwe3a,"~2,700 Well-Formed Haikus (about 2020 news articles)","**Dataset:**   
[https://www.kaggle.com/newshaikus/dataset](https://www.kaggle.com/newshaikus/dataset)

**Search / Browse Haikus here:**   
[https://doomhaikus.3iap.co](https://doomhaikus.3iap.co)

**Context:**

Dataset from an attempt to teach computers to write silly poems, given a prompt / topic. 

I wrote a (surprisingly elaborate / painful) script to post each day's top news stories to Mechanical Turk, asking turkers to summarize each article as a haiku. I verified the syllable counts for each haiku against a syllable dictionary and/or manually (for unrecognized words).

It's been running since March. About \~2,000 people have responded and there are now \~2,700 haikus, forever memorializing the worst year of our lives, as punchy/gloomy sets of 5, 7, 5 syllables.

**Semi-plausible use cases:**

* Data art
* Language models
* Translation (with unusual constraints)
* Summarization",50,6,elibryan,2020-12-23 16:42:44,https://www.reddit.com/r/datasets/comments/kiwe3a/2700_wellformed_haikus_about_2020_news_articles/,0,datasets
i855ln,The Essential Guide to Training Data for ML,,46,0,Shirappu,2020-08-12 02:04:54,https://lionbridge.ai/training-data-guide/,0,datasets
i40z2o,Looking for beta-testers for historical weather data API service,"Every now and then there's a request looking for weather dataset - I've had this exact problem and have been working on making this less painful.

I've recently launched an API service for this and would greatly appreciate if you'd like to give it a try. If you find this useful, I'd be curious to learn about your use cases and your industry. You can try it out at:

https://oikolab.com

Some key features:
* Mainly uses [ERA5 reanalysis](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5) data from ECMWF, available hourly with global coverage (0.25 x 0.25 latitude/longitude gridded) from 1980 onward with parameters such as short-wave/long-wave radiations, 100m wind, and soil temperature that are less commonly available.
* Time-series data, with single API call for any location regardless of the duration.

Thanks!",50,21,a__square__peg,2020-08-05 07:35:17,https://www.reddit.com/r/datasets/comments/i40z2o/looking_for_betatesters_for_historical_weather/,0,datasets
ex4575,Hey /r/datasets! I made a classification dataset that consists of track features fetched using Spotify's Web API. Tracks are labeled as 'hit' or 'flop' depending on some criteria.,,47,2,paperpeople56,2020-02-01 10:14:51,https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset,0,datasets
ddx7od,CGI moon kit dataset released by nasa,,52,2,getkaizer,2019-10-06 02:15:52,https://svs.gsfc.nasa.gov/4720,0,datasets
b0uitc,"SEC financial statements and notes, as tab-delimited data; from 2009-forward, roughly 2GB per quarter",,50,11,danwin,2019-03-14 01:52:06,https://www.sec.gov/dera/data/financial-statement-and-notes-data-set.html,0,datasets
9q6k95,A database of paper airplanes with easy to follow folding instructions.,,48,9,cavedave,2018-10-21 19:53:24,https://www.foldnfly.com/#/1-1-1-1-1-1-1-1-2,0,datasets
8mgcnn,[Dataset] Copy of 10565 public domain but paywalled IEEE papers,,49,1,nemobis,2018-05-27 07:35:53,https://www.reddit.com/r/DataHoarder/comments/8mbj9h/copy_of_10565_public_domain_but_paywalled_ieee/,0,datasets
6zeb9t,Student Debt Growing Faster Than GDP,,49,4,rodionos,2017-09-11 09:26:18,https://github.com/axibase/atsd-use-cases/blob/master/Chart_of_the_Day/Student_Loan/README.md#outstanding-students-loan-debt-continues-to-exceed-one-trillion-dollars,0,datasets
5ovoyz,CIA releases 13m pages of declassified documents online,,48,5,cavedave,2017-01-19 09:35:07,https://www.cia.gov/library/readingroom/collection/crest-25-year-program-archive,0,datasets
59039y,Updated: Reddit comment dataset up to 2016-08 including sentiment data,"Hi,
I have updated the torrent for the Reddit comment data set.

There are two torrents:
[The first torrent](http://code.dewarim.com/reddit-2016-08.torrent) is simply a collection of all comments as collected by pushshift.io (files available individually). Format is JSON, compressed by bz2 per month. Download size: 236 GByte

[The second torrent](http://code.dewarim.com/reddit-parquet-sentiment-2016-08.torrent) contains the same data plus the sentiment score for each comment (score, max_positive, max_negative). The format is Apache Parquet, which makes this better suited for Spark/Hadoop queries. Download size: 419 GByte

With sentiment data, you can compute who is the most positive commentor of reddit (Spoiler: bots and people who compliment the ladies on /gonewild) or which [car brands](http://code.dewarim.com/index.html) are popular on reddit).  

I have written some simple code for working with both formats which can be found on [Github:reddit-data-tools](https://github.com/dewarim/reddit-data-tools)",48,15,Dewarim,2016-10-23 18:58:37,https://www.reddit.com/r/datasets/comments/59039y/updated_reddit_comment_dataset_up_to_201608/,0,datasets
54xfkt,Google releases 8 million YouTube video URLs with 4800 classes,,49,8,SteelPangolin,2016-09-28 17:15:02,https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html,0,datasets
4f38m3,"Is there a ""github for datasets""?",I've seen people talk about datasets.co but that seems to be a side project. ,45,14,foxh8er,2016-04-16 19:08:16,https://www.reddit.com/r/datasets/comments/4f38m3/is_there_a_github_for_datasets/,0,datasets
10lkeiv,National Database of Childcare Prices,,47,3,brianckeegan,2023-01-26 05:23:23,https://www.dol.gov/agencies/wb/topics/childcare,0,datasets
zdn0m4,A Comprehensive FIFA World Cup 2022 dataset with detailed player and team statistics.,,47,0,swaptr,2022-12-05 22:56:15,https://www.kaggle.com/datasets/swaptr/fifa-world-cup-2022-statistics,0,datasets
ye4rhc,What's that humming sound? The World Hum Database,"The World Hum Database (aka ""The Hum"") has thousands of user-submitted reports about an ""unusual unidentified low-frequency sound that scientists now call the Worldwide Hum.""

[https://thehum.info/](https://thehum.info/)

[https://thehum.info/ewExternalFiles/march22dbase.csv](https://thehum.info/ewExternalFiles/march22dbase.csv)",47,1,n1nja5h03s,2022-10-26 18:09:56,https://www.reddit.com/r/datasets/comments/ye4rhc/whats_that_humming_sound_the_world_hum_database/,0,datasets
vrmbt3,Gun Violence Data for January 2014 through 2022,"[Kaggle Link](https://www.kaggle.com/datasets/jasonmobley/united-states-gun-violence-data-20142022)

All pulled from [GunViolenceArchive.org](https://GunViolenceArchive.org) in 2000 row sets (as their database csv export won't do more than that at a time) and consolidated by hand. Cleaned and organized by date using R.

I hope this provides better availability of information on this topic for any analysts out there exploring Gun Violence in the United States.",46,3,Able_Signature_85,2022-07-05 01:23:11,https://www.reddit.com/r/datasets/comments/vrmbt3/gun_violence_data_for_january_2014_through_2022/,1,datasets
v812n2,The Melvin Dataset: Sentiment Analysis of Social Media Stock Conversations,,50,0,BB4evaTB12,2022-06-08 21:56:53,https://www.surgehq.ai/blog/sentiment-analysis-dataset-of-social-media-conversations,0,datasets
r4u4yo,[self-promotion] 6.4m Steam Reviews Dataset,"Hello mates,  


Here's a steam reviews dataset, containing a little more than 6.4 million observations.

You can find it [here](https://www.kaggle.com/andrewmvd/steam-reviews).

  
It was taken on 2017 and all credits are due to the following authors:

> Antoni Sobkowicz. (2017). Steam Review Dataset (2017) \[Data set\]. Zenodo. [https://doi.org/10.5281/zenodo.1000885](https://doi.org/10.5281/zenodo.1000885) 

&#x200B;

Cheers",46,1,larxel,2021-11-29 11:48:04,https://www.reddit.com/r/datasets/comments/r4u4yo/selfpromotion_64m_steam_reviews_dataset/,0,datasets
ppqmmx,RIP World Bank Doing Business Dataset,"Following an external report citing manipulation and unethical behavior at the highest levels of the organization, the World Bank just announced that they will be canceling the ""Doing Business"" dataset which informed almost all country-level global ""business climate"" indicators.  I would be curious if anyone knows a good replacement?    


P.S. Read the report, it is SCATHING: ([https://thedocs.worldbank.org/en/doc/84a922cc9273b7b120d49ad3b9e9d3f9-0090012021/original/DB-Investigation-Findings-and-Report-to-the-Board-of-Executive-Directors-September-15-2021.pdf](https://thedocs.worldbank.org/en/doc/84a922cc9273b7b120d49ad3b9e9d3f9-0090012021/original/DB-Investigation-Findings-and-Report-to-the-Board-of-Executive-Directors-September-15-2021.pdf))",48,2,Philooflarissa,2021-09-17 01:34:08,https://www.reddit.com/r/datasets/comments/ppqmmx/rip_world_bank_doing_business_dataset/,0,datasets
o7x3rr,US County & Zipcode Historical Demographics,"In developing a data cleaning app I have found that fundamental demographics data on the US is hard to get to or fairly costly. I was tired of it and think basic data like this should be free and easy.

&#x200B;

Easily lookup US historical demographics by county FIPS or zipcode in seconds with this file containing over 5,901 different columns including:

\*Lat/Long  
\*Boundaries  
\*State FIPS  
\*Population from 2010-2019  
\*Death Rate from 2010-2019  
\*Unemployment from 2001-2020  
\*Education from 1970-2019  
\*Gender and Age Population

[https://www.kaggle.com/bitrook/us-county-historical-demographics](https://www.kaggle.com/bitrook/us-county-historical-demographics)

&#x200B;

Open for feedback :-)",44,3,weber_stephen,2021-06-25 21:33:38,https://www.reddit.com/r/datasets/comments/o7x3rr/us_county_zipcode_historical_demographics/,0,datasets
m75sm6,Everything collects data about you. Where are some easily accessible datasets about yourself?,Are there any apps for example that collect data and let you download it about yourself?,48,38,Quentin_the_Quaint,2021-03-17 17:33:41,https://www.reddit.com/r/datasets/comments/m75sm6/everything_collects_data_about_you_where_are_some/,0,datasets
lqi0d8,"Rows launches a spreadsheet with data, integrations, and sharing",,50,0,HadyElHady,2021-02-23 13:07:21,https://blog.rows.com/p/rows-beta,0,datasets
ipf89w,"Does anyone have any good insights into massive sources of ""dirty"" data?","As the title says, I am looking for very large datasets (csv or databases) that contain uncleaned data. I am aiming to improve my data cleaning and pre-processing skills. I have done quite a bit of web scraping and cleaning based on that, just looking for different things to get exposure to.

Edit: Thanks to everyone who responded with ideas, these are all excellent. u/harry_comp_16 had a great idea for creating a GitHub repo with all of the datasets in them (and future ones as well). I have created a repo found [here](https://github.com/mattsunner/datasets). I have not had an opportunity to add the suggestions in yet, but will do so this week. If you have any datasets you want to include, PM me, email me, or submit a PR. Thanks!",44,21,thebadassets,2020-09-09 12:47:34,https://www.reddit.com/r/datasets/comments/ipf89w/does_anyone_have_any_good_insights_into_massive/,0,datasets
h8t47a,Economic history datasets,,48,0,cavedave,2020-06-14 12:40:10,https://gpih.ucdavis.edu/,0,datasets
gmr4x2,How to Quickly Classify Chatbot Datasets,,46,5,cavedave,2020-05-19 15:50:21,https://www.youtube.com/watch?v=HLg0x7QZgxc,0,datasets
f0fzkx,Is there a data set of fake news tweets?,,44,11,TheMordant,2020-02-07 20:04:27,https://www.reddit.com/r/datasets/comments/f0fzkx/is_there_a_data_set_of_fake_news_tweets/,0,datasets
dot0xh,A free way to find and clean up personal data online,"I'm just kicking off this project with a friend. I've spent 4 years in the personal data space and he's spent 5 years on security teams. 

Thoughts from supporters, users, critics would be great. 

[https://www.thekanary.com/](https://www.thekanary.com/)

1. Verifiable by sharing sites scanned, info found, and aggregate progress / improvement
2. Doesn’t claim to secure accounts that already have large security teams and privacy settings settings
3. Free
4. Actionable so you can request information be taken down, report incidences to the government, participate in class action claims, know if a site re-posts information it shouldn’t
5. Works with minimal information like email",50,18,ravvit22,2019-10-29 17:23:25,https://www.reddit.com/r/datasets/comments/dot0xh/a_free_way_to_find_and_clean_up_personal_data/,0,datasets
dazny1,Atlas of Urban Expansion,,49,2,cavedave,2019-09-29 19:12:18,http://atlasofurbanexpansion.org/cities,0,datasets
cevja8,How to download and use the DEA pain pills database (100GB uncompressed),,48,3,danwin,2019-07-18 17:31:38,https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/,0,datasets
bwrdna,Can r/datasets Sticky the Most Helpful Resources?,"Can the mods at r/datasets create a useful stickied post for people who come here to first search in order to potentially answer their questions before leading to a subsequent post? I think there is an opportunity to not have the last two monthly discussion threads pinned, rather only keeping one and pinning a common resource thread.

It's not that this subreddit is overcrowded, but it's that I think a lot of questions could be easily answered if we provided a list of websites that have freely available, clean datasets. Many requests are simply to complete course exercises, and the requestor is simply looking for ***any*** dataset.

I checked the wiki here and there isn't really much in there besides a couple of political resources and reddit comment data.

The most common resource that I find myself suggesting is [Kaggle](https://www.kaggle.com/datasets). They have 17,000+ datasets of various topics, variables, and structures. Are there others that should be included? What do you think?",48,4,medavis6,2019-06-04 17:24:20,https://www.reddit.com/r/datasets/comments/bwrdna/can_rdatasets_sticky_the_most_helpful_resources/,0,datasets
aviogq,A Guide to Analyzing (American) Political Data in R,,47,2,cavedave,2019-02-27 21:28:10,https://www.thecrosstab.com/project/r-politics-guide/guide.html,0,datasets
anqu6v,A global dataset of CO2 emissions and ancillary data related to emissions for 343 cities,,49,2,yaph,2019-02-06 13:45:52,https://doi.pangaea.de/10.1594/PANGAEA.884141,0,datasets
8o0alx,YCombinator / Hackernews dataset will be published in approximately one week!,"I will be publishing the entire YCombinator / Hackernews dataset in approximately one week.  Quick question -- before publishing the dataset, should I HTML decode any encoded entities in the data?

(Example:  ""Glad you\&#x27;ve had a great experience)

There are a total of ~ 18 million objects in this dataset.",49,22,Stuck_In_the_Matrix,2018-06-02 12:42:58,https://www.reddit.com/r/datasets/comments/8o0alx/ycombinator_hackernews_dataset_will_be_published/,0,datasets
8cnj1t,2.8 Million tweets related to the Syrian Civil War,,47,0,iSmokeGauloises,2018-04-16 13:31:44,https://archive.org/details/tweets_201804,0,datasets
4p36ie,1mb archive of Donald Trump speeches,,47,8,ryansworks,2016-06-21 06:05:25,https://github.com/ryanmcdermott/trump-speeches,0,datasets
12215by,Scrape Thousands of Records of Housing Data Using Python [Self-Promotion],"Hey r/datasets,

I originally posted this library earlier this week, but it got downvoted once within 10 minutes and was never heard from again. And I get it, this is a place for posting/requesting datasets.

So, here's an actual [dataset](https://www.kaggle.com/datasets/ryansherby/ca-housing-data) of CA housing data I generated using the [RedfinScraper](https://github.com/ryansherby/RedfinScraper) library. Scraping these 47,000 records took just over 3 minutes.

While this data may be useful today, the fact is, it will only be useful for about a week longer. The high-velocity nature of housing data means that datasets need to be updated frequently.

This issue was the driving force for sharing this library publically: to allow users to quickly scrape the latest housing data at their leisure.

I hope you find this library useful, and I am excited to see what you create with it.",44,8,ryan_s007,2023-03-25 22:15:08,https://www.reddit.com/r/datasets/comments/12215by/scrape_thousands_of_records_of_housing_data_using/,0,datasets
z3mds7,"For data explorers, here's a cool dataset!","Exercise your data munging skills by playing with this cool dataset I discovered.

[https://www.kaggle.com/datasets/amorybristol/playboy-playmate-features](https://www.kaggle.com/datasets/amorybristol/playboy-playmate-features)",46,2,jollygoodlad,2022-11-24 15:11:31,https://www.reddit.com/r/datasets/comments/z3mds7/for_data_explorers_heres_a_cool_dataset/,0,datasets
yb49tk,"So, I catalogued the bestiaries of 100 popular fantasy tabletop RPGs. I'm good at research but kinda bad at math What types of cool stuff could I do with this?",,48,10,None,2022-10-23 01:20:05,https://www.reddit.com/r/rpg/comments/yaw0qo/last_year_i_created_the_average_fantasy_bestiary/,0,datasets
vafirb,The Amazon most popular books dataset: top reviewed books with 38 data points on each listing,,49,3,Gidoneli,2022-06-12 05:57:21,https://github.com/luminati-io/Amazon-popular-books-dataset,0,datasets
ui2g3x,Flight Datasets: Ticket Price and AirCraft Carbon Emissions,"Hello, Is any one interested in airline flight datasets?

Here is a one which might be usful  (it covers some top airports from europe/asia/america/africa), with airline info , air ticket price, Co2 (carbon) emission etc. Good for those who care about the airline sector or those who want to study then environmentals of the aircraft . Feel free to leave any thoughts or ideas. 

Dataset Link : [Link to Dataset](https://barkingdata.com/?flight-dataset/)",48,2,FaultyHoarding,2022-05-04 09:11:52,https://www.reddit.com/r/datasets/comments/ui2g3x/flight_datasets_ticket_price_and_aircraft_carbon/,0,datasets
qvh0qi,World News Headline Dataset. With Sentiment Scores. Free download in JSON format. Updated often.,,44,7,ACertainKindOfStupid,2021-11-16 20:17:27,https://github.com/fwd/news,0,datasets
nq41js,How ING sets up its Data Quality Framework of its Data Analytics Platform,"Here’s a great article from the Data Engineering team of ING describing the need for and components of their Data Analytics Platform’s Data Quality Framework.

Tools:

* **Data Quality** (using the  [Great Expectations library](https://greatexpectations.io/) )
* **Data Profiling** (using the  [pandas-profiling library](https://github.com/pandas-profiling/pandas-profiling) )
* **Data Stability** (using  [popmon — Population Shift Monitoring](https://medium.com/wbaa/population-monitoring-open-source-1ce3139d8c3a) )

Read the whole thing here:  [https://medium.com/wbaa/the-data-analytics-platforms-data-quality-framework-6a3f7cda8c36](https://medium.com/wbaa/the-data-analytics-platforms-data-quality-framework-6a3f7cda8c36)",46,1,superconductiveKyle,2021-06-01 20:41:31,https://www.reddit.com/r/datasets/comments/nq41js/how_ing_sets_up_its_data_quality_framework_of_its/,0,datasets
ndae3u,The number of police officers with complaints filed against them?,"There are roughly 700k cops in America and I'm trying to see how many of those cops have complaints filed against them. Is there a dataset that has this information, or something like number of police with complaints filed against them by state? 

I found something in [bjs](https://www.bjs.gov/content/pub/pdf/ccpuf.pdf) but it's from 2006 and it's looking at the total number of complaints, not the number of officers with complaints.",46,29,None,2021-05-15 22:40:54,https://www.reddit.com/r/datasets/comments/ndae3u/the_number_of_police_officers_with_complaints/,0,datasets
lc0uft,Datasets for social good,,45,2,cavedave,2021-02-03 22:57:54,https://github.com/shreyashankar/datasets-for-good,0,datasets
kz6vrg,Covid chest imaging dataset for AI training. Need to request access.,,45,1,TrinityRevelations,2021-01-17 13:58:48,https://www.gov.uk/government/news/ai-at-the-forefront-of-efforts-to-treat-coronavirus-patients,0,datasets
jy35w5,Is NOAA the best weather API for historical weather data at this point?,"Needing historical weather data for a machine learning project. Initially, I was intending to use Weather Underground or Dark Sky before learning their APIs got shutdown. I was hoping to find another API to get the following data:

* mean temperature
* mean dewpoint
* mean pressure
* max humidity
* min humidity
* max dewpoint
* min dewpoint
* max pressure
* min pressure
* precipitation

I need a few years worth of this data for a handful of different locations(to be clear only these daily summaries though, not the hourly info). So back to the title of my post, is NOAA the best weather API for this kind of historical weather data at this point? From what I can tell it's one of the only ones left.",45,5,AVendettaForV,2020-11-21 02:48:05,https://www.reddit.com/r/datasets/comments/jy35w5/is_noaa_the_best_weather_api_for_historical/,0,datasets
j40ljs,Good subreddits to scrape NSFW datasets,"Can anyone suggest some good subreddits from where I can cover all cases of NSFW images. I define a good subreddit as:

1. Most posts are images
2. Has diverse variety of nsfw/nsfl posts (gore nudity etc.)
3. NSFW tags are actually relevant (no false positives)
4. There won't be copyright issues or anything (not sure what I mean by this either, just don't want to get in any trouble)My  intention is to annotate these images and build a classifier that can recognise NSFW post

Edit: I know of r/MedicalGore , r/FiftyFifty etc. Please don't just suggest porn, or a specific type/subgenre of porn, as it would lead to bias. Also if possible please suggest something where we won't have to delete too many images because of privacy concerns (like images that show faces etc.)Edit 2: I found this dataset: [https://www.reddit.com/r/MachineLearning/comments/b78j1q/p\_nudity\_detection\_and\_censoring\_in\_images\_with/](https://www.reddit.com/r/MachineLearning/comments/b78j1q/p_nudity_detection_and_censoring_in_images_with/) but it solely focuses on nudes

&#x200B;",43,15,bipolarbear1797,2020-10-02 18:56:21,https://www.reddit.com/r/datasets/comments/j40ljs/good_subreddits_to_scrape_nsfw_datasets/,0,datasets
hzdkbp,A Huge Collection of Reddit Voting Data,,50,16,LocalOptimum,2020-07-28 12:12:37,https://www.kaggle.com/josephleake/huge-collection-of-reddit-votes/,0,datasets
gtabcy,DBpedia: latest releases of core data from en.wikipedia.org,,48,2,Lewoniewski,2020-05-30 07:37:18,https://databus.dbpedia.org/dbpedia/collections/latest-core,0,datasets
gopkcn,"Help create Common Voice's first target segment - Mozillas voice public domain dataset Common Voice now collects records of the number 0-9 and the words yes, no, hey and Firefox from every donor in 13 languages",,47,1,stergro,2020-05-22 19:03:35,https://discourse.mozilla.org/t/help-create-common-voices-first-target-segment/59587,0,datasets
gkgw7n,Searching for Datasets for R Beginner,"Hello, 

I am searching for sources where I can practice R using tidyverse. I just want to practice importing, cleaning, manipulating, and creating visualizations. If you could point me in the right direction, I really appreciate it. Thank you.",44,12,slopers_pinches,2020-05-15 20:30:47,https://www.reddit.com/r/datasets/comments/gkgw7n/searching_for_datasets_for_r_beginner/,0,datasets
dqme2q,"32,487 A/B tests conducted by Upworthy from January 2013 to April 2015",,46,2,jofish22,2019-11-02 16:20:11,https://medium.com/@natematias/announcing-the-upworthy-research-archive-c9b11087ddeb,0,datasets
b2c9w7,Visualizing One Million NCAA Basketball Shots [data posted on BigQuery],,45,4,danwin,2019-03-18 00:43:47,https://minimaxir.com/2018/03/basketball-shots/,0,datasets
b11shk,Facial recognition's dirty little secret: Millions of online photos scraped without consent,,45,13,jonfla,2019-03-14 15:39:07,https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921,0,datasets
90w91m,This is just about everything...,"Just came across this huge list which has everything I have ever seen and more!

[https://dreamtolearn.com/ryan/1001\_datasets](https://dreamtolearn.com/ryan/1001_datasets)",46,2,None,2018-07-22 07:48:42,https://www.reddit.com/r/datasets/comments/90w91m/this_is_just_about_everything/,0,datasets
8c9f4j,I have implemented a crawler for reddit data.,"https://github.com/YaboLee/reddit_crawler

Solution One: Acquire data from public data.

Solution Two: Acquire data according to subreddit.

More detail is included in the Readme.md. Why not leave me with your star and comments and critiques?

Note: Solution two needs your own reddit developed APP id & secret.

**UPDATE:** I am sorry that this is really an immature experimental tool. There are many things I didn't consider, like the accurate API rules, JSON url, storage, continent...Thanks for your enjoyment, comments and critiques. I will try to revise it in the future! ",48,30,SegmFault,2018-04-14 18:27:16,https://www.reddit.com/r/datasets/comments/8c9f4j/i_have_implemented_a_crawler_for_reddit_data/,0,datasets
7zn7fd,Conversational datasets to train a chatbot,,47,1,cavedave,2018-02-23 10:46:29,http://freeconnection.blogspot.ie/2016/04/conversational-datasets-for-train.html,0,datasets
6h8whd,Supreme Court audio recordings since 1955- really interesting for NLP,,46,0,DataScienceInc,2017-06-14 17:12:01,https://github.com/free-law-coalition/oyez-scotus,0,datasets
3es1s4,"33k NYTimes and 18k BuzzFeed Facebook Posts, and a Python scraper for any other Facebook Page","These are the datasets generated in the process of writing my blog post [How to Scrape Data From Facebook Page Posts for Statistical Analysis](http://minimaxir.com/2015/07/facebook-scraper/), which scrapes a Facebook page and puts all the posts into a CSV for import into any stats program.

You can download the [NYTimes data here](https://dl.dropboxusercontent.com/u/2017402/nytimes_facebook_statuses.zip) [4.6MB] and the [BuzzFeed data here](https://dl.dropboxusercontent.com/u/2017402/buzzfeed_facebook_statuses.zip) [1.5MB]

The [GitHub repository](https://github.com/minimaxir/facebook-page-post-scraper) contains the raw scraper itself.",49,12,minimaxir,2015-07-27 15:11:48,https://www.reddit.com/r/datasets/comments/3es1s4/33k_nytimes_and_18k_buzzfeed_facebook_posts_and_a/,0,datasets
1l8f8t,"I scraped the top 1,000 all-time posts from the top 2,500 subreddits over the past few days. Here they are in CSV format - maybe you folks are interested?",,47,26,umbrae,2013-08-28 02:35:45,https://github.com/umbrae/reddit-top-2.5-million,0,datasets
1bdn4om,Dateno - a new dataset search engine,"Hi! Just recently we launched [Dateno](https://dateno.io), a dataset search engine with 10M dataset search index from 4.9k data catalogs, near real-time search, 13 facets and filters and data quality in mind and priority. It's still very beta, lots of duplicates, errors, broken links and so on, but it works and you could try it. 

Inside the search engine is a Common Data Index, a registry of all available data catalogs that I worked on last year. 

Nearly 10k data catalogs were collected, documented, analyzed, API discovered and so on. Actually quite boring but necessary work to see the data catalog landscape around the world. 

  
Dateno is the next step after these catalogs. We analyzed existing API, tested several crawling techniques outside OAI-PMH indexing or indexing schema.org dataset objects. Finally now search index complete and open API will come soon. 

  
The final goal is very ambitious, we would like to create open search index and dataset search engine that will be bigger, wider, deeper and better data quality than Google Dataset Search (50M datasets in early 2023). We plan to add more than 20M datasets during 2024, more features, more filters and better understanding and representation of dataset metadata. 

  
Really want to see your thoughts on this. 

  
**Disclaimer**: I am the creator and founder of Dateno, feel free to ask me anything about it and datasets discovery topics.",46,12,ivan-begtin,2024-03-13 09:22:46,https://www.reddit.com/r/datasets/comments/1bdn4om/dateno_a_new_dataset_search_engine/,0,datasets
12jsx6k,We made a newsfeed for tracking new and deleted datasets across 200+ open data portals (and they're all queryable with SQL),,43,2,chatmasta,2023-04-12 17:18:07,https://open-data-monitor.splitgraph.io/,0,datasets
yfhxnb,The Stack - A 3TB Dataset of permissively-licensed code in 30 languages,,47,4,dwrodri,2022-10-28 07:56:48,https://twitter.com/bigcodeproject/status/1585631176353796097?s=46&t=mLrACB0pej1c7ge2uX2vKg,0,datasets
uspq4m,Datasets for Current Rent Prices In The United States,"Hey everyone, does anyone know where I can get datasets for residential housing in the United States? I’m mainly looking for large datasets displaying rent prices, mainly 1bdr and 2bdr condos. I plan on building a model to determine accurate average rent per state as well as various other trends, and feel like looking at this type of data would help significantly. Initially, I tried to scrape Zillow with Selenium but I’m not too proficient in web scraping/coding so I was looking for alternatives. Anything helps!",45,11,psaiful28,2022-05-18 23:26:01,https://www.reddit.com/r/datasets/comments/uspq4m/datasets_for_current_rent_prices_in_the_united/,0,datasets
ud0dnz,Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items,,47,2,andrewlapp,2022-04-27 10:27:46,https://arxiv.org/abs/2204.11918,0,datasets
soa7hr,High-frequency Cryptocurrencies market data | +20 Related Trading Notebooks,"

>**TL;DR:** See datasets / example notebooks below 👇


Hi Guys,

I collected high frequency cryptocurrencies market data and uploaded them as.csv's to Kaggle. 
They include over 4 years of data for some of the most traded coins.

The datasets were collected using an automated collection pipeline that collected minute-by-minute market data for Cryptocurrencies and updated it every day to Kaggle!
The whole project took me a lot of time to develop and is not easy to maintain, so please if you find this of value: Your feedback & support is highly appreciated!

This was mainly done for the cryptocurrencies forecasting competition that is currently in it's ""frozen stage"" where participants now watch their algorithms forecast the future and had not happen yet. (And hope for the best ;))


## Trading Related Kaggle Notebooks

Also, I also released **+20 example notebooks** that use the datasets for trading, each demonstrates a different approach for forecasting future returns.
This project is also beginner-friendly since it is highly documented, this can serve as a ""first stop"" when studying Time Series analysis.


## The Datasets:

* [Bitcoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin)
* [Ethereum](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum)
* [Binance Coin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-binance-coin)
* [Bitcoin Cash](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin-cash)
* [Cardano](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-cardano)
* [Dogecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-dogecoin)
* [Eos.io](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-eos-io)
* [Ethereum Classic](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum-classic)
* [Iota](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-iota)
* [Litecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-litecoin)
* [Monero](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-monero)
* [Maker](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-maker)
* [Stellar](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-stellar)
* [TRON](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-tron)


## Baselines & Starter Notebooks

|CV + Model|Hyperparam Optimization|Time Series Models|Feature Engineering|
|:-|:-|:-|:-|
|[Neural Network Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-nn)|[MLP + AE](https://www.kaggle.com/yamqwe/bottleneck-encoder-mlp-keras-tuner)|[LSTM](https://www.kaggle.com/yamqwe/time-series-modeling-lstm)|[Technical Analysis #1](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features)|
|[LightGBM Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-lgbm)|[LightGBM](https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna)|[Wavenet](https://www.kaggle.com/yamqwe/time-series-modeling-wavenet)|[Technical Analysis #2](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-feats-2)|
|[Catboost Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-extra-data-catboost)|[Catboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-catboost-gpu-optuna)|[Multivariate-Transformer \[written from scratch\]](https://www.kaggle.com/yamqwe/time-series-modeling-multivariate-transformer)|[Time Series Agg](https://www.kaggle.com/yamqwe/features-all-time-series-aggregations-ever)|
|[XGBoost Starter](https://www.kaggle.com/yamqwe/xgb-extra-data)|[XGboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-xgboost-gpu-optuna)|[N-BEATS](https://www.kaggle.com/yamqwe/crypto-forecasting-n-beats)|[Neutralization](https://www.kaggle.com/yamqwe/g-research-avoid-overfit-feature-neutralization/)|
|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-adapted-to-crypto)|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-keras-tuner)|[DeepAR](https://www.kaggle.com/yamqwe/probabilistic-forecasting-deepar/)|⏳Target Engineering|
|[Transformer)](https://www.kaggle.com/yamqwe/let-s-test-a-transformer)|[Transformer](https://www.kaggle.com/yamqwe/sh-tcoins-transformer-baseline)||⏳Quant's Volatility Features|
|||||
|[Reinforcement Learning (PPO) Starter](https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter)|||⏳Wavelets|

[About the validation: GroupTimeSeriesSplit](https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit)

(⏳ - in the making..)

Fork them as you please! Enjoy Yourself!

## Technical details about the Data

For every asset, the following fields from [Binance's official API endpoint for historical candlestick data](https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#klinecandlestick-data) are collected, saved, and processed.

1. timestamp - A timestamp for the minute covered by the row.
2. Asset\_ID - An ID code for the cryptoasset.
3. Count - The number of trades that took place this minute.
4. Open - The USD price at the beginning of the minute.
5. High - The highest USD price during the minute.
6. Low - The lowest USD price during the minute.
7. Close - The USD price at the end of the minute.
8. Volume - The number of cryptoasset u units traded during the minute.
9. VWAP - The volume-weighted average price for the minute. 10.Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
10. Weight - Weight, defined by the competition hosts [here](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)
11. Asset\_Name - Human readable Asset name.

**Indexing** The dataframe is indexed by `timestamp` and sorted from oldest to newest. The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.


>**Bonus dataset:** I've also uploaded a dataset containing the most powerful source for predicting cryptocurrencies movement: Elon Musk's Twitter 😂! It is simply an updated dataset of all Elon Musk's tweets 😂. I must check if Elon Musk can help us win! 👌 You can play with it yourself [here](https://www.kaggle.com/yamqwe/elon-musks-twitter-updated-031121).


Enjoy Yourself!
And thank you in advance for your support! This was not an easy system to maintain!",43,1,yamqwe,2022-02-09 10:14:39,https://www.reddit.com/r/datasets/comments/soa7hr/highfrequency_cryptocurrencies_market_data_20/,0,datasets
qwkpkx,I created an Auto-Updating Kaggle dataset that collects high-frequency market data - Updates daily! | +20 Related Trading Notebooks,">**TL;DR:** See example notebooks below 👇

I am happy to announce that I finally finished cleaning, organizing, creating baselines, and developing an automated collection pipeline that collects minute-by-minute market data for Cryptocurrencies. It updates on Kaggle every day! And will keep doing so until the competition is over! \[Maybe even more\]

The whole project took me a lot of time to develop and is not easy to maintain, so please if you find this of value: Your feedback & support is highly appreciated!

## The Competition

As some of you know, there is Crypto forecasting competition is running on Kaggle: ""G-Research Crypto Forecasting"". In this competition, we need to use machine learning for forecasting short-term returns of popular cryptocurrencies \[such as bitcoin, ether, dogecoin..\] We are provided a dataset of millions of rows of high-frequency market data dating back to 2018 which we should use to build our models on. Once the submission deadline has passed, the final score will be calculated over the following 3 months using live crypto data as it is collected.

## Auto-updating Kaggle dataset

To make things more interesting: I created an Auto-Updating Kaggle dataset that collects high-frequency market data for multiple cryptocurrencies.

* Updates daily on Kaggle!
* Available for anyone to play with!

Also, I also released **20+ starter notebooks** each demonstrating a different model or method for forecasting future returns.

This project was meant to be for the currently running Crypto Forecasting Competition by G-Research. However, since it is publicly available I assumed many others would like to also have a look :)

**Mimics ""Real-Life"" better than typical datasets**

This is a unique opportunity to work in a much more ""real-life"" setup than usual Kaggle. Because the datasets update daily.

* so.. If you mess up and overfit..
* You see it tomorrow! 😂

Anyway, this is an ongoing project that is also beginner-friendly since it is highly documented. Many more Time Series / Finance-related notebooks will be released in the future so this can also serve as a ""first stop"" when studying Time Series analysis.

## Baselines & Starter Notebooks

|CV + Model|Hyperparam Optimization|Time Series Models|Feature Engineering|
|:-|:-|:-|:-|
|[Neural Network Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-nn)|[MLP + AE](https://www.kaggle.com/yamqwe/bottleneck-encoder-mlp-keras-tuner)|[LSTM](https://www.kaggle.com/yamqwe/time-series-modeling-lstm)|[Technical Analysis #1](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-features)|
|[LightGBM Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-with-extra-data-lgbm)|[LightGBM](https://www.kaggle.com/yamqwe/purged-time-series-cv-lightgbm-optuna)|[Wavenet](https://www.kaggle.com/yamqwe/time-series-modeling-wavenet)|[Technical Analysis #2](https://www.kaggle.com/yamqwe/crypto-prediction-technical-analysis-feats-2)|
|[Catboost Starter](https://www.kaggle.com/yamqwe/purgedgrouptimeseries-cv-extra-data-catboost)|[Catboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-catboost-gpu-optuna)|[Multivariate-Transformer \[written from scratch\]](https://www.kaggle.com/yamqwe/time-series-modeling-multivariate-transformer)|[Time Series Agg](https://www.kaggle.com/yamqwe/features-all-time-series-aggregations-ever)|
|[XGBoost Starter](https://www.kaggle.com/yamqwe/xgb-extra-data)|[XGboost](https://www.kaggle.com/yamqwe/purged-time-series-cv-xgboost-gpu-optuna)|[N-BEATS](https://www.kaggle.com/yamqwe/crypto-forecasting-n-beats)|[Neutralization](https://www.kaggle.com/yamqwe/g-research-avoid-overfit-feature-neutralization/)|
|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-adapted-to-crypto)|[Supervised AE \[Janestreet 1st\]](https://www.kaggle.com/yamqwe/1st-place-of-jane-street-keras-tuner)|[DeepAR](https://www.kaggle.com/yamqwe/probabilistic-forecasting-deepar/)|⏳Target Engineering|
|[Transformer)](https://www.kaggle.com/yamqwe/let-s-test-a-transformer)|[Transformer](https://www.kaggle.com/yamqwe/sh-tcoins-transformer-baseline)||⏳Quant's Volatility Features|
|||||
|[Reinforcement Learning (PPO) Starter](https://www.kaggle.com/yamqwe/g-research-reinforcement-learning-starter)|||⏳Wavelets|

[About the validation: GroupTimeSeriesSplit](https://www.kaggle.com/yamqwe/let-s-talk-validation-grouptimeseriessplit)

(⏳ - in the making..)

Fork them as you please! Enjoy Yourself!

## Auto updating - Full Price Datasets

I created an up-to-today \[auto updating\] dataset which contains the full historical data for all assets of the competition so you can easily build models that utilize it. The datasets are split to each asset since they are much heavier than the competition data. The datasets have also been labeled as described in the competition overview and had been organized in a way that they are at the exact format of the competition data.

**The goal of this is to provide a dataset that:**

1. Contains the FULL history for each asset. Currently, the competition data goes back to 2018. This dataset contains data from even earlier.
2. Auto updating daily - Due to the high volatility of the cryptocurrency market, we should train our models on the most recent data available. These datasets have a backend pipeline for collecting, formatting, and reuploading to kaggle. They are scheduled to be updated daily, every single day until the end of the competition.
3. Preprocessed - The datasets had been ffilled to overcome any missing values issue that is present in the original competition dataset.

**The Datasets:**

* [Binance Coin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-binance-coin)
* [Bitcoin Cash](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin-cash)
* [Bitcoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-bitcoin)
* [Cardano](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-cardano)
* [Dogecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-dogecoin)
* [Eos.io](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-eos-io)
* [Ethereum](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum)
* [Ethereum Classic](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum-classic)
* [Iota](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-iota)
* [Litecoin](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-litecoin)
* [Monero](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-monero)
* [Maker](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-maker)
* [Stellar](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-stellar)
* [TRON](https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-tron)

>**Bonus dataset:** I've also uploaded a dataset containing the most powerful source for predicting cryptocurrencies movement: Elon Musk's Twitter 😂! It is simply an updated dataset of all Elon Musk's tweets 😂. I must check if Elon Musk can help us win! 👌 You can play with it yourself [here](https://www.kaggle.com/yamqwe/elon-musks-twitter-updated-031121).



**Technical details about the Data** For every asset in the competition, the following fields from [Binance's official API endpoint for historical candlestick data](https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#klinecandlestick-data) are collected, saved, and processed.

1. timestamp - A timestamp for the minute covered by the row.
2. Asset\_ID - An ID code for the cryptoasset.
3. Count - The number of trades that took place this minute.
4. Open - The USD price at the beginning of the minute.
5. High - The highest USD price during the minute.
6. Low - The lowest USD price during the minute.
7. Close - The USD price at the end of the minute.
8. Volume - The number of cryptoasset u units traded during the minute.
9. VWAP - The volume-weighted average price for the minute. 10.Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
10. Weight - Weight, defined by the competition hosts [here](https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition)
11. Asset\_Name - Human readable Asset name.

**Indexing** The dataframe is indexed by `timestamp` and sorted from oldest to newest. The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.


Enjoy Yourself! 
And thank you in advance for your support! This is not an easy system to maintain!",45,5,yamqwe,2021-11-18 07:46:01,https://www.reddit.com/r/datasets/comments/qwkpkx/i_created_an_autoupdating_kaggle_dataset_that/,0,datasets
peojd9,"Analyzing US Census Data: Methods, Maps, and Models in R",,45,7,cavedave,2021-08-30 18:53:41,https://walker-data.com/census-r/index.html,0,datasets
ofkmgi,5000 Scientific Papers - Conference Slides Pairs for Automatic Slide Generation,"Hello frens,  
I made available a dataset containing 5000 scientific paper - conference slides pairs for the purpose of automatic slide generation, it is [Available here](https://www.kaggle.com/andrewmvd/automatic-slide-generation-from-scientific-papers).

All credits are due to authors that made it public:  
Athar Sefid, Prasenjit Mitra, Jian Wu, C Lee Giles: Extractive Research Slide Generation Using Windowed Labeling Ranking. Proceedings of the Second Workshop on Scholarly Document Processing (2021).

&#x200B;

Cheers",44,3,larxel,2021-07-07 15:00:59,https://www.reddit.com/r/datasets/comments/ofkmgi/5000_scientific_papers_conference_slides_pairs/,0,datasets
ntkgpc,Database of 999 chemicals based on liver-specific carcinogenicity,,42,9,helpme_change_huhuhu,2021-06-06 12:16:15,https://www.fda.gov/science-research/bioinformatics-tools/nctr-liver-cancer-database-nctrlcdb,0,datasets
mwsdsd,20k+ Hotel Reviews from Yelp for 5 Star Hotels in Las Vegas [self-promotion] [2021-04-22],,46,2,raunaqss,2021-04-23 10:37:30,https://blog.unwrangle.com/2021/04/23/yelp-las-vegas-hotel-reviews-dataset/,0,datasets
llukax,Shark Attack Dataset,,43,1,SubstantialRange,2021-02-17 13:32:09,https://www.kaggle.com/ncsaayali/shark-attack-dataset,0,datasets
fnle50,CSV for bitcoin price history,"I’m the creator of [https://tokendatabase.com](https://tokendatabase.com/), an API service for accessing cryptocurrency market data.

We have a new CSV download feature for candle data from over 5700+ crypto-currency markets. You can check it out here [https://tokendatabase.com/download](https://tokendatabase.com/download)

Let me know if you have any requests or feedback!",44,4,Tokendatabase,2020-03-23 15:03:05,https://www.reddit.com/r/datasets/comments/fnle50/csv_for_bitcoin_price_history/,0,datasets
f2rxjz,US Fading happiness,"US is on a descending trend regarding reported happiness since 2017. US previously had a positive trend with increasing happiness for every year stretching from the start of collecting data in 2013 until 2016. The source providing no explanation model. What is your theory?

[US - World Happiness Index](https://countryeconomy.com/demography/world-happiness-index/usa)",41,35,books-smart,2020-02-12 14:23:28,https://www.reddit.com/r/datasets/comments/f2rxjz/us_fading_happiness/,0,datasets
efnj6t,Where can I find historical flight price datasets or APIs?,I am trying to find historical flight prices to build an airline ticket purchase timing recommendation system. Does anyone know of any data providers or APIs to get this data? I am willing to consider paid solutions if there is no good free solution. I looked at the Skyscanner API and several others but none of them seem to support past dates. Do GDS systems like Amadeus/Sabre have the capability to query and export historical prices?,44,15,cvcnexus30,2019-12-25 23:31:47,https://www.reddit.com/r/datasets/comments/efnj6t/where_can_i_find_historical_flight_price_datasets/,0,datasets
cfab5r,Sharks and Lasers!,"OK, so there are no lasers here, but this dataset does have shark attacks and weather which is close to lasers. :-)  I prepared this dataset containing all of the US shark attacks since 2000 and joined it with weather data for a correlation analysis. I filtered and cleaned it manually before geocoding and adding weather for the dates of the attacks.

[https://drive.google.com/file/d/1fWyXxfVNcDDiXDb\_jYXcGX4AArj39Sc5/view?usp=sharing](https://drive.google.com/file/d/1fWyXxfVNcDDiXDb_jYXcGX4AArj39Sc5/view?usp=sharing)

The shark attack records are from the Global Shark Attack File: [http://www.sharkattackfile.net/](http://www.sharkattackfile.net/)

The geolocations and weather data are from Visual Crossing Weather: [https://www.visualcrossing.com/weather-data](https://www.visualcrossing.com/weather-data)

&#x200B;

I hope that it is interesting and/or useful to others. Any feedback would be appreciated.

(You will note that a few of the recorded didn’t geocode and others didn’t have weather data. Having “most” of the data was good enough for my analysis. If you need 100% coverage, you would need to do some more manually cleaning before geocoding since some of the location are tricky to locate automatically.)

Thanks!

Ginger",45,5,GingerData,2019-07-19 17:17:42,https://www.reddit.com/r/datasets/comments/cfab5r/sharks_and_lasers/,0,datasets
celn6x,"U.S. Street Network Analytic Measures | Metric and topological measures of the street networks of every US city/town, urbanized area, county, census tract, and Zillow-defined neighborhood.",,45,2,tonylstewart,2019-07-18 01:02:46,https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/F5UNSK,0,datasets
ase75r,Is machine learning causing reproducibility crisis in science?,,46,16,jonfla,2019-02-19 19:19:16,https://www.bbc.com/news/science-environment-47267081,0,datasets
ac211b,~2000 common and scientific names for plants (JSON),,43,16,itdnhr,2019-01-03 04:39:47,https://gist.github.com/itdaniher/793b537ed05e1f20b3ffb8fee7ba216f,0,datasets
9tuijp,"Statistics Canada releases Open Database of Building, containing building data for 61 Canadian municipalities",,43,1,Kinost,2018-11-03 14:29:18,https://www.statcan.gc.ca/eng/open-building-data/open-database,0,datasets
9lvj2d,Here's 734 health-related CDC datasets,,45,0,PHealthy,2018-10-06 12:20:44,https://data.cdc.gov/browse,0,datasets
8rx79k,Reddit broke 100 million comments in May -- April and May Reddit Comments are now available!,"##Data (ndjson files)

###April
https://files.pushshift.io/reddit/comments/RC_2018_04.xz

8,371,376,260 bytes (compressed) | 82,497,938,640 bytes (uncompressed)

98,096,451 comments


###May
https://files.pushshift.io/reddit/comments/RC_2018_05.xz

8,633,799,504 bytes (compressed)  | 84,330,099,226 bytes (uncompressed)


100,109,100 comments",47,27,Stuck_In_the_Matrix,2018-06-18 05:55:55,https://www.reddit.com/r/datasets/comments/8rx79k/reddit_broke_100_million_comments_in_may_april/,0,datasets
8aen5g,Update for the Reddit Corpus,"I have worked hard to fill in the gaps for the missing data and will be uploading the new files shortly to https://files.pushshift.io/reddit   (Update:  The files are now available here:  https://files.pushshift.io/reddit/submissions -- the filename format is RS_v2_yyyy-mm.xz)

I would like to thank Nathan Matias (MIT Media Lab) and Devin Gaffney (Northeastern University) for their assistance in reviewing the data (https://arxiv.org/abs/1803.05046) that was collected and pointing out deficiencies.  Their work was a huge contribution to help keep this project as accurate as possible.

Below is a table showing the incompleteness of the existing data dumps for submissions.  The new files will be titled in the format ""RS_v2_yyyy-mm"" For the old files that are being replaced, I will move them to a directory labeled ""old_v1_data"" for historical purposes.  

Here is a breakdown of how data was affected for Reddit submissions. v1_data is the number of submissions from my original data dumps.  v2_data represents the new data that was collected over the past week.  

Date | v1_data | v2_data | Percent Incomplete
:--|--:|--:|--:
 2005-06 |            0 |          103 |      100
 2005-07 |            0 |         1079 |      100
 2005-08 |            0 |         2069 |      100
 2005-09 |            0 |         2449 |      100
 2005-10 |            0 |         3498 |      100
 2005-11 |            0 |         3699 |      100
 2005-12 |            0 |         5356 |      100
 2006-01 |          145 |         8048 |    98.20
 2006-02 |          427 |         9501 |    95.51
 2006-03 |          206 |        12525 |    98.36
 2006-04 |          313 |        12556 |    97.51
 2006-05 |          206 |        14701 |    98.60
 2006-06 |          400 |        16942 |    97.64
 2006-07 |          120 |        24026 |    99.50
 2006-08 |            0 |        40750 |      100
 2006-09 |            0 |        54043 |      100
 2006-10 |            0 |        38333 |      100
 2006-11 |            0 |        36824 |      100
 2006-12 |            0 |        36434 |      100
 2007-01 |          385 |        43725 |    99.12
 2007-02 |          919 |        47317 |    98.06
 2007-03 |          461 |        58642 |    99.21
 2007-04 |            0 |        61544 |      100
 2007-05 |            0 |        65098 |      100
 2007-06 |          606 |        62693 |    99.03
 2007-07 |         1271 |        73248 |    98.26
 2007-08 |            0 |        84952 |      100
 2007-09 |            0 |        91294 |      100
 2007-10 |        58366 |       101633 |    42.57
 2007-11 |       106747 |       106868 |     0.11
 2007-12 |       110969 |       111193 |     0.20
 2008-01 |       141857 |       142310 |     0.32
 2008-02 |       147985 |       147834 |    -0.10
 2008-03 |       168617 |       168227 |    -0.23
 2008-04 |       167745 |       167472 |    -0.16
 2008-05 |       177537 |       177022 |    -0.29
 2008-06 |       191173 |       190682 |    -0.26
 2008-07 |       218585 |       218092 |    -0.23
 2008-08 |       212930 |       212552 |    -0.18
 2008-09 |       256999 |       256268 |    -0.29
 2008-10 |       283972 |       282974 |    -0.35
 2008-11 |       273846 |       272505 |    -0.49
 2008-12 |       285426 |       283915 |    -0.53
 2009-01 |       332872 |       331060 |    -0.55
 2009-02 |       330357 |       329042 |    -0.40
 2009-03 |       364512 |       362805 |    -0.47
 2009-04 |       359146 |       357107 |    -0.57
 2009-05 |       356919 |       355193 |    -0.49
 2009-06 |       385892 |       383969 |    -0.50
 2009-07 |       429196 |       427135 |    -0.48
 2009-08 |       437135 |       435860 |    -0.29
 2009-09 |       444242 |       443037 |    -0.27
 2009-10 |       463647 |       461702 |    -0.42
 2009-11 |       454776 |       452320 |    -0.54
 2009-12 |       494608 |       492225 |    -0.48
 2010-01 |       554641 |       549007 |    -1.03
 2010-02 |       512342 |       506868 |    -1.08
 2010-03 |       612493 |       602586 |    -1.64
 2010-04 |       627439 |       617302 |    -1.64
 2010-05 |       527512 |       515637 |    -2.30
 2010-06 |       489998 |       478396 |    -2.43
 2010-07 |       512149 |       504098 |    -1.60
 2010-08 |       547488 |       537480 |    -1.86
 2010-09 |       610255 |       600209 |    -1.67
 2010-10 |       641176 |       630298 |    -1.73
 2010-11 |       685551 |       674615 |    -1.62
 2010-12 |       742380 |       729840 |    -1.72

**A few points:**

The far right column shows the completeness factor for the original data dumps.  100 represents a complete absence of any data for that time period from the original ingest (v1_data).  Some of the percentages are negative reflecting that the original data dump has more data compared to the new data dumps.  The reason for this is that, over time (since I collected the original data) some submissions were removed from Reddit.  

As you can see from the table above, there was a substantial amount of missing submissions from the original data dumps for data that was submitted to Reddit before November, 2007. 

If you are doing analysis on data from the earlier period of Reddit, you will probably want to grab both the old and new data and do a merge on the data as you see fit.  Keep in mind that the scores will be different for the same ids because Reddit changed how they represent scores for submissions and comments a year or so ago.  Previously, they applied a ""fudge factor"" to scores.  The new scores (from the v2 data) is a more realistic representation of the true score -- so use those scores when possible.  I hope to eventually grab all submissions again to capture the new scores. I will keep the old data for historical / academic purposes.

This is an ongoing project to help make these data dumps as complete as possible.  If you have any questions, please let me know.  

**Update:**

The connection to https://files.pushshift.io has recently been upgraded to a one gigabit fiber connection.  The total bandwidth allotted for downloads is now approximately 100 megabytes per second.  This should substantially help with download speeds when multiple people are downloading the data.  ",44,23,Stuck_In_the_Matrix,2018-04-07 01:01:39,https://www.reddit.com/r/datasets/comments/8aen5g/update_for_the_reddit_corpus/,0,datasets
7ey59v,Curated list of lists of datasets - Some we may already know!,,44,2,charudatta2684,2017-11-23 08:07:02,http://www.analyticsbodhi.com/category/datasets/,0,datasets
55noe3,I created an easier way to discover and explore data.gov by different levels of gov. & individual organizations. Works on mobile too.,,45,3,mskm203,2016-10-03 13:38:09,https://public.tableau.com/views/Data_govrepository/Dashboard1?:embed=y&:display_count=yes,0,datasets
50tmud,"U.S. prisoner admissions, term, and population data from the National Corrections Reporting Program (~18M records)",,45,5,danwin,2016-09-02 14:58:52,http://www.icpsr.umich.edu/icpsrweb/NACJD/studies/36404,0,datasets
4r97sd,"Monthly grain prices in England, 1270-1955",,45,6,cavedave,2016-07-04 20:51:05,http://www.iisg.nl/hpw/poynder-england.php,0,datasets
3kxv4q,[Data] This is the most detailed data yet on what graduates make after college. The Government Just Made It Much Easier to Tell Which Colleges Are a Waste of Money. [USA.ED.GOV],,43,5,ConradKilroy,2015-09-14 18:26:10,http://www.slate.com/blogs/moneybox/2015/09/14/college_scorecard_data_the_government_just_made_it_easier_to_tell_which.html,0,datasets
zb2c1o,Map of corn on the cob street vendors in Mexico City.,"I found [this dataset](https://goo.gl/maps/nojtpSWNEHsAvfQGA). It can me exported to KML to parse and analyse. It's the crowdsourced geolocation of every corn on the cob street vendor in Mexico City.

I dare y'all to train a neural network on it!",43,5,SCP_radiantpoison,2022-12-03 00:06:15,https://www.reddit.com/r/datasets/comments/zb2c1o/map_of_corn_on_the_cob_street_vendors_in_mexico/,0,datasets
waia70,"US Border Patrol, ICE, DHS, Coast Guard, NGO, State, other Fed sources for GIS datasets related to immigration and security on the southern border seem to be intentionally constrained. Spatial data related to border concerns seem purposefully obfuscated and kept away from GIS analysis.","Actual ports of entry data are hidden by undocumented APIs (DHS) and poor UI design.  Updates of what  fence existed on the southern border, which stretches of border fence may have been fixed, replaced or updated are unavailable or simply not maintained or published.   GIS accessible fence data should not depend on who hates / hated Trump more, or at what point in time.

Although some of the secrecy about facility location is understandable it's hard to believe that there are no cohesive immigration data showing the totality of onshore or near-US offshore immigrant communities trying to land a boat on US shores.   Understandable that the Border Patrol folks don't want or need a continuously updated map to all their shifting requirements up and down the border.  

My intent was to create a simple project to see the southern border issues on a map.  This soon became a much more difficult task due to inconsistency and lack of GIS data maintenance.  Concerted effort with Google / Bing showed searching for such data to be a failure.  Scoping the problem is not possible due to the lack of organization of GIS data as well as the apparent lack of Federal GIS professionals in tending to such a complex dataset.  

No wonder the vast majority of US citizens have no idea of what is happening at the border of the US and Mexico.    Without a coherent map showing the depth of the problem there will never be a means to educate US citizens on its scope.

Pointers to data defining location and types of port facilities would be much appreciated.  Similarly, maps of fencing would be helpful; old, new, replaced and / or history would be useful.",42,7,RamblerUsa,2022-07-28 19:05:01,https://www.reddit.com/r/datasets/comments/waia70/us_border_patrol_ice_dhs_coast_guard_ngo_state/,0,datasets
viay1a,There are more male than female specimens in natural history collections,,44,10,cavedave,2022-06-22 17:45:44,https://www.nhm.ac.uk/discover/news/2019/october/more-male-than-female-specimens-in-natural-history-collections.html?utm_source=tw-link-post-20220621-jd&utm_medium=social&utm_campaign=news,0,datasets
v43tzk,I've created an open data index for Turkey. It includes datasets from economy to air quality. Feel free to check the repo and possibly help me by leaving a star! :),,42,1,None,2022-06-03 16:59:08,https://github.com/evrifaessa/open-data-turkey,0,datasets
u4nuup,"One year of research papers on Multiple Sclerosis, still growing and with a free API",,41,5,brunoamaral,2022-04-16 02:26:40,https://gregory-ms.com/downloads/,0,datasets
u3lzi9,"[self-promotion] I broke down our (open) housing dataset to look at the hottest housing markets in the US. Analysis was done with python/polars, code included",,45,14,alecs-dolt,2022-04-14 16:54:45,https://www.dolthub.com/blog/2022-04-13-many-faces-of-housing-market/,0,datasets
s6zyf5,Top 5 Captcha Solving Services for Web Scraping in 2022,,43,1,VictorAVB,2022-01-18 15:22:34,https://webautomation.io/blog/top-5-captcha-solving-services-for-web-scraping/,0,datasets
qbaqn4,"DeepMind buys Physics simulator MuJuCo, will open-source it soon!",,42,0,AdventurousSea4079,2021-10-19 12:44:19,https://deepmind.com/blog/announcements/mujoco,0,datasets
n9wf5i,A global database of COVID-19 vaccinations,,45,5,smurfyjenkins,2021-05-11 12:58:10,https://www.nature.com/articles/s41562-021-01122-8,0,datasets
kncqqg,Datasets to work on for 2021,"What interesting science-themed/ health-themed data can I collect and collate in the course of a year for a proposed #datascience analysis after 365 days? Thanks.
PS: I am a student of Medical Microbiology.",44,13,BioIsaac6716,2020-12-30 22:46:09,https://www.reddit.com/r/datasets/comments/kncqqg/datasets_to_work_on_for_2021/,0,datasets
k1n2f6,Executions in the U.S. 1608-2002: The Espy File,,47,0,cavedave,2020-11-26 20:47:31,https://deathpenaltyinfo.org/executions/executions-overview/executions-in-the-u-s-1608-2002-the-espy-file,0,datasets
jhtqdh,[Dataset] Images of Male and Female Chickens,,45,6,cavedave,2020-10-25 13:57:38,https://drive.google.com/drive/folders/1eGq8dWGL0I3rW2B9eJ_casH0_D3x7R73,0,datasets
iwm4vv,Today saw dozens of potentially career-ending football injuries on the field due to lacking a preseason. Does anyone know where I could find longitudinal data related to Football on-field injuries so I could visualize the reality of this?,"Title says it all. 

Best,

u/Zaddiac",42,10,Zaddiac,2020-09-20 20:59:28,https://www.reddit.com/r/datasets/comments/iwm4vv/today_saw_dozens_of_potentially_careerending/,0,datasets
i1h3kx,We Combined The PPP Loan Data Sets Into a CSV [self-promotion],,46,4,mergeyourdata,2020-07-31 22:33:38,/r/tableau/comments/i1gf9l/we_combined_the_ppp_loan_data_sets_into_a_csv/,0,datasets
gp3ops,Transform Text Files to Data Tables,"Hi guys, I wrote a short guide to extract information from text files, combine them in a data frame and export the data with python. Since I usually work with java and this is my first article ever, I highly appreciate any feedback! Thanks!

https://medium.com/@sebastian.guggisberg/transforming-text-files-to-data-tables-with-python-553def411855",44,6,guggio,2020-05-23 11:57:23,https://www.reddit.com/r/datasets/comments/gp3ops/transform_text_files_to_data_tables/,0,datasets
gnxped,Financial Crimes News Intelligence Dataset (registration needed),,44,3,cavedave,2020-05-21 14:20:29,http://info.aylien.com/financial-crimes-dataset-download?utm_campaign=Dataset%20Downloads%20-%20All&utm_content=129885470&utm_medium=social&utm_source=twitter&hss_channel=tw-186325183,0,datasets
gg5pb1,Anyone in need of Datasets?,"Hello all,

I have a week off and wanted to do a quick RPA project, mostly for the COVID-19 pandemic, but can be for anything. If anyone needs a specific dataset that needs to be scraped, gathered, or organized in some fashion, comment it below!

&#x200B;

**Update:** So I did some research today and concluded that I will attempt to do 2 of the most requested datasets this week, time permitting and prioritized as follows.

1. Coronavirus daily cases count per country, updated daily. Might upload to a GitHub for it unless we have another suggestion for that.
2. Instead a strict data set for someone yawning for example, Im going to be looking into building a solution that can easily mine data of whatever type of picture using google images. While this may lead to some junk in the data, I believe the dynamic / generic value of the bot will be greater. I can distribute a how-to-guide on using the bot, and ways to improve the data it mines. If anyone has any other suggestions, please feel free to comment.

If either of these fall through, I will be working on a dataset for the environmental or social factors to compare the impacts of covid. Thanks for all of the awesome ideas! I will look to post the links here.

Also thanks for the award!

**Update 2:** I have mostly been working on the generic solution to data mining desired pictures, however I also created this repo with the initial upload of COVID-19 cases. If anyone has any suggestions, please let me know. I will be working on a way to collect older daily data, though I plan on updating this every night at 9PM EST, which will represent that current day's case count.

That can be found here:  [https://github.com/Ryzen120/COVID-19\_Daily\_Cases](https://github.com/Ryzen120/COVID-19_Daily_Cases)

**Update 3:** Discontinuing my daily case project, as I found this.

[https://ourworldindata.org/coronavirus-data](https://ourworldindata.org/coronavirus-data) \-> Chart -> Data -> Download csv.

I am still continuing on the picture mining bot.",41,56,Ryzen120,2020-05-09 01:00:12,https://www.reddit.com/r/datasets/comments/gg5pb1/anyone_in_need_of_datasets/,0,datasets
dj2jnf,A Structural Reevaluation of the Collapse of World Trade Center 7 - Open Data Released from University of Alaska Fairbanks Study,,44,2,futureroboticist,2019-10-17 06:46:38,https://www.reddit.com/r/engineering/comments/ditvcn/a_structural_reevaluation_of_the_collapse_of/,0,datasets
cne83w,Data on Mass Shooters,"With this new bill in Congress that allows assessment of individuals mental state by police, it got me wondering if there was a comprehensive database on people who commit these mass shootings?  Worse case scenario I guess I'll scrape wikipedia, but I'm assuming I'm not the first person to want this data.",44,39,AyEhEigh,2019-08-08 00:40:52,https://www.reddit.com/r/datasets/comments/cne83w/data_on_mass_shooters/,0,datasets
c073oh,awesome-public-datasets: A topic-centric list of HQ open datasets.,,44,2,cavedave,2019-06-13 15:19:01,https://github.com/awesomedata/awesome-public-datasets,0,datasets
boxx01,"The full text of the Bible, available in several machine-readable formats",,43,6,TrueBirch,2019-05-15 13:25:44,http://www.hackathon.bible/data.html,0,datasets
b761im,Federal Reserve's new time series on wealth distribution,"introduction paper: [https://www.federalreserve.gov/econres/feds/files/2019017pap.pdf](https://www.federalreserve.gov/econres/feds/files/2019017pap.pdf)

dataset: [https://www.federalreserve.gov/RELEASES/Z1/current/](https://www.federalreserve.gov/RELEASES/Z1/current/)",44,1,loopback2019,2019-03-30 02:28:41,https://www.reddit.com/r/datasets/comments/b761im/federal_reserves_new_time_series_on_wealth/,0,datasets
axvfli,End of day stock market data for 3396 US companies (5 years),"[https://drive.google.com/file/d/1ElC\_Jsc4ZAq8CNXQgx23OmhVFeaPZXd3/view?usp=sharing](https://drive.google.com/file/d/1ElC_Jsc4ZAq8CNXQgx23OmhVFeaPZXd3/view?usp=sharing)

&#x200B;

This zipped file contains CSV with the stock symbol as the name of the file. Each file contains all date, open, high, low, close and volume data avalible with in the date range of 03/06/2014 - 03/05/2019.

&#x200B;

Source: IEX",42,1,atreyuroc,2019-03-06 05:54:15,https://www.reddit.com/r/datasets/comments/axvfli/end_of_day_stock_market_data_for_3396_us/,0,datasets
aa07oe,Hospital price list,"Starting Jan 1st, US hospitals have to post prices online. Anyone have a link of a price list that is already up?",41,9,karlw00t,2018-12-27 15:42:12,https://www.reddit.com/r/datasets/comments/aa07oe/hospital_price_list/,0,datasets
8du8ty,"BBC release 16,000 clips of sound effects noises",,44,2,cavedave,2018-04-21 07:09:58,http://bbcsfx.acropolis.org.uk/,0,datasets
6rl21o,Dataset of 6 years of clickbait/fake headlines from a single source,,44,0,therohk,2017-08-04 15:07:07,https://www.kaggle.com/therohk/examine-the-examiner,0,datasets
6oirtz,"Get 20,000 dog pictures programmatically with this Dog API",,44,4,erinpetenko,2017-07-20 19:51:17,https://dog.ceo/dog-api/,0,datasets
664e6o,/r/place dataset published by reddit on BigQuery and CSV (full 16M tiles placement history),,43,0,fhoffa,2017-04-18 17:42:47,https://www.reddit.com/r/redditdata/comments/6640ru/place_datasets_april_fools_2017/,0,datasets
4odknc,"Script for scraping *ALL* posts from a Facebook Page/Group, including individual Reaction counts",,42,3,minimaxir,2016-06-16 14:26:28,https://github.com/minimaxir/facebook-page-post-scraper,0,datasets
3h072g,"The military equipment and guns the Pentagon has given to civilian police, ~195,000 records",,44,4,danwin,2015-08-14 18:17:20,https://github.com/datahoarder/leso_1033,0,datasets
2k02rj,We put together a list of OpenData Sources for our Applicants but it was so popular we thought we'd share it here!,"[Part 1](http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-1/) and [Part 2](http://blog.thedataincubator.com/2014/10/data-sources-for-cool-data-science-projects-part-2/)

Like the title says, we provide our applicants a list of open data sources for their challenge questions, but the posts have been so popular we thought r/datasets would like them too. Enjoy!",46,5,None,2014-10-22 15:48:18,https://www.reddit.com/r/datasets/comments/2k02rj/we_put_together_a_list_of_opendata_sources_for/,0,datasets
15970ym,"Github: List of Public, Real-time Datasets",,46,1,semicausal,2023-07-25 12:24:48,https://github.com/bytewax/awesome-public-real-time-datasets,1,datasets
14kakga,DiffusionDB: A large-scale text-to-image prompt gallery dataset based on Stable Diffusion,,42,1,yaph,2023-06-27 11:02:07,https://github.com/poloclub/diffusiondb,0,datasets
10y2zpc,"24,000 High-Resolution Left-Facing Sneaker Images for Machine Learning (GANs) and Computer Vision",,43,0,amirdol7,2023-02-09 19:02:33,https://www.kaggle.com/datasets/aahashemi/sneaker-image-dataset,0,datasets
zf6qze,[self-promotion] Open Source U.S. Healthcare Transparency Data,"Hey ya'll, I work on a project dedicated to helping US consumers navigate the hellscape that is US healthcare.

One aspect of the project involves designing and maintaining open source datasets that help inform existence, pricing, and practices of healthcare providers, insurers, and plans. Currently we expose this in flat files, just for accessibility for a broad audience. A lot of the data is naturally relational in nature. You can check it out here:

[https://github.com/TPAFS/transparency-data](https://github.com/TPAFS/transparency-data)  


Worth noting: There are many efforts doing this sort of work (particularly because new-ish laws require a lot of self-reporting from hospitals and insurers), but there are not many efforts that both curate centralized, complete data on these topics, and open source it. Among efforts that do both that I know of, the data in this repo tends to be complementary. The data that exists in the repository currently all comes from data which is made public or required to be made public by the US gov't, but the plan is to crowdsource lots of other data that is nonexistent on the internet, and to succeed in that, we'll need help. Would love to hear your thoughts and feedback.",41,7,tpafs,2022-12-07 17:06:38,https://www.reddit.com/r/datasets/comments/zf6qze/selfpromotion_open_source_us_healthcare/,0,datasets
ylj17o,Selling a data set for the first time - how to price?,"Hello all, 

I have been approached by an AI company that would like to purchase rights to a clinical data set (depersonalized/anonymous) and I would like to know what factors to consider in pricing it out.  

Does any have any recommendations on what to consider, or if there are any good resources to review on this topic? 

Factors I am considering in the pricing are:

The past cost of acquiring the data in assessments (the labor costs)

the licensing fees of the assessment tools

The cost of cleaning and anonymizing the data (more labor costs)

the length of the license term

&#x200B;

Any info on this topic would be most appreciated!",45,13,FUS-RO-DONT,2022-11-03 23:54:26,https://www.reddit.com/r/datasets/comments/ylj17o/selling_a_data_set_for_the_first_time_how_to_price/,0,datasets
yl1qo5,Anime and Manga Data + summary analysis,"This data was scraped from MAL (MyAnimeList), an anime and manga social networking and social cataloging application website run by volunteers. The site provides its users with a list-like system to organize and score anime and manga. With this information, users can find out more about their favorite animes, look for new anime to watch in the future, or even use an analytical lens and try to understand what makes successful shows successful. This dataset includes the title, type, mean rating, number of scoring users, status, number of episodes, start date, end date, source, and SO much more.

\[Self-promo\] We've done some light **analysis** on this data and re-hosted it here for exploration online before downloading: [https://www.gigasheet.com/data-community/anime-analytics](https://www.gigasheet.com/data-community/anime-analytics)

**Source**: [https://www.kaggle.com/datasets/andreuvallhernndez/myanimelist](https://www.kaggle.com/datasets/andreuvallhernndez/myanimelist)",44,1,n1nja5h03s,2022-11-03 12:37:44,https://www.reddit.com/r/datasets/comments/yl1qo5/anime_and_manga_data_summary_analysis/,0,datasets
xpo29j,United States Presidential Debate Transcripts (primaries included) 1960-2020,"I found this nice dataset on GitHub that covers all of the US Presidential debates since 1960 and has the most coverage of presidential primary debates in those years.

According to the page the data was scraped from The American Presidency Project. The dataset has variables that let users know if the speaker is a candidate, what type of debate the text comes from, and what election year the debate occurred in.

[https://www.kaggle.com/datasets/josiahmcmillan/us-presidential-debate-transcripts](https://www.kaggle.com/datasets/josiahmcmillan/us-presidential-debate-transcripts)

[https://github.com/JosiahMcMillan/PresDebates](https://github.com/JosiahMcMillan/PresDebates)",41,0,Ordoliberal,2022-09-27 18:07:49,https://www.reddit.com/r/datasets/comments/xpo29j/united_states_presidential_debate_transcripts/,0,datasets
rbnub8,Has anyone got the leaked tiktok algo 101 document?,"Sorry if this isn't the right place, but since the document is about what to do with data I would be very interested. Searched some forums but didn't find anything.",43,22,woodandscrews,2021-12-08 09:50:28,https://www.reddit.com/r/datasets/comments/rbnub8/has_anyone_got_the_leaked_tiktok_algo_101_document/,0,datasets
q7xeko,Apple AppStore Apps Dataset with 1.2 million apps,"Apple AppStore Apps dataset with 1.2 million application data (21 attributes) now available to download for free from Kaggle or Github (Json).

Kaggle: [https://www.kaggle.com/gauthamp10/apple-appstore-apps](https://www.kaggle.com/gauthamp10/apple-appstore-apps)

Json format: [https://github.com/gauthamp10/apple-appstore-apps](https://github.com/gauthamp10/apple-appstore-apps)",43,17,ClassicLaw9284,2021-10-14 11:04:20,https://www.reddit.com/r/datasets/comments/q7xeko/apple_appstore_apps_dataset_with_12_million_apps/,0,datasets
pn0hpe,Art Gallery Datasets: a collection of museum datasets from around the world,,41,0,yourbasicgeek,2021-09-12 20:30:49,https://www.artnome.com/art-data,0,datasets
nk5cue,Looking for a Pokemon Dataset,"I'm looking for a dataset of all 825 Pokemon (this includes Alolan Forms).  It would be preferable if there are at least 100 images of ***each*** individual Pokemon. This would be used to train an ML program to identify the Pokemon image, and autoping users on Discord for their shiny hunts on bots such as Poketwo. 

Thanks!",43,6,Sea_Pickle07_,2021-05-24 18:56:40,https://www.reddit.com/r/datasets/comments/nk5cue/looking_for_a_pokemon_dataset/,0,datasets
l62bl8,IMDb TV Dataset,,40,4,alotofrandomcrap,2021-01-27 11:10:32,https://www.kaggle.com/hazimahmed/imdb-tv-dataset,0,datasets
jw3mnr,A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896,"This is data going back to 1896 that shows how the Dow Jones performed during times when Mars was within 30 degrees of the lunar node. The data contains the daily percentage changes of the Dow Jones since 1896. [https://zenodo.org/record/3711110#.X7KJEGhKjIU](https://zenodo.org/record/3711110#.X7KJEGhKjIU)

also a year by year comparison (going back to 1897) between the annual Dow Jones returns vs annual Dow Jones returns that leave out the days that the planet Mars was within 30 degrees of the lunar node [https://zenodo.org/record/4038211#.X2YAz2hKjIU](https://zenodo.org/record/4038211#.X2YAz2hKjIU)",46,9,thedowcast,2020-11-17 22:55:07,https://www.reddit.com/r/datasets/comments/jw3mnr/a_hypothesis_that_the_federal_reserve_can_set/,0,datasets
jkznsi,The Harvard admissions data 'made public'?,"I'm wondering: does anyone know where I might be able to find the Harvard admissions data that was supposedly made public by that lawsuit from awhile back?  

https://news.harvard.edu/gazette/story/2018/06/documents-released-in-admissions-lawsuit/

This article says the data has been 'released' and 'made public', yet I can't seem to find the raw dataset even though you can find the reports by the dueling economists from either side of the lawsuit about their regression analyses.  I have to do my own regression analysis for econometrics and remembered that this data should be out there somewhere, anyone know where?  Thanks!",43,3,Starcraft_III,2020-10-30 16:10:27,https://www.reddit.com/r/datasets/comments/jkznsi/the_harvard_admissions_data_made_public/,0,datasets
hjkii3,Open Elections SQL database,"Open Elections is an exciting source of election data, but having the data spread across per state repository and directory per cycle makes it hard to consume. Also CSVs do not provide type safety. We built a Dolt dataset using the precinct level voting totals (almost) nationwide. Would be great to get contributions from those interested.

Also, Open Elections is awesome and folks should contribute to the valuable work they are doing!

&#x200B;

[https://www.dolthub.com/blog/2020-07-01-open-elections/](https://www.dolthub.com/blog/2020-07-01-open-elections/)

[https://www.dolthub.com/repositories/open-elections/voting-data](https://www.dolthub.com/repositories/open-elections/voting-data)",46,4,oscarbatori,2020-07-01 22:20:55,https://www.reddit.com/r/datasets/comments/hjkii3/open_elections_sql_database/,0,datasets
ghxgc2,SQL version of NBA player data,"[https://www.dolthub.com/repositories/Liquidata/nba-players](https://www.dolthub.com/repositories/Liquidata/nba-players)

As part of a blog post, I scraped the [NBA.com](https://NBA.com) API and got all player season and career statistics. I put it in Dolt and created per game and per 36 views. It took me a few days to work around [NBA.com](https://NBA.com) rate limiting so I thought I'd let the community know the Dolt version exists. You can clone this in seconds and start running SQL, much better than doing your own scraping.",43,5,timsehn,2020-05-11 21:59:21,https://www.reddit.com/r/datasets/comments/ghxgc2/sql_version_of_nba_player_data/,0,datasets
eyyc4k,"[OC] Bulk Campaign Finance Data - ActBlue (1378435) filed Jan 31, 2020","Hello /r/datasets!

## What?

On Friday, [ActBlue](https://secure.actblue.com/) filed their [2019 Year End Report](https://www.fec.gov/data/committee/C00401224/?tab=filings), and it's an absolute beast - the largest filing ever generated by the [Federal Election Commission](https://www.fec.gov/)! It contains 24,656,453 contributions by individuals totaling **$525,124,217.30**. 

We publicly open sourced [our analysis](https://github.com/CircaVictor/actblue-analysis-1378435) of the filing along with [state data sources](https://github.com/CircaVictor/actblue-analysis-1378435/tree/master/data/states). This type of data is never seen in a timely manner (or at all) and is a HUGE undertaking to export and analyze, examples:

* [https://fivethirtyeight.com/features/how-actblue-is-trying-to-turn-small-donations-into-a-blue-wave/](https://fivethirtyeight.com/features/how-actblue-is-trying-to-turn-small-donations-into-a-blue-wave/) [[data]](https://github.com/fivethirtyeight/actblue-analysis)
* [https://www.buzzfeednews.com/article/katherinemiller/elizabeth-warren-actblue-data-bernie-sanders-kamala-harris](https://www.buzzfeednews.com/article/katherinemiller/elizabeth-warren-actblue-data-bernie-sanders-kamala-harris) [[data]](https://github.com/BuzzFeedNews/2019-08-actblue-donations)

If you curious about some fresh high level data, here are some numbers that we pulled across the entire cycle for the [Iowa Caucus ](https://drive.google.com/drive/u/1/folders/18AMOMG01cyGgwTWTH0R_ZreRnNMTbCL2). Surprisingly a lot of individuals from [tech companies](https://i.imgur.com/nvGEsUy.png) (from Bernie's ""Overall by Employer"" tab) are now contributing.

## Additional Analysis

  * [Principal Committees by total](https://github.com/CircaVictor/actblue-analysis-1378435/blob/master/analysis/principal-committees-overall.csv)
  * [Individual contributors by total](https://github.com/CircaVictor/actblue-analysis-1378435/blob/master/analysis/individual-contributors-overall.csv)
  * [Zipcodes by average >= 250 contributions](https://github.com/CircaVictor/actblue-analysis-1378435/blob/master/analysis/zip_codes-avg.csv)

## Who?

My name is Justin and I'm the cto cofounder of a nonpartisan company named [Circa Victor](https://circavictor.com/). Since 2015 we have taken a tech approach to political problems starting with Federal Campaign Finance.

To accomplish this goal, we created a clone of the [Federal Election Commission](https://www.fec.gov/) and have tracked EVERY data point that has been generated since January 1st, 2010.

## Why?

We feel that it's every citizen's right to be informed. We are not trying to push any political agenda. Our desire is data comprehension and accuracy. 

We __*strongly*__ believe that accurate, up-to-minute data will lead to better, responsible, and actionable reporting which in turn will help nurture an informed populace.

*Anecdotally*: I have a friend who worked at a payment processor (rhymes with pipe) and internally they felt Trump had a bigger chance of winning than the media lead on due to the sheer number of payments they were processing for him vs Hillary.

I'm NOT saying that Campaign Finance is the missing piece to figuring out who is going to win an election. However, campaign finance data is a LOT more concrete than polling data as polling data is relative. It's the missing vertical that most people are knowingly overlooking because they are not looking at it in an accurately and timely manner.

Unfortunately we feel that we are the only group actively working toward this. We have reached out to Committees, Non-Profits, Researchers, Campaign Finance leaders all with little to no traction. 

We are a small team of 4 guys (I'm the only software engineer; self-taught, didn't go to college) and we humbly admit that this problem is too big for us to solve alone. We need help and want to raise awareness. The best way we know how to do that is to press data into the public's hands.

---

To be more transparent, this data work does not make enough money to pay the bills. None of us have taken a paycheck since the Winter of 2017, but we have continued to work on this because we feel this is imporant.

To pay the bills, I work from Seattle as a remote software engineer for [PBS Kids](https://twitter.com/twaffl3s/status/1207163692787257344) - a job which I love and am very proud of.

Happy to answer any questions as time permits. Feel free to dm me if you want a response or if you're looking for more in depth data and analysis. Thank you.

---

**tldr;** we built a clone of the Federal Election Commission. As a public service we are open sourcing our analysis w/ data of the largest filing (ActBlue 1378435.fec) ever generated containing 24,656,453 contributions totaling $525,124,217.30.",44,9,transphorm,2020-02-04 22:24:32,https://www.reddit.com/r/datasets/comments/eyyc4k/oc_bulk_campaign_finance_data_actblue_1378435/,0,datasets
dkurvp,Student Debt Dataset,I need student debt data for analyzing the trends changing over the period of time.,43,21,abhi_d104,2019-10-21 03:46:09,https://www.reddit.com/r/datasets/comments/dkurvp/student_debt_dataset/,0,datasets
azmbjh,"The Digital Hoarders Who Collect Tumblrs, Medieval Manuscripts, and Terabytes of Text Files",,42,2,cavedave,2019-03-11 00:00:44,https://gizmodo.com/delete-never-the-digital-hoarders-who-collect-tumblrs-1832900423,0,datasets
9m4ek0,Social inequality datasets,,41,1,PHealthy,2018-10-07 11:33:27,https://opportunityinsights.org/data/,0,datasets
71cjxv,"Ten thousand books, six million ratings",,45,0,None,2017-09-20 17:13:28,https://github.com/zygmuntz/goodbooks-10k,0,datasets
5cosw9,cnn exit poll json us election 2016,,44,5,None,2016-11-13 06:33:44,http://data.cnn.com/ELECTION/2016/full/P.full.json,0,datasets
53sbya,A simple dataset of 15 million+ Stack Overflow questions and tags,,46,3,cavedave,2016-09-21 09:25:31,https://github.com/dgrtwo/StackLite,0,datasets
4vebnf,I built a dataset of self-reported symptoms of autoimmune diseases,"Over the past two years I've built a tool for patients of chronic autoimmune and ""invisible"" illnesses to track their symptoms, treatments, and disease triggers. It recently launched to the public at www.flaredown.com after a lengthy private beta.

The goal is to use the data the Flaredown gathers to identify patterns and correlations that might help patients of these hard-to-treat illnesses. For example:

- Does X treatment affect Y symptom positively/negatively/not at all?

- Are there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments?

- Can we reliably predict what triggers a flare for a given user or all users with a certain condition?

- Could we recommend treatments effectively, based on similarity of users rather than specific symptoms? (like Netflix recommendations for treatments) 

- Can we quantify a patients level of disease activity based on their self-reported symptoms? How different is it from our existing measures?

We're starting to accrue a real body of data by now and I want to share it with data scientists, researchers, or anyone interested in questions like the ones above. To start, I could use advice on how to format the data for this kind of use. But after that I really want to dig in and start analyzing what we've got.

Let me know your thoughts, and if you're interested in helping out!",43,12,captfitz,2016-07-30 22:24:59,https://www.reddit.com/r/datasets/comments/4vebnf/i_built_a_dataset_of_selfreported_symptoms_of/,0,datasets
3pl8v4,"MassMine: An open source command line tool for harvesting data from Twitter, Tumblr, Wikipedia, Google Trends (and more to come)",,47,3,n3mo,2015-10-21 03:58:12,http://www.massmine.org/,0,datasets
381oj2,1001 Datasets and Data repositories ( List of lists of lists ) - updated link,,40,0,rustyoldrake,2015-06-01 05:20:21,https://dreamtolearn.com/ryan/1001_datasets,0,datasets
1dacxt,I've collected and categorised over 173+ publicly available Social Network Datasets and put them on a wiki I built. Have fun (and add more / update the existing list if you can)!,,46,5,None,2013-04-28 19:24:08,http://arcane-coast-3553.herokuapp.com/sna/visual,0,datasets
12u90v4,Taylor Swift (42 Albums) Lyrical Data in textual format [self-promotion],"I started on this idea of how there's a taylor swift song for almost every generic scenario one could think of and thought maybe I could analyse sentiment for it. Quickly found out that I'll have to collect it on my own since the other sources I found (mainly on Kaggle) were not of the desired format (I wanted completely textual data).

So sharing it here in case it helps anyone else.

>[**Dataset link**](https://www.kaggle.com/datasets/ishikajohari/taylor-swift-all-lyrics-30-albums)

This data was collected using the lyricsgenius python library and the Genius API.

Also sharing the other datasets I found if they might help someone - 

* [https://www.kaggle.com/datasets/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums](https://www.kaggle.com/datasets/PromptCloudHQ/taylor-swift-song-lyrics-from-all-the-albums)
* [https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums](https://www.kaggle.com/datasets/thespacefreak/taylor-swift-song-lyrics-all-albums)",42,7,ishika_jo,2023-04-21 16:01:16,https://www.reddit.com/r/datasets/comments/12u90v4/taylor_swift_42_albums_lyrical_data_in_textual/,0,datasets
y1ev3p,Dataset: Google Maps list of attractions in the most visited cities in the world,"[This exploration](https://www.metabase.com/blog/data-guide-to-travel) contains two datasets:
- Top 100 City Destinations in 2019 by [Euromonitor](https://go.euromonitor.com/white-paper-travel-2019-100-cities.html)
- Scraped attractions data from Google Maps for those destinations.",43,1,plaincandy,2022-10-11 17:24:55,https://www.reddit.com/r/datasets/comments/y1ev3p/dataset_google_maps_list_of_attractions_in_the/,0,datasets
v8em02,Interesting Datasets for Exploratory Data Analysis?,"Hello! I'm looking for ideas about interesting datasets/topics to perform EDA on. I would like to avoid classic datasets like housing, stock market, sports related etc and find something a bit more unique. I would also like to avoid medical datasets as I have zero knowledge on the topic. 

I would like to find a dataset on which EDA can provide valuable information using graphs.

More specifically, ideally I'm looking for a dataset with these characteristics:

* Interesting, intriguing, unique topic
* More than 10-15 features
* Mix of feature types but mainly numeric or ordinal
* Minimum a couple of hundred instances
* Datasets that can be used in Machine Learning/Deep Learning

I'm eager to hear your suggestions. I would also love to hear what's the most interesting/unique dataset you've worked with even if it's not publically availliable or doesn't fit into my list of characteristics.",46,11,Water-Friendly,2022-06-09 11:22:53,https://www.reddit.com/r/datasets/comments/v8em02/interesting_datasets_for_exploratory_data_analysis/,0,datasets
u5jupd,ArcaneFaces - A Handcrafted Large-Scale Dataset of Faces from Arcane,"Hey everyone!  
I have created a dataset of 3411 high-quality images taken from all 9 episodes of the show, as well as the music video Enemy by Imagine dragons.  
You can use the images from the dataset for generating new characters from this image domain or creating a paired dataset for training your own Arcane machine learning models, as well as various other uses.  
Link to the GitHub repo with the dataset: [https://github.com/avermilov/ArcaneFaces](https://github.com/avermilov/ArcaneFaces)",45,0,ArtyMentlegen,2022-04-17 09:51:37,https://www.reddit.com/r/datasets/comments/u5jupd/arcanefaces_a_handcrafted_largescale_dataset_of/,0,datasets
nwi94g,Daily Air Quality Index Dataset of some of the major Indian cities,"Hi All,

People from India interested in how the Air Quality has been varying since 2019 could use this dataset for research and analysis.

[Air Quality Index Data](https://www.kaggle.com/sumandey/daily-air-quality-dataset-india)

Dataset Source: [https://aqicn.org](https://aqicn.org)

P.S.: Please share and leave an upvote in Kaggle.

&#x200B;",43,3,Classic_Eagle_4137,2021-06-10 08:07:33,https://www.reddit.com/r/datasets/comments/nwi94g/daily_air_quality_index_dataset_of_some_of_the/,0,datasets
nw16vz,Cars for sale on Germany from 2011 to 2021,"Dataset obtained scraping [https://www.autoscout24.com/](https://www.autoscout24.com/). In the file, you will find features describing 46405 vehicles: mileage, make, model, fuel, gear, offer type, price, horse power, registration year

[https://www.kaggle.com/ander289386/cars-germany/tasks](https://www.kaggle.com/ander289386/cars-germany/tasks)

If you are interested in more data or a different Country/Region let me know.

Edit: if anyone uses the data for a demo or anything I'd be grateful if you share it :D",45,20,AnderRV,2021-06-09 17:20:53,https://www.reddit.com/r/datasets/comments/nw16vz/cars_for_sale_on_germany_from_2011_to_2021/,0,datasets
npdn6r,"Copenhagen network dataset combines network data, personality traits, GPS data, and course grades",,41,1,Quantsel,2021-05-31 21:18:44,https://www.nature.com/articles/s41597-019-0325-x,0,datasets
nknfvw,30x30 m Worldwide High-Resolution Population and Demographics Data,"We created an ETL pipeline for Facebook's research project to provide detailed large-scale demographics data. It's broken down into roughly 30x30 m grid cells and provides info on groups by age and gender.

👾 [https://github.com/kuwala-io/kuwala/tree/master/kuwala-pipelines/population-density](https://github.com/kuwala-io/kuwala/tree/master/kuwala-pipelines/population-density)  
📖 [https://medium.com/kuwala-io/querying-the-most-granular-demographics-dataset-62da16b441a8](https://medium.com/kuwala-io/querying-the-most-granular-demographics-dataset-62da16b441a8)",43,9,kuwala-io,2021-05-25 11:27:05,https://www.reddit.com/r/datasets/comments/nknfvw/30x30_m_worldwide_highresolution_population_and/,0,datasets
mwgtbg,Where can I find more realistic data sets to use to teach a course?,"When I last taught my course, I had a request from a student that they wanted to have realistic datasets that they would possibly be working at an actual company. I remember I tried searching online but was not able to. For this upcoming semester I would like to see if I can have such datasets since I know I might possibly get the same request again.",42,40,kosar7,2021-04-22 22:38:48,https://www.reddit.com/r/datasets/comments/mwgtbg/where_can_i_find_more_realistic_data_sets_to_use/,0,datasets
mnixnr,Reddit Imposter April Fools Dataset - 5 Million Games of the recent Reddit April Fools project. JSON and Mysql Exports.,,43,3,Cintanyervadasz,2021-04-09 14:53:57,http://spacescience.tech/,0,datasets
mc4i8u,Data Is Plural. Dataset resource. [Repost],,42,9,cavedave,2021-03-24 12:31:06,https://www.data-is-plural.com/,0,datasets
l45g0h,"Where to find datasets that involve legislation, legal rulings and policy proposals?",Does such a database exist?,39,8,RoughDraft95,2021-01-24 19:04:19,https://www.reddit.com/r/datasets/comments/l45g0h/where_to_find_datasets_that_involve_legislation/,0,datasets
jvp37f,Indian Toilet Data,,42,9,cavedave,2020-11-17 07:51:36,https://sbm.gov.in/sbmdashboard/IHHL.aspx,0,datasets
jnynl2,2020 November General Election Turnout Rates,,41,1,cavedave,2020-11-04 15:28:36,http://www.electproject.org/2020g,0,datasets
iry829,"Dataset: Transnational Terrorist Hostage Event (TTHE) Data Set, 1978 to 2018. Four types of hostage incidents—kidnappings, barricade missions, skyjackings, and non-aerial hijackings—are recorded for a global sample.",,43,0,smurfyjenkins,2020-09-13 13:24:57,https://journals.sagepub.com/doi/full/10.1177/0022002720957714,0,datasets
gofbnw,Face Mask Detection Dataset," [https://makeml.app/datasets/mask](https://makeml.app/datasets/mask)  


Also available here:  
 [https://www.kaggle.com/andrewmvd/face-mask-detection](https://www.kaggle.com/andrewmvd/face-mask-detection)",42,1,larxel,2020-05-22 08:15:48,https://www.reddit.com/r/datasets/comments/gofbnw/face_mask_detection_dataset/,0,datasets
ghqaw2,see19 - Comprehensive COVID Dataset,"All, 

I have spent the last several weeks compiling my own aggregate dataset of covid19 and have decided to make it publicly available [here](https://github.com/ryanskene/see19).

It has case and fatality counts covering over 300 regions including provincial / state level data for the US, Brazil, Canada, Australia, Italy, and China.

The data includes exogenous factors for each region (either country or state level) including a wide array of demographic age ranges, land and city density, daily average temperature, uvb radiation, relative humidity, pollution, the Oxford Government Response Tracker, Google mobility data, and some rough GDP and international travel estimates.

And its all rolled up into one csv file.

you can [download the csv directly from github](https://github.com/ryanskene/see19)

i have also developed a python package to further manipulate the dataset and generate a number visualization tools. [you can download the package here](https://pypi.org/project/see19/)

I have used the package to generate all the charts I have posted here on reddit and on a new [twitter feed you can find here](https://twitter.com/covidcharts1). The data still has some kinks but it has become a pretty effective tool for me the last couple weeks.

[All of the direct sources are listed here](https://ryanskene.github.io/see19/#section2.1)

I endeavour to update daily.

Any input or feedback is of course welcome.",41,7,None,2020-05-11 16:01:06,https://www.reddit.com/r/datasets/comments/ghqaw2/see19_comprehensive_covid_dataset/,0,datasets
fuumgc,Turkey Coronavirus Cases by Cities,"Hi everyone,

Since april 1st, Turkey Ministry of Health started to announce coronavirus cases by cities. There is no open dataset for access to these numbers of cases. So I have gathered the announced data. Anyone interested can use these dataset by the link below:

[https://github.com/GoktugOcal/covid19/blob/master/data/turkeyConfirmedCasesbyCities.csv](https://github.com/GoktugOcal/covid19/blob/master/data/turkeyConfirmedCasesbyCities.csv) 

I will be updating dataset by every new announcement. Thanks.",42,4,bangbangcontroller,2020-04-04 14:30:34,https://www.reddit.com/r/datasets/comments/fuumgc/turkey_coronavirus_cases_by_cities/,0,datasets
f3ugb1,College Tuition data from 1960s to 2010s by college,"Hi,

I've been doing some research and I have a dataset that shows all institute average tuition from the 1960s to roughly today, but not broken down by college. The datasets that *are* broken down by college only start from 1990s to roughly today.

Anybody have any thoughts on where I can find both?",42,11,Dear-Albatross,2020-02-14 16:19:06,https://www.reddit.com/r/datasets/comments/f3ugb1/college_tuition_data_from_1960s_to_2010s_by/,0,datasets
e5jb7c,Github developer social network of web and machine learning developers,"A large social network of GitHub developers which was collected from the public API in June 2019. Nodes are developers who have starred at least  10 repositories and edges are mutual follower relationships between them. The vertex features are extracted based on the location,  repositories starred, employer and e-mail address. The task related to the graph is binary node classification - one has to predict whether the  GitHub user is a web or a machine learning developer. This target feature was derived from the job title of each user.

Dataset:

[https://github.com/benedekrozemberczki/datasets#github-social-network](https://github.com/benedekrozemberczki/datasets#github-social-network)

Related project:

[https://github.com/benedekrozemberczki/MUSAE](https://github.com/benedekrozemberczki/MUSAE)",41,6,benitorosenberg,2019-12-03 16:12:32,https://www.reddit.com/r/datasets/comments/e5jb7c/github_developer_social_network_of_web_and/,0,datasets
d1mjuf,"I'm not sure if you are aware, but you can get some pretty interesting datasets for free from Quandl. There is an Excel add-on too.",,40,7,OculoDoc,2019-09-09 05:22:45,https://www.reddit.com/r/datasets/comments/d1mjuf/im_not_sure_if_you_are_aware_but_you_can_get_some/,0,datasets
brhx2h,New York state may (finally) be releasing police data,"I really don't understand why North Carolina has been releasing traffic stop data since 2000 and New York state is only finally getting around to it. Here are two links on the topic:

&#x200B;

overview:

[https://www.changethenypd.org/PoliceSTATAct](https://www.changethenypd.org/PoliceSTATAct)

&#x200B;

NY Senate bill:

[https://www.nysenate.gov/legislation/bills/2019/s1830](https://www.nysenate.gov/legislation/bills/2019/s1830)",43,8,Rt17_2019,2019-05-22 00:12:39,https://www.reddit.com/r/datasets/comments/brhx2h/new_york_state_may_finally_be_releasing_police/,0,datasets
bd279x,Crypto15: a dataset of snapshots taken every 15 minutes (for more than 1 year) of the status of 9 cryptocurrencies,"I just released a dataset (in SQLite3 + tensorflow-datasets format) of snapshots captured every 15 minutes of the status of 9 cryptocurrencies.

The snapshots have been captured from  2017-10-26 14:34:10 to 2019-02-25 09:50:02; with an average snapshot captured per day of 95. 

The currencies monitored are: ""BTC"", ""XRP"", ""ETH"", ""LTC"", ""XMR"", ""MIOTA"", ""ZEC"", ""EOS"", and ""ETC"".

URL: https://github.com/galeone/crypto15

The data has been gathered by the opensource algorithmic trading daemon I wrote (still WIP) you can find linked in the dataset page.",45,7,pgaleone,2019-04-14 12:20:15,https://www.reddit.com/r/datasets/comments/bd279x/crypto15_a_dataset_of_snapshots_taken_every_15/,0,datasets
9itdyv,"Microsoft releases automated machine learning, a new capability of Azure Machine Learning--automatic data transformation, model selection, and hyperparameter tuning for AI development",,41,2,myinnerbanjo,2018-09-25 16:00:06,https://blogs.microsoft.com/ai/automated-ai-development/,0,datasets
7w5wo8,"Dataset containing Valid Addresses from all ~20,000 Zipcodes in the US",,42,6,Theriley106,2018-02-08 16:16:53,https://www.kaggle.com/theriley106/valid-addresses-by-us-zip-code,0,datasets
6z054s,people of tinder,"The [""people of tinder"" dataset](https://www.kaggle.com/scolianni/people-of-tinder) has been removed from kaggle and I can not find a download link somewhere else. Does anybody still have it and could upload it?",40,5,throwaway5826152,2017-09-09 05:50:08,https://www.reddit.com/r/datasets/comments/6z054s/people_of_tinder/,0,datasets
5zzpli,Previous 2 years of White House budgets available as CSVs on the official White House Github repo,,43,0,danwin,2017-03-17 19:26:41,https://github.com/WhiteHouse/budgetdata/tree/2017,0,datasets
5e01mh,56 Major Speeches by Donald Trump (June 2015 - November 2016),,42,5,None,2016-11-20 20:47:39,https://github.com/PedramNavid/trump_speeches,0,datasets
370pwl,"Texas provides a data dump of their sex offender registry. However, to download it, you have to register as a sex-offender-registry-downloader, and your name will be posted",,43,6,danwin,2015-05-23 19:54:48,https://records.txdps.state.tx.us/SexOffender/PublicSite/Application/Export/History.aspx,0,datasets
vzr6wa,NASA datasets and what to do with them,"Hey! I'm new to this sub and I was looking for some data scientists who can tell me what's possible to do with NASA public datasets coming from their space telescopes, such as Kepler, Hubble, and now the new and incredible James Webb.

My question is: what can be achieved with it? Is it actually possible to discover new things using these public datasets? Like what?

I'm a data scientist myself but I'm not even at a junior level, so I'd love if someone linked me some materials about this topic so I can learn and hopefully play a role in discovering new and exciting things from my home.",37,9,Fada_Cosmica,2022-07-15 15:17:25,https://www.reddit.com/r/datasets/comments/vzr6wa/nasa_datasets_and_what_to_do_with_them/,0,datasets
txr7qi,"Reddit Place 2022 websites, images and datasets",,43,5,zewo_,2022-04-06 17:18:25,/r/place/comments/txr57v/place_2022_images_datasets/,0,datasets
sqx538,NFL player/team statistics datasheets galore,"Reddit has been a great place for me to find great data in the past so I made a bot that scraped NFL regular season player/team stats from 2016-2021. This is just my contribution to paying it forward. There are 180 datasheets in both CSV and JSON format. It is hosted on both GitHub and Kaggle for ease of use. I hope many people have fun using all of this data! Links below...  
[https://github.com/edsonjaramillo/nfl-stats](https://github.com/edsonjaramillo/nfl-stats)  
[https://www.kaggle.com/edsonjaramillo/nflstats](https://www.kaggle.com/edsonjaramillo/nflstats)",43,7,GucciButtcheeks,2022-02-12 17:55:48,https://www.reddit.com/r/datasets/comments/sqx538/nfl_playerteam_statistics_datasheets_galore/,0,datasets
rr71t4,What are the best tools for scrapping?,Just wanted to know what are the best tools available in the market for web scraping including both the free and the paid ones.,44,14,Effect_Exotic,2021-12-29 12:25:31,https://www.reddit.com/r/datasets/comments/rr71t4/what_are_the_best_tools_for_scrapping/,0,datasets
nu45o7,Searching for unclean datasets for my project that I can clean and use for analysis,"I am searching for untidy datasets that I can clean and then I can perform some analysis on the cleaned dataset.

It would be great if you can help me find the same.

Thanks in advance",39,14,forambarot04,2021-06-07 04:46:06,https://www.reddit.com/r/datasets/comments/nu45o7/searching_for_unclean_datasets_for_my_project/,0,datasets
m1bxkq,Million Song Dataset on AWS is deprecated,"I'm looking for the entire million song dataset. [https://aws.amazon.com/cn/datasets/million-song-dataset/](https://aws.amazon.com/cn/datasets/million-song-dataset/) leads me to AWS, but I cannot find matching results for the snapshot-id ""snap-5178cf30"".

The snapshot seems to be deprecated. Anyone has the full dataset can share with me? Thanks a lot! I would pay him DogeCoin if anyone shares with me.",40,3,voczkee,2021-03-09 17:48:45,https://www.reddit.com/r/datasets/comments/m1bxkq/million_song_dataset_on_aws_is_deprecated/,0,datasets
la6zuq,Massive multi-turn conversational dataset based on cleaned discord data," This is a long-context, anonymized, clean, multi-turn and single-turn conversational dataset based on discord data scraped from a large variety of severs, big and small.

The raw data for this version contained 51,826,268 messages  
5103788 (regex) + 696161 (toxic)/51826268, or 0.11% of the messages were removed  
**The dataset's final size is 46,026,319 messages across 456810 conversations**, which is reduced from 33.06 GB of raw json data to 968.87 MB

[https://www.kaggle.com/jef1056/discord-data](https://www.kaggle.com/jef1056/discord-data)",38,4,QTE1056,2021-02-01 16:21:33,https://www.reddit.com/r/datasets/comments/la6zuq/massive_multiturn_conversational_dataset_based_on/,0,datasets
knsfl2,"I wanna do a data science project that takes factors like your parents' medical history, environmental factors, and predicts their chance to develop a certain disease-what datasets can I use?",I explained in the title.,41,9,noinspirationwhatsoe,2020-12-31 15:57:34,https://www.reddit.com/r/datasets/comments/knsfl2/i_wanna_do_a_data_science_project_that_takes/,0,datasets
jd57ir,1.45 million course reviews on Coursera,"[https://www.kaggle.com/imuhammad/course-reviews-on-coursera](https://www.kaggle.com/imuhammad/course-reviews-on-coursera)

&#x200B;

  \[self-promotion\]",41,0,mcnakhaee,2020-10-17 23:07:32,https://www.reddit.com/r/datasets/comments/jd57ir/145_million_course_reviews_on_coursera/,0,datasets
j2zghx,[Self Promotion] 20k Tripadvisor Hotel Reviews,"Hello frens,  


I made available a dataset of 20k crawled Tripadvisor reviews [here](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews).  
It is essentially an NLP dataset that allows topic modeling, sentiment extraction and review rating regression  


>The original authors are:  
Alam, M. H., Ryu, W.-J., Lee, S., 2016. Joint multi-grain topic sentiment: modeling semantic aspects for online reviews. Information Sciences 339, 206–223. 

  
Cheers!",40,1,larxel,2020-10-01 01:49:14,https://www.reddit.com/r/datasets/comments/j2zghx/self_promotion_20k_tripadvisor_hotel_reviews/,0,datasets
g44dbm,What are your favorite platforms to share/find datasets.,"Hi,
I was wondering if there is a platform where you can share datasets. Because at the moment, I always download then from different sites and I need a lot of time to search for them.",43,18,levelupdigital,2020-04-19 08:42:59,https://www.reddit.com/r/datasets/comments/g44dbm/what_are_your_favorite_platforms_to_sharefind/,0,datasets
g0ubae,"Film and TV: 208,000 critic reviews and 10.7 million user reviews, full text with scores",,40,8,snappcrack,2020-04-13 23:34:18,http://components.one/datasets/film-reviews-208000-critic-reviews-and-10-7-million-user-reviews/,0,datasets
fhb2cb,"Increase your text dataset size using ""Back Translation""",,40,4,amitness,2020-03-12 04:54:30,https://amitness.com/2020/02/back-translation-in-google-sheets/,0,datasets
eue85r,[OC] A dataset of the 500 most upvoted Reddit posts of all time,,41,3,IMakeInfantsCry,2020-01-26 22:33:40,https://www.kaggle.com/nossair/top-500-reddit-posts-of-all-time,0,datasets
ejbojz,Synesthesia survey (What colour is each month to you?),"Synesthesia. What is synesthesia? According to google, ""Synesthesia is a condition in which one sense (for example, hearing) is simultaneously perceived as if by one or more additional senses such as sight. Another form of synesthesia joins objects such as letters, shapes, numbers or people's names with a sensory perception such as smell, color or flavor.""

One of the types of synesthesia, one of the most common, is Grapheme-Colour synesthesia, where, as defined earlier, is where people associate things such as numbers, dates, letters, and many more, with colours. For instance, to me, the number five is a very bright orange, and the month of March is a deep green. This form is NOT only for people with synesthesia. You can submit to it whether you have synesthesia or not, just please submit to it seriously, with what you feel represents it best.  
Other things to note:  
\-Submitting for the months is mandatory, the weekdays and the numbers are not.  
\-The form will close on January 18   
\-My goal is 200 submissions. If anyone could somehow give this survey to others that would be greatly appreciated!  
[https://forms.gle/hYFqECH9EGBRPMd36](https://forms.gle/hYFqECH9EGBRPMd36)",42,11,Brady-Forrest,2020-01-03 06:43:41,https://www.reddit.com/r/datasets/comments/ejbojz/synesthesia_survey_what_colour_is_each_month_to/,0,datasets
bxx9wb,VisualData: A Search Engine for Computer Vision Datasets,,42,3,Yuqing7,2019-06-07 17:23:45,https://medium.com/syncedreview/visualdata-a-search-engine-for-computer-vision-datasets-f6126b69f341?postPublishedType=initial,0,datasets
apdpzz,"20,783 Pitchfork reviews, Jan. 5, 1999 - Jan. 11, 2019",,43,6,snappcrack,2019-02-11 07:05:29,https://components.one/datasets/#pitchfork-reviews,0,datasets
9y988c,"35,000 US students play ""Never Have I Ever""",,42,10,cavedave,2018-11-18 20:03:52,http://blog.collegepulse.com/2018/11/pulse-plays-never-have-i-ever.html,0,datasets
8aqno0,"In 83 Million Eviction Records, a Sweeping and Intimate New Look at Housing in America",,40,5,cavedave,2018-04-08 15:33:03,https://www.nytimes.com/interactive/2018/04/07/upshot/millions-of-eviction-records-a-sweeping-new-look-at-housing-in-america.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news,0,datasets
7r0uv9,The NYC Marriage Index: searchable and downloadable database of marriage licenses from 1950-1995 (3.1M records),,41,4,danwin,2018-01-17 13:12:14,https://www.nycmarriageindex.com/,0,datasets
7ehnpp,"10,000 food products and their ingredients",,42,4,datadotworld,2017-11-21 13:00:25,https://data.world/datafiniti/food-ingredient-lists,0,datasets
6vw54h,speech commands dataset released by google research for deep learning speech recognitioin,,40,0,ashwinids,2017-08-25 03:57:44,https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html,0,datasets
6kiryl,Data On Drug Use Is Disappearing Just When We Need It Most,,44,1,cavedave,2017-06-30 20:58:15,https://fivethirtyeight.com/features/data-on-drug-use-is-disappearing-just-when-we-need-it-most/,0,datasets
6cw889,The Library of Congress Just Released a Huge Dataset for Free,,41,1,cavedave,2017-05-23 17:41:21,http://fortune.com/2017/05/17/library-of-congress-free-record-release/,0,datasets
50kucg,"Settlers of Catan dataset: starting positions, roll distributions, postgame statistics of 50 4-player games",,41,2,fasnoosh,2016-09-01 01:53:40,https://www.kaggle.com/lumins/settlers-of-catan-games,0,datasets
3k3mr9,"Reddit data for ~900,000 subreddits (includes both public and private subreddits)","Data includes subreddit creation date, number of subscribers, subreddit title and descriptions, public/private, etc.

http://files.pushshift.io/reddit/subreddits/subreddit_data.bz2

**sha256sum** 75d68a71f7a8b67f0b5948ccd12d12af7c4d313b3c2c2e91b600246075c8ffc9

This data was captured within the past 24 hours.  This is the complete list of subreddits minus a couple that weren't returned by the Reddit API (Status 500).  I'm assuming they may be ones that were once completely removed from the system such as the subreddit /r/jailbait, etc.  ",40,20,Stuck_In_the_Matrix,2015-09-08 14:56:18,https://www.reddit.com/r/datasets/comments/3k3mr9/reddit_data_for_900000_subreddits_includes_both/,0,datasets
1za14y,"""Making Sense of Data"", a new free course by Google",,43,4,habash1986,2014-03-01 17:48:33,https://datasense.withgoogle.com/preview,0,datasets
1b6doe,Yelp Dataset ,,44,2,somnophobiac,2013-03-28 14:05:28,http://www.yelp.com/dataset_challenge/,0,datasets
16pfj0,"Database of Cat Pictures. (No, really, and not /r/pics).",,41,8,notheory,2013-01-16 20:55:19,http://137.189.35.203/WebUI/CatDatabase/catData.html,0,datasets
126vth5,Magic: the Gathering deck lists scraped from MtgTop8,"# Magic: the Gathering deck dataset

I scraped deck lists from a competitive deck sharing platform called MtgTop8 for a project I'm working on.

Decks are separated by format in the following:

\- standard

\- modern

\- pioneer

\- historic

\- explorer

\- pauper

\- legacy

\- vintage

&#x200B;

They're stored as Apache feather files which can be easily converted to either pickle or csv files.

Feel free to use them for whatever purpose.

[Here's the link](https://github.com/ppapuli102/MTG-Land-Recommender/tree/main/data/decklists)",40,4,ArmyOfCorgis,2023-03-30 18:51:10,https://www.reddit.com/r/datasets/comments/126vth5/magic_the_gathering_deck_lists_scraped_from/,0,datasets
10mlp5h,Kaggle competition to train a text-based geolocation model with a dataset of 500k annotated texts from across the world 🌎📍,"You can check out the Kaggle competition: [https://www.kaggle.com/competitions/text-based-geolocation/](https://www.kaggle.com/competitions/text-based-geolocation/)

Also, our Github is here if you need it: [https://github.com/1712n/yachay-public/tree/master/conf\_geotagging\_model](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model)",38,3,yachay_ai,2023-01-27 14:08:17,https://www.reddit.com/r/datasets/comments/10mlp5h/kaggle_competition_to_train_a_textbased/,0,datasets
zmjk9h,The Countries that Pay the Most and Least for LEGO,,40,2,cavedave,2022-12-15 12:09:37,https://thetoyzone.com/the-countries-that-pay-the-most-and-least-for-lego?utm_source=join1440&utm_medium=email&utm_placement=newsletter,0,datasets
z6enau,Ideas for PPP Loan Dataset for visualization or analysis?,"I know there's got to be a valuable and great visualization or analysis of the PPP loan data.  This is freely available and IMHO is going to be seen as one of the greatest transfers of wealth in modern history.  That aside, the fields are: loanamount, business name, address, city , state, zip, naicscode, businesstype, jobsreported, dateapproved, lender, CD, Lat, Lon (a few others but not relevant or lacking data).

Of course we could break the loan amounts down from the state to the county to the street level, but doesn't seem that interesting.   I was thinking along the lines of extrapolating the billions of dollars given away translates into potentially trillions of spending power?  

Thoughts appreciated.",41,6,methods21,2022-11-27 22:24:57,https://www.reddit.com/r/datasets/comments/z6enau/ideas_for_ppp_loan_dataset_for_visualization_or/,0,datasets
z1ug3b,Let's collect unusual NLP datasets here!,"I am interested in ""unusual"" text datasets. 

For instance: 

- Twitter Mental Disorder Tweets and Musics Dataset https://www.kaggle.com/datasets/rrmartin/twitter-mental-disorder-tweets-and-musics

or 
- All of the Enron emails: https://www.kaggle.com/datasets/wcukierski/enron-email-dataset


Do you have similarly ""niche"" or unusual datasets? Let's collect them in the comments!",37,3,theologi,2022-11-22 14:06:09,https://www.reddit.com/r/datasets/comments/z1ug3b/lets_collect_unusual_nlp_datasets_here/,0,datasets
ywshka,[P] MARVEL SNAP dataset (decks and cards) on kaggle,"Hello my card game lovers,

Last week, I entered into a profound addiction to a game called marvel snap, and for the last two days, I have encountered a big wall in the game; I need to step up my game to build more efficient decks.

📚People around me advise me to look at articles and videos online, and I prefer to go with the good old data way to collect data from online communities. Marvel snap zone is one of these communities with thousands of decks built by the community, and I started to compile them in a kaggle dataset.

🛠So here we are for all of my data people in the same situation; you have an excuse to play the game, and test/improve your data/ml skills simultaneously, and yes, you are welcome.

dataset: https://www.kaggle.com/datasets/jeanmidev/marvel-snap-decks-and-cards tutorial + recsys : https://www.kaggle.com/code/jeanmidev/tutorial-marvel-snap-dataset",41,1,jeanmidev,2022-11-16 12:49:58,https://www.reddit.com/r/datasets/comments/ywshka/p_marvel_snap_dataset_decks_and_cards_on_kaggle/,0,datasets
tjchvc,platform to easily enrich your datasets," Hi everyone!

I’m Erik, Co-Founder of [subsets.io](https://subsets.io/) Subsets.io is a platform where you can upload datasets, and get matched with relevant external datasets which can be merged into your dataset with one click.

Our goal is to make it easier to pull in relevant external data. No more dealing with APIs and their rate limits, pagination, etc.

Check it out on [https://subsets.io](https://subsets.io/). Let me know what you think. :)",42,7,hodeeee,2022-03-21 14:01:19,https://www.reddit.com/r/datasets/comments/tjchvc/platform_to_easily_enrich_your_datasets/,0,datasets
repci1,Suicide Detection Dataset for Natural language Processing!,"Hey!

I am looking for a Suicide detection dataset.. Anybidy has any idea, on where can I get that ?

I need it to perform experimentations for my college disseration, as I have always worked on NLP. If anyone has any idea, where I can find one it'd be helpful. Also, I can't use API and then have it annotated, as I tried before but they won't accept it.",39,11,done-for-life,2021-12-12 13:47:36,https://www.reddit.com/r/datasets/comments/repci1/suicide_detection_dataset_for_natural_language/,0,datasets
pa9k6c,American Kennel Club Dog Breeds by Size dataset: just added to github,"I'm just finishing up a project this summer and put together this dog breed dataset. Doesn't exist anywhere online and AKC's site makes it hard to extract the info so I figured I'd make it public.

Essentially it just categorizes all the AKC dog breeds into 5 breed size categories (xs, s, m, l, xl). I also added the average weight of each breed into the file.

Hope someone finds it useful so that they don't have to rip their hair out trying to find this info.

Here's the link:

[https://github.com/MeganSorenson/American-Kennel-Club-Breeds-by-Size-Dataset](https://github.com/MeganSorenson/American-Kennel-Club-Breeds-by-Size-Dataset)",41,7,margarita4uz,2021-08-23 21:43:57,https://www.reddit.com/r/datasets/comments/pa9k6c/american_kennel_club_dog_breeds_by_size_dataset/,0,datasets
p5ilj1,Dataset of every company in S&P 500 per year 1980 to today.,"Hi everyone. I'm working on a project that requires looking at the performance of the S&P 500 through time. I'm looking for a dataset or even just a list of every company in the S&P 500 per year from 1980 to now. It sounds like something that would pop up in a pretty basic google search but it's been incredibly hard to find. Any help is greatly appreciated, thank you!",41,5,americanegus,2021-08-16 15:40:41,https://www.reddit.com/r/datasets/comments/p5ilj1/dataset_of_every_company_in_sp_500_per_year_1980/,0,datasets
n5igd4,Enron Spam Dataset - over 33k Emails preprocessed in a single sv-file,"[Link to dataset](https://github.com/MWiechmann/enron_spam_data)

The Enron-Spam dataset is a fantastic ressource collected by V. Metsis, I. Androutsopoulos and G. Paliouras. The dataset contains a total of 17.171 spam and 16.545 non-spam (""ham"") e-mail messages (33.716 e-mails total). The original dataset and documentation can be found [here](http://www2.aueb.gr/users/ion/data/enron-spam/readme.txt).

However, the original datasets is recorded in such a way, that every single mail is in a seperate txt-file, distributed over several directories. This can make reading in the data a bit cumbersome, especially for beginners. Since the data set is such an excellent ressource, I wanted to create a offer a single download of the data through a simple csv-file.",41,5,TravellingRobot,2021-05-05 15:22:25,https://www.reddit.com/r/datasets/comments/n5igd4/enron_spam_dataset_over_33k_emails_preprocessed/,0,datasets
m6erjl,Time-Series Forecasting with Facebook Prophet and OmniSci,,37,4,_paige_joseph,2021-03-16 17:43:49,http://omnisci.link/tcgia0,0,datasets
m3od66,Tracking the vaccination progress with Snowflake and Starschema,,39,2,fhoffa,2021-03-12 18:55:36,https://medium.com/snowflake/tracking-the-vaccination-progress-with-snowflake-and-starschema-bfc68a45cb70,0,datasets
lj487y,[REQUEST] Need some datasets to perform linear regression analysis,"Hey guys, I'm new to data science, and I'm looking for some datasets to perform linear regression analysis using R, I'd appreciate some cool datasets recommendations from you so that i can practice a little bit and play around with em as long as the data is bivariate. I'm having trouble finding a good dataset or maybe i'm just a newbie :'). Thank you",41,16,Elliobu,2021-02-13 16:56:36,https://www.reddit.com/r/datasets/comments/lj487y/request_need_some_datasets_to_perform_linear/,0,datasets
kki55g,Nintendo games dataset,,40,0,msoedov,2020-12-26 12:51:39,https://www.kaggle.com/msoedov/nintendo-games,0,datasets
k980za,Covid Vaccine Data,"FDA Briefing Document Pfizer-BioNTech COVID-19 Vaccine [https://www.fda.gov/media/144245/download](https://www.fda.gov/media/144245/download) 

Oxford Vaccine ChAdOx1 nCoV-19 vaccine (AZD1222)  [https://www.thelancet.com/lancet/article/s0140-6736(20)32661-1](https://www.thelancet.com/lancet/article/s0140-6736(20)32661-1)",39,1,cavedave,2020-12-08 17:02:23,https://www.reddit.com/r/datasets/comments/k980za/covid_vaccine_data/,0,datasets
jn9gn4,Google Playstore Dataset,"Google Playstore Dataset now updated with 600k+ Applications and 23 attributes. Available in Kaggle:

[https://www.kaggle.com/gauthamp10/google-playstore-apps](https://www.kaggle.com/gauthamp10/google-playstore-apps)",40,1,ClassicLaw9284,2020-11-03 12:32:41,https://www.reddit.com/r/datasets/comments/jn9gn4/google_playstore_dataset/,0,datasets
izm0s0,"I’m looking for data on abandoned property in America, especially industrial sites, malls and retail centers, or other buildings like hospitals, schools, hotels, etc. Homes and other miscellaneous places are welcomed too.","Grad student here, embarking on phase one of thesis research. I’ve learned abandoned properties aren’t well documented so I’m having a hard time finding cumulative information on the subject. I’m also open to related datasets or other suggestions, resources, etc. 

Thank you!

(First time posting here, sorry if I used the wrong tag)",39,7,lilnugs14,2020-09-25 15:54:37,https://www.reddit.com/r/datasets/comments/izm0s0/im_looking_for_data_on_abandoned_property_in/,0,datasets
gh7l6n,60 3D CT lung scans dataset for Lung Segmentation,,36,5,beyonsense,2020-05-10 19:18:07,https://wiki.cancerimagingarchive.net/display/Public/Lung+CT+Segmentation+Challenge+2017,0,datasets
fvvpv8,It there a data set about 'real' and fake news around covid-19 ?,,40,22,PlatoTheSloth,2020-04-06 09:30:33,https://www.reddit.com/r/datasets/comments/fvvpv8/it_there_a_data_set_about_real_and_fake_news/,0,datasets
fhn7yo,Airbnb historical data,Looking for an API of historical Airbnb where you can specify the city. Please share any ideas!,44,3,None,2020-03-12 20:42:23,https://www.reddit.com/r/datasets/comments/fhn7yo/airbnb_historical_data/,0,datasets
f9tide,"Hey everyone! For any python & pandas users out there, here's a free tool to visualize your dataframes","\- Here's a demo of all the functionality: [demo video](https://youtu.be/yAKClxCsrDQ)  
\- Github is located [here](https://github.com/man-group/dtale)  
\- Live interactive demo [here](http://alphatechadmin.pythonanywhere.com/)

This is D-Tale ""details of your data"" is a free visualizer for pandas data structures.  It is currently supported within the following:

* python terminals
* jupyter notebooks
* hosted notebooks like kaggle & google colab
* within R terminals through the use of reticulate

Hope it helps spawn some ideas around visualizations for your data sets!  In the end, building out your own visualizations is still king, but this gives you a quick way to possibly see if your data might produce something beautiful before you spend time fully building it out.",41,2,aschonfe,2020-02-26 13:51:35,https://www.reddit.com/r/datasets/comments/f9tide/hey_everyone_for_any_python_pandas_users_out/,0,datasets
f678iw,Cryptocurrency Exchange: Binance Full History (17GB),,41,3,jorijnsmit,2020-02-19 07:22:59,https://www.kaggle.com/jorijnsmit/binance-full-history,0,datasets
esns0v,Does anyone have this chart's underlying case-onset data for SARS back in 2003?,,39,5,mott_the_tuple,2020-01-23 03:55:34,http://sarsreference.com/sarsref/images/hk.gif,0,datasets
d92kkj,"code we used to download, clean, analyse and plot hundreds of ice concentration images",,37,2,cavedave,2019-09-25 12:28:15,https://github.com/johnburnmurdoch/R_resources/blob/master/Arctic%20ice%20concentration%20anomalies/arctic_sea_ice_concentration.R,0,datasets
d6upkm,List of bird sounds datasets,"Hello guys! Recently, I started to work on a project connected to bird song recognition. I have managed to find quite a few datasets and gathered them, along with other resources, in one place. Feel free to check my repo - [https://github.com/AgaMiko/Bird-recognition-review](https://github.com/AgaMiko/Bird-recognition-review) 

Maybe someone will find it useful. Also, feel free to pull a request if you want to add something, or just simply write it here - I will try to add it to the repo as fast as possbile.",37,1,Chitoyo,2019-09-20 13:11:46,https://www.reddit.com/r/datasets/comments/d6upkm/list_of_bird_sounds_datasets/,0,datasets
crzdtw,"Data, including geodata, for UK parliamentary constituencies",,40,1,cavedave,2019-08-18 09:59:22,https://github.com/alasdairrae/wpc,0,datasets
ag5ded,"A list of all offensive words and slurs (racial, homophobic, sexist, etc.)","I found [this](https://github.com/amoudgl/short-jokes-dataset/blob/master/shortjokes.csv) dataset, 231,000 short jokes. But there's so many racist ones, if you think of any racial slur you can think of it's in there. ""N-word"" is in there 95 times. I could delete them all, but I'd need another dataset full of racial slurs to filter by. Maybe even all the common misspellings of racial slurs as well. There must be a dataset like this out there, how else would Club Penguin stop me from expressing myself in sixth grade.",43,14,CollectableRat,2019-01-15 05:52:53,https://www.reddit.com/r/datasets/comments/ag5ded/a_list_of_all_offensive_words_and_slurs_racial/,0,datasets
9dkgxm,[REQUEST]: Articles by US Vice President Mike Pence,"I am requesting articles (particularly op-eds) written by US Vice President Michael Pence for use in a personal text mining project.

Some context: in today's news, there has been speculation (and speculation only) that Mike Pence wrote an article in the NY Times ([see the article here](https://www.nytimes.com/2018/09/05/opinion/trump-white-house-anonymous-resistance.html?action=click&module=Opinion&pgtype=Homepage)) criticizing major aspects of the Trump presidency. The only thing that is known about the article is that it was written by a senior official in the Trump administration. I want to test this speculation by using statistical clustering (kmeans, decision tree) to see if there is any correlation in writing styles between the recent op-ed and what he has written in the past.

**What I do not want:** speeches orated by Mike Pence that were not written by Mike Pence. That would give me false results.

Thank you in advance!",38,14,None,2018-09-06 16:47:40,https://www.reddit.com/r/datasets/comments/9dkgxm/request_articles_by_us_vice_president_mike_pence/,0,datasets
8wt69o,Walt Disney Animation Studios Data Sets,,37,1,DaveLak,2018-07-07 13:30:29,https://www.disneyanimation.com/technology/datasets,0,datasets
8g7wv9,Artificial Intelligence Opens the Vatican Secret Archives,,39,1,cavedave,2018-05-01 12:02:41,https://www.theatlantic.com/technology/archive/2018/04/vatican-secret-archives-artificial-intelligence/559205/,0,datasets
7skscc,"Network Dynamics Datasets: 38,000 Networks, 2.5 Million Graphs (~1 TB of compressed data) constructed form Reddit, Microsoft Academic, Chess Games (FICS), Bitcoin, and WikiTree",,38,5,fire_u,2018-01-24 04:49:40,https://dynamics.cs.washington.edu,0,datasets
7cuem8,Core Data: Essential Datasets for Data Wranglers and Data Scientists,,42,5,rufuspollock,2017-11-14 09:00:20,http://datahub.io/blog/core-data-essential-datasets-for-data-wranglers-and-data-scientists,0,datasets
6ow8j3,Google doc of the personal financial disclosures of Trump staffers,,39,1,cavedave,2017-07-22 17:31:40,http://boingboing.net/2017/07/20/this-massive-searchable-google.html,0,datasets
65o7py,Updated reddit comment dataset as torrents,"Hi, I have updated the reddit comment dataset to include all comment files available on files.pushshift.io. (as always, thanks to /r/Stuck_in_the_Matrix for collecting the data in the first place!)

Since I guess many people do not want to download all 300+ GByte again and again whenever a new chunk of data is available, I have split them into one torrent per year. This also makes it easier if one broken file slips by again.

* [2005](https://cinnamon.dewarim.com/torrents/reddit-2005.torrent) (just 2005-12, 116 KB)
* [2006](https://cinnamon.dewarim.com/torrents/reddit-2006.torrent) (45 MB)
* [2007](https://cinnamon.dewarim.com/torrents/reddit-2007.torrent) (212 MB)
* [2008](https://cinnamon.dewarim.com/torrents/reddit-2008.torrent) (618 MB)
* [2009](https://cinnamon.dewarim.com/torrents/reddit-2009.torrent) (1.72 GB)
* [2010](https://cinnamon.dewarim.com/torrents/reddit-2010.torrent) (4.4 GB)
* [2011](https://cinnamon.dewarim.com/torrents/reddit-2011.torrent) (11 GB)
* [2012](https://cinnamon.dewarim.com/torrents/reddit-2012.torrent) (24 GB)
* [2013](https://cinnamon.dewarim.com/torrents/reddit-2013.torrent) (38 GB)
* [2014](https://cinnamon.dewarim.com/torrents/reddit-2014.torrent) (53 GB)
* [2015](https://cinnamon.dewarim.com/torrents/reddit-2015.torrent) (68 GB)
* [2016](https://cinnamon.dewarim.com/torrents/reddit-2016.torrent) (81 GB)
* [2017](https://cinnamon.dewarim.com/torrents/reddit-2017.torrent) (up to 2017-03, 23 GB)

Please make sure to compare checksums with http://files.pushshift.io/reddit/comments/sha256sums

Format is JSON per line, compressed with bzip2. 

Some scripts and tools for handling the data are available at [Github.com: reddit-data-tools](https://github.com/dewarim/reddit-data-tools). I am working on putting up the sentiment analysis data  once it's been computed again.

Edit: added submissions:

* 2006-2007 is not complete yet.
* [2008](https://cinnamon.dewarim.com/torrents/reddit-submission-2008.torrent)
* [2009](https://cinnamon.dewarim.com/torrents/reddit-submission-2009.torrent)
* [2010](https://cinnamon.dewarim.com/torrents/reddit-submission-2010.torrent)
* [2011](https://cinnamon.dewarim.com/torrents/reddit-submission-2011.torrent)
* [2012](https://cinnamon.dewarim.com/torrents/reddit-submission-2012.torrent)
* [2013](https://cinnamon.dewarim.com/torrents/reddit-submission-2013.torrent)
* [2014](https://cinnamon.dewarim.com/torrents/reddit-submission-2014.torrent)
* [2015](https://cinnamon.dewarim.com/torrents/reddit-submission-2015.torrent)
* [2016](https://cinnamon.dewarim.com/torrents/reddit-submission-2016.torrent)
* [2017](https://cinnamon.dewarim.com/torrents/reddit-submission-2017.torrent) (up to 2017-03)
",41,14,Dewarim,2017-04-16 08:41:41,https://www.reddit.com/r/datasets/comments/65o7py/updated_reddit_comment_dataset_as_torrents/,0,datasets
4qzkl7,"Complete list of all Reddit subreddits including subscribers, description, creation date and other data.",,36,32,Stuck_In_the_Matrix,2016-07-02 23:24:13,http://files.pushshift.io/subreddits/,0,datasets
3mk1vg,"Real-time data is available including comments, submissions and links posted to reddit","**Endpoint**
http://stream.pushshift.io

There is a lot you can do with this.

This will return SSE styled events for all new comments and submissions (99.99% of the time in correct order -- still QA'ing this part).  

Please let me know if you notice any issues.  Eventually I will have parameters to only stream submissions or comments or specific subreddits.  The event type is either ""t1"" for comments, or ""t3"" for submissions, or ""links"" for any links posted to reddit.  I do a head request for each link posted to reddit and also get other information for youtube videos such as querying Google's API.

Event data is a JSON string.

**Parameters**:
-------------------------------------------------------

*If more than one parameter is specified, they are treated as OR operations.  Meaning that if you are filtering on the subreddit ""askreddit"" and also on the author ""automoderator"", you will get both in your stream.

**subreddit**: Include any submissions or comments with this subreddit in the stream.

**author**: Include any submissions or comments with this author.

**over_18**:  Restrict returned submissions (""t3"" events) to either NSFW(over_18=1) or non-NSFW (over_18=0)

**event**: Limit to only comments or subreddits.  Values are ""t1"" for comments and ""t3"" for submissions. (i.e. event=t3 to get only submissions)

**match**:  Does a regex on the body (comments) and title or self_text (submissions).  Limit in any way you want.  (i.e. match=subreddit"":""askreddit for only askreddit comments and submissions -- case insensitive).  If you search for ""star"", it will match start.

**start_id**:  If you lose your connection to the stream and want to reconnect at a specific id location, pass the start_id parameter and the stream will replay from that id until it goes real-time.  The stream buffers the last half hour of all reddit activity.  Use the last processed event id.

**previous**:  Get the last X events starting from the current event.  In other words, if previous=1000 is passed, the stream will give you the last 1,000 events in order and then resume in real-time mode.  Max value is 100,000.

**Examples**:
---------------------------------------------------------

     wget -qO- 'http://stream.pushshift.io/?subreddit=askreddit'

**Filter only comments or submissions from the subreddit askreddit**

    wget -qO- 'http://stream.pushshift.io/?author=""automoderator""

**Filter only comments or submissions from the author automoderator**

     wget -qO- 'http://stream.pushshift.io/?event=t3&match=imgur

**Show submissions that contain imgur anywhere in the JSON response**

    wget -qO- ""http://stream.pushshift.io/?event=t3&over_18=1""

**Get only NSFW submissions** *There is an underscore between over and 18.  It should be over_18*

    wget -qO- --header='Accept-Encoding: gzip,deflate' ""http://stream.pushshift.io/?event=t1&previous=10000"" | gzip -dc

**Stream the previous 10,000 events using compression (saves bandwidth) and then resume in real-time.**

**Notes**
-----------------------------------------------------------

The timeout is set to 600 seconds.  If you filter by something very esoteric, the stream could disconnect after 600 seconds.  Please be aware of that.  The timeout resets for any activity.

Also, there can only be one active stream per IP address.  If you need more streams, we can discuss solutions for you.",38,2,Stuck_In_the_Matrix,2015-09-27 07:40:41,https://www.reddit.com/r/datasets/comments/3mk1vg/realtime_data_is_available_including_comments/,0,datasets
1wkyho,We (UNICEF) just released our latest data about children's well-being. Thought it might interest some of you. Scroll to bottom of site for the goods.,,41,15,UNICEFdigital,2014-01-30 19:50:16,http://uni.cf/data,0,datasets
12u6gw1,"Open Public Domain Exercise Dataset in JSON format, over 800+ exercises & images with a browsable public searchable frontend [self promotion]","I started building another fitness related app and was looking for free/open source exercise datasets and imagery and I stumbled upon [exercises.json](https://github.com/wrkout/exercises.json) though it needed a bit of cleaning up & restructuring so I

1. Renamed/Restructured the JSON to be more usable
2. Added JSON Schema for validation
3. Added some useful Makefile build tasks to concatenate the JSON into one single file or for importing into PostgreSQL if needed
4. Added a browsable/searchable/frontend available at [https://yuhonas.github.io/free-exercise-db/](https://yuhonas.github.io/free-exercise-db/)

The repo is available at

[https://github.com/yuhonas/free-exercise-db](https://github.com/yuhonas/free-exercise-db)

All feedback welcome but above all else enjoy!",39,5,yuhonas,2023-04-21 15:11:50,https://www.reddit.com/r/datasets/comments/12u6gw1/open_public_domain_exercise_dataset_in_json/,0,datasets
12sz2ak,"A free, open source mock data stream generator for your next project",,40,3,tinybirdco,2023-04-20 13:40:30,https://www.tinybird.co/blog-posts/mockingbird-announcement-mock-data-generator,0,datasets
y32yf0,Beyond the trillion prices: pricing C-sections in America,,42,2,alecs-dolt,2022-10-13 16:23:25,https://www.reddit.com/r/datascience/comments/y32xmc/beyond_the_trillion_prices_pricing_csections_in/?,0,datasets
v5rnqv,[Dataset] US Supreme Court Oral Arguments,"I have created a dataset of the Supreme Court of the United States' oral arguments. This dataset takes the transcripts from oral arguments at the Supreme Court and breaks them down by line labeled by speaker and case number. These cases start from the 2017 term to the current term (2021).   


There are a few cases which have not been loaded due to unique errors with the files. I am working on adding them and have included a list on the Kaggle website.  


[https://www.kaggle.com/datasets/jameslabadorf/us-supreme-court-arguments-20172021](https://www.kaggle.com/datasets/jameslabadorf/us-supreme-court-arguments-20172021)",38,0,Squ3lchr,2022-06-06 01:32:17,https://www.reddit.com/r/datasets/comments/v5rnqv/dataset_us_supreme_court_oral_arguments/,0,datasets
ululat,How to analyze our hospitals prices dataset and find the most expensive hospitals (code in post),,39,6,alecs-dolt,2022-05-09 16:04:30,https://www.dolthub.com/blog/2022-05-06-the-most-expensive-hospitals/,0,datasets
s0ud6e,"[Self Promotion] Free Basketball database of leagues, teams, and players from around the world","Hi r/datasets,

CEO of DoltHub here. We just finished our basketball database bounty called SHAQ. Here's the data if anyone wants to use it:

[https://www.dolthub.com/repositories/dolthub/SHAQ](https://www.dolthub.com/repositories/dolthub/SHAQ)

And here's the write up on how it did:

[https://www.dolthub.com/blog/2022-01-10-shaq-bounty-retrospective/](https://www.dolthub.com/blog/2022-01-10-shaq-bounty-retrospective/)

We're on to our next data bounty, US Housing Prices, [https://www.dolthub.com/repositories/dolthub/us-housing-prices](https://www.dolthub.com/repositories/dolthub/us-housing-prices), if the idea of getting paid to build databases intrigues you.",38,3,timsehn,2022-01-10 20:58:20,https://www.reddit.com/r/datasets/comments/s0ud6e/self_promotion_free_basketball_database_of/,0,datasets
q1oq2y,[self-promotion] Board Games Dataset,"Hello frens,  


I made available on kaggle a dataset for 20k board games extracted from BoardGamesGeek website.  
It is available [here](https://www.kaggle.com/andrewmvd/board-games).  


All credits go to the author:

>Dilini Samarasinghe, July 5, 2021, ""BoardGameGeek Dataset on Board Games"", IEEE Dataport, doi: [https://dx.doi.org/10.21227/9g61-bs59](https://dx.doi.org/10.21227/9g61-bs59). 

&#x200B;

Cheers m8s",37,8,larxel,2021-10-05 06:06:17,https://www.reddit.com/r/datasets/comments/q1oq2y/selfpromotion_board_games_dataset/,0,datasets
pi6816,Does anyone know where I could find NFL datasets to use for fantasy football?,Title,38,18,Prestigious_Passion,2021-09-05 04:13:55,https://www.reddit.com/r/datasets/comments/pi6816/does_anyone_know_where_i_could_find_nfl_datasets/,0,datasets
p7ew6m,Can I publish a dataset I scraped and parsed from the internet?,"I scraped a handful of online blogs and parsed the HTML, extracting the useful data, to create a tabular dataset. Can I publish this structured dataset? I’m interested in any legal/ethical considerations.

I’ve tried to reach out to these online blogs to seek their permission, but they’ve been unresponsive.",36,14,__loplop,2021-08-19 13:31:25,https://www.reddit.com/r/datasets/comments/p7ew6m/can_i_publish_a_dataset_i_scraped_and_parsed_from/,0,datasets
no8cha,Curated social network datasets with summary statistics and background info,,41,0,Quantsel,2021-05-30 10:26:43,http://snap.stanford.edu/data/index.html,0,datasets
n1q3hc,Is there a PornHub search history dataset like Google Trends?,I read in a book about some out-of-the-box PornHub searches from different regions lol so I thought I should check it out but I am not able to find any such datasets.,38,16,shivang__,2021-04-30 09:15:16,https://www.reddit.com/r/datasets/comments/n1q3hc/is_there_a_pornhub_search_history_dataset_like/,0,datasets
ldcl5q,Data set which shows the relationship between an accused persons race and the likelihood that their mug shot will be used when reporting the crime. (USA),"Long shot I know, but even if it’s just for any particular news station it will work! Thanks!!",40,6,Bobello10000,2021-02-05 17:41:52,https://www.reddit.com/r/datasets/comments/ldcl5q/data_set_which_shows_the_relationship_between_an/,0,datasets
kv60fs,Anybody collected the tweets during the Capitol insurrection day?,Any sources that can be used for text mining for this event published?,36,12,verypsb,2021-01-11 16:26:20,https://www.reddit.com/r/datasets/comments/kv60fs/anybody_collected_the_tweets_during_the_capitol/,0,datasets
heaxg0,What are the most relevant data sources online for the training of Machine Learning models by industry?,"Hello Machine Learning Enthusiasts and Practitioners.

Let's pitch in together and create a list of **external data sources for the training of Machine Learning models**.

Take part in my anonymous survey to help me find the **most relevant data sources by industry** and their usage in Machine Learning:

[https://www.surveyhero.com/s/external\_data\_sources\_ML](https://www.surveyhero.com/s/external_data_sources_ML)

The research is part of my Master Thesis at the Berlin School of Economics and Law.

I will post the outcome on my Linkedin profile, the link to which you can find on the last page of the survey.

**Thank you for your time** (the survey takes roughly 10-15mins), I truly appreciate it.

Best wishes from Germany

Stuart",42,4,None,2020-06-23 08:37:48,https://www.reddit.com/r/datasets/comments/heaxg0/what_are_the_most_relevant_data_sources_online/,0,datasets
h0elhf,"Looking for US police shootings, by state","I’m looking for a dataset that encompasses police shootings (preferably by state) in the US in 2014, or more years.

Can’t seem to find any reliable source",39,16,logicallyzany,2020-06-10 16:42:51,https://www.reddit.com/r/datasets/comments/h0elhf/looking_for_us_police_shootings_by_state/,0,datasets
g9vcp4,US Supreme Court Transcripts,"Earlier this week, [u/EricW\_CS](https://www.reddit.com/user/EricW_CS/) posted about his [US Supreme Court transcript dataset](https://github.com/EricWiener/supreme-court-cases). 

I took the liberty of importing it into [Dolt](https://github.com/liquidata-inc/dolt) which is a SQL database with Git Semantics. It's on DoltHub for everyone to use:

https://www.dolthub.com/repositories/Liquidata/us-supreme-court-cases

I did a little bit of cleaning in the process or importing it. It's a very cool dataset to run SQL on. Famous quote by Sandra Day O'Connor:

```
timsehn$ dolt sql -q ""select * from transcripts where speaker='Sandra Day O\'Connor' and text like '%pornography%'""
+--------------------------------------------+-----------------------------------+----------------------------------------------------------------------+---------------------+----------+----------+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| case_name                                  | title                             | link                                                                 | speaker             | start    | stop     | duration | text                                                                                                                                                                                                                                                                                  |
+--------------------------------------------+-----------------------------------+----------------------------------------------------------------------+---------------------+----------+----------+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Ashcroft v. American Civil Liberties Union | Oral Argument - March 02, 2004    | https://apps.oyez.org/player/#/rehnquist10/oral_argument_audio/22093 | Sandra Day O'Connor | 136.198  | 154.982  | 18.78    | Mr. Olson, part of the problem is that the pornography laws that would apply to adult viewers don't seem to be enforced very well, the obscenity laws.                                                                                                                                |
| Ashcroft v. Free Speech Coalition          | Oral Argument - October 30, 2001  | https://apps.oyez.org/player/#/rehnquist10/oral_argument_audio/21372 | Sandra Day O'Connor | 483.178  | 492.276  | 9.1      | Mr. Clement, may I ask you a question again relating to the affirmative defenses or youthful adult pornography.                                                                                                                                                                       |
| Hunter v. Underwood                        | Oral Argument - February 26, 1985 | https://apps.oyez.org/player/#/burger8/oral_argument_audio/19281     | Sandra Day O'Connor | 1538.688 | 1559.452 | 20.76    | The Court of Appeals also indicated, I think in a footnote, that the statute was under-inclusive because sometimes that apparently it would be characterized at least by the Court of Appeals as crimes of moral turpitude are not included such as mailing pornography and so forth. |
| United States v. X-Citement Video, Inc.    | Oral Argument - October 05, 1994  | https://apps.oyez.org/player/#/rehnquist10/oral_argument_audio/20030 | Sandra Day O'Connor | 1643.659 | 1654.034 | 10.38    | Well, General Days, I thought we had already agreed that it doesn't require obscenity or pornography, but just a visual depiction of sexually explicit conduct.                                                                                                                       |
| Wal-Mart Stores Inc. v. Samara Bros. Inc.  | Oral Argument - January 19, 2000  | https://apps.oyez.org/player/#/rehnquist10/oral_argument_audio/20162 | Sandra Day O'Connor | 253.073  | 257.584  | 4.51     | It's... it's sort of like pornography: I know it when I see it.                                                                                                                                                                                                                       |
+--------------------------------------------+-----------------------------------+----------------------------------------------------------------------+---------------------+----------+----------+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```",37,5,timsehn,2020-04-28 20:51:48,https://www.reddit.com/r/datasets/comments/g9vcp4/us_supreme_court_transcripts/,0,datasets
g5w76s,A curated list of data annotation companies,,38,4,igorsusmelj,2020-04-22 06:40:26,https://data-annotation.com/list-of-data-annotation-companies,0,datasets
f5eb83,1002 Short stories dataset for research and fun,"A fresh dataset on Kaggle which could help with NLP prototyping across sentiment analysis to similarity text research

[https://www.kaggle.com/shubchat/1002-short-stories-from-project-guttenberg](https://www.kaggle.com/shubchat/1002-short-stories-from-project-guttenberg)",39,0,ShubC,2020-02-17 19:42:27,https://www.reddit.com/r/datasets/comments/f5eb83/1002_short_stories_dataset_for_research_and_fun/,0,datasets
dy2cz7,When not to use machine learning?,"When you are solving a problem, in what circumstances will you apply machine learning?

Is it true that in every circumstance, machine learning will always outperform rules and heuristic approaches?

In this article, I will explain using several real-world cases to illustrate why sometimes machine learning will not be the best choice to tackle a problem.

Link: [https://towardsdatascience.com/when-not-to-use-machine-learning-14ec62daacd7?source=friends\_link&sk=90b0f6d1945e92f9fcdccc1d6c6a95f7](https://towardsdatascience.com/when-not-to-use-machine-learning-14ec62daacd7?source=friends_link&sk=90b0f6d1945e92f9fcdccc1d6c6a95f7)

Comment below if you have any thoughts to add on!",39,9,weihong95,2019-11-18 12:08:34,https://www.reddit.com/r/datasets/comments/dy2cz7/when_not_to_use_machine_learning/,0,datasets
b39dal,Fanfic. Just... all of it.,"[https://archive.org/download/AO3\_story\_dump\_continuing](https://archive.org/download/AO3_story_dump_continuing)

[https://archive.org/details/updateablefanfic](https://archive.org/details/updateablefanfic)

[https://archive.org/details/FanficRepack\_Redux](https://archive.org/details/FanficRepack_Redux)

[https://archive.org/details/fanfic-meta-sqlite](https://archive.org/details/fanfic-meta-sqlite)

[https://archive.org/details/fictionpress\_save\_01\_nov\_26\_2018](https://archive.org/details/fictionpress_save_01_nov_26_2018)

&#x200B;

I'm BACK! I've uploaded hundreds of gigs of fanfic from [archiveofourown.org](https://archiveofourown.org), fictionpress, and [fanfiction.net](https://fanfiction.net), along with a sqlite db of the metadata, for easy searching. Need a natural language corpus? there are probably better ones out there, but here's this one! in about a dozen different languages too!

&#x200B;

enjoy.


EDIT: ao3continuing and updateable are compilations of datadumps I've had sitting around a while, ao3's are identical to the previous ones, with the addition of the newer dumps in one place.

I made these as one stop shops for those 2 website dumps in the future.",38,16,nerdguy1138,2019-03-20 08:13:24,https://www.reddit.com/r/datasets/comments/b39dal/fanfic_just_all_of_it/,0,datasets
av2wvq,Reconstructing Twitter's Firehose: How to reconstruct over 99% of Twitter's firehose for any time period,"Here's an interesting idea by the owner of [Pushshift.io](https://Pushshift.io), along with a great explanation of how Twitter IDs work:  [Reconstructing Twitter's Firehose: How to reconstruct over 99% of Twitter's firehose for any time period](https://docs.google.com/document/d/1xVrPoNutyqTdQ04DXBEZW4ZW4A5RAQW2he7qIpTmG-M/edit?usp=sharing).",38,3,ppival,2019-02-26 18:44:20,https://www.reddit.com/r/datasets/comments/av2wvq/reconstructing_twitters_firehose_how_to/,0,datasets
9v4hu1,"Huge list of Georgia Voter Absentee ballot information, direct from the GA SoS website (in theory, public data)",,41,5,skoorbevad,2018-11-07 23:11:14,http://sos.ga.gov/index.php/elections/november_6_2018_general_election_absentee_ballot_return_data?fbclid=IwAR2K0SY4t8Z9S9hMhiOmLu0UpzwjWCE3cQBUfqZMiGwdReQkj599C8JWs-Y,0,datasets
9pv8ft,Daily S&P 500 prices 1986-2018,"[daily SPX prices 1896-2018](https://www.kaggle.com/pdquant/stocks-significance-testing-p-hacking/data)

Surprisingly hard to find daily prices going back this far!

Also I’ve done an analysis to go with it.",42,6,pdquant,2018-10-20 15:57:32,https://www.reddit.com/r/datasets/comments/9pv8ft/daily_sp_500_prices_19862018/,0,datasets
909jjg,World Cup 2018 statistics for each game (CSV),"Scraped the [fifa.com](https://fifa.com) statistics page for each game into CSV.  

[https://gitlab.com/djh\_or/2018-world-cup-stats/blob/master/world\_cup\_2018\_stats.csv](https://gitlab.com/djh_or/2018-world-cup-stats/blob/master/world_cup_2018_stats.csv)  

Columns are: \`Game,Group,Team,Opponent,Home/Away,Score,WDL,Pens?,Goals For,Goals Against,Pen Shootout For,Pen Shootout Against,Attempts,On-Target,Off-Target,Blocked,Woodwork,Corners,Offsides,Ball possession %,Pass Accuracy %,Passes,Passes Completed,Distance Covered km,Balls recovered,Tackles,Blocks,Clearances,Yellow cards,Red Cards,Second Yellow Card leading to Red Card,Fouls Committed\`  

There's a little ruby scraper in that repo along with the source pages, but the above CSV is what is created.",40,10,None,2018-07-19 20:39:18,https://www.reddit.com/r/datasets/comments/909jjg/world_cup_2018_statistics_for_each_game_csv/,0,datasets
8pqm8s,"Complete Hackernews items dump is now available. ~17.2 million items including stories, comments, jobs, polls, etc.","**Site:** https://files.pushshift.io/hackernews/

The format of the files is ndjson.  Each json blob has a ""type"" parameter to specify if it is a comment, story, poll, etc.  This is a complete historical dump of Hackernews.  

Monthly updates are planned including a possible search API at some later point.

Please let me know if you have any questions!",40,3,Stuck_In_the_Matrix,2018-06-09 05:09:37,https://www.reddit.com/r/datasets/comments/8pqm8s/complete_hackernews_items_dump_is_now_available/,0,datasets
8ko3r0,"[Dataset] A sample of YouTube comments from Donald Glover's This is America up to May 14, 2018. Fields Include: Comment ID, Text Original, Published Date, Like Count, Author's Name.",,40,3,MzAtoz,2018-05-19 21:05:50,https://data.world/popculture/donald-glovers-this-is-america-youtube-comments,0,datasets
8bpm7x,We Have Just Released the Largest First Person Video Dataset: EPIC-Kitchens.Research,,42,2,cavedave,2018-04-12 12:04:30,https://epic-kitchens.github.io/2018,0,datasets
884vkh,My response to the paper highlighting issues with data incompleteness concerning my Reddit Corpus.,"First, I apologize for being away for more than a month \-\- I had some personal and family issues that weighed heavily on me and caused my extended absence.  For those who have or have not seen this paper \([https://www.reddit.com/r/Against\_Astroturfing/comments/84ipa1/\_/dvputaz/](https://www.reddit.com/r/Against_Astroturfing/comments/84ipa1/_/dvputaz/)\), The paper itself is available here:[https://towardsdatascience.com/caveat\-emptor\-computational\-social\-science\-8c2c2d5a7cc5](https://towardsdatascience.com/caveat-emptor-computational-social-science-8c2c2d5a7cc5).  I'd like to make a response and answer any questions from others pertaining to how I collect Reddit data.  The paper itself was nicely done, and although I do not agree with all of the conclusions in the paper, the paper itself was excellent and highlights the necessity for researchers to confirm the integrity of any data that they use for academic purposes.

It will take a while for me to properly address the major points in the paper, but I would like to provide an open forum for others to ask questions about the methodology and procedures I use when collecting Reddit data.  I recommend that everyone read the paper and get familiar with the major points within the study.

I would like to make a quick response now and say that throughout the process of collecting and publishing these datasets, I have learned a great deal about the ins and outs of Reddit's API as well as better managing the process of big data collection and improving on my efforts to ensure that the data that I collect is as complete as possible given the fact that a lot of the data is constantly changing and evolving through time.  A majority of the data that was missing from the archives was focused on Reddit's earlier years \(2009 and earlier\).  There was also gaps in the data from the Reddit Blackout in July of 2015 that affected some of the data in the archives due to subreddits going private while I was ingesting data.  Once I have time to properly address the concerns in this study, I will follow up with an explanation of what caused these issues and what I learned from the process and how I am now addressing the various issues that caused gaps in the data due to different circumstances.

If anyone has questions, please feel free to ask them here and I will do my best to answer as quickly as possible.  I will be following up soon with more details and addressing the various issues that I faced collecting Reddit data.

Thank you!

**Edit:**  I am reingesting submissions and will be replacing the RS_yyyy-mm files with RS2_yyyy-mm files to signify the second revision.  I will still keep the old files for academic purposes and will place them in an ""old"" directory.  The new submission files will be as complete as possible for the early years of Reddit.  I may just reingest all submissions because Reddit changed their scoring algo and the current submissions have the old adjusted scores.  The new files (RS2) will have the updated scores.  I will still keep the retrieved_on key which gives the epoch time of when I collected the data.  I estimate it will take a couple months to update and reingest all submissions.  I will replace the 2006-2010 period as quickly as possible.  I estimate it will take several weeks to complete that.

Since there are far more comments than submissions, I will first work to make sure the early period of Reddit is complete and then decide how to proceed with the later years.  
",42,6,Stuck_In_the_Matrix,2018-03-29 20:56:59,https://www.reddit.com/r/datasets/comments/884vkh/my_response_to_the_paper_highlighting_issues_with/,0,datasets
6hvu43,"Google Creates a Digital Archive of World Fashion: Features 30,000 Images, Covering 3,000 Years of Fashion History",,37,1,cavedave,2017-06-17 21:11:53,http://www.openculture.com/2017/06/google-creates-a-digital-archive-of-world-fashion.html,0,datasets
61zlcl,A list of threatened federal datasets saved by data journos,,37,0,ehp29,2017-03-28 14:16:13,http://ire.org/blog/nicar/2017/03/24/ire-nicar-federal-data-directory/,0,datasets
5y9i2t,Transcripts of last year's 10 highest grossing films,,41,2,DataScienceInc,2017-03-08 18:07:46,https://github.com/ProQuestionAsker/2016MovieDialogue,0,datasets
5pcqtz,The official list of where all of the Obama White House social media data has migrated to,,38,3,danwin,2017-01-21 20:38:36,https://obamawhitehouse.archives.gov/blog/2017/01/17/obama-administration-digital-transition-moving-forward,0,datasets
43e4kf,"Data Generator tool allows you to generate up to 10,000 rows of data in several file formats (CSV, Excel, SQL, JSON, HTML and XML).",,41,1,Lbienn,2016-01-30 13:48:40,http://www.yandataellan.com/,0,datasets
3wyhka,Wikipedia now has a pageviews API,,37,4,danwin,2015-12-15 16:42:04,http://blog.wikimedia.org/2015/12/14/pageview-data-easily-accessible/,0,datasets
3m1rkm,NASA open datasets,,36,3,dfhsr,2015-09-23 10:02:53,https://open.nasa.gov/,0,datasets
3dg9t0,Black Market Archives: 1.5 TB of Dark Net Market scrapes,,38,5,danwin,2015-07-16 01:08:11,http://www.gwern.net/Black-market%20archives,0,datasets
1jvs19,Zillow US housing datasets now accessible through API,,37,0,Kalemic,2013-08-07 13:45:54,http://www.quandl.com/housing,0,datasets
zp7e8m,"U.S. Patents dataset, including expiry date (or issue date) and patent number.",I don't see how to search for this information on https://uspto.gov .... Issue date would also work as expiry date would be easily deduced.,37,14,g51BGm0G,2022-12-18 20:22:06,https://www.reddit.com/r/datasets/comments/zp7e8m/us_patents_dataset_including_expiry_date_or_issue/,0,datasets
yxrjxa,Bigfoot Sightings Dataset (and some analysis),"Some extremely interesting data on the sightings of Bigfoot, aka Sasquatch. With this dataset, you can see exactly where the most sightings are coming from, what these sightings look like, and even the environment in which the sighting took place.

This data originated from [The Bigfoot Field Researchers Organization](https://www.bfro.net/), and became available on Kaggle [here](https://www.kaggle.com/datasets/josephvm/bigfoot-sightings-data). \[Self-promotion\] We've re-hosted the data in Gigasheet here for exploration before downloading the file: [https://app.gigasheet.com/spreadsheet/Bigfoot-Sightings/3f64218d\_3ea2\_47a4\_900c\_5c975ffcf0ad?public=true](https://app.gigasheet.com/spreadsheet/Bigfoot-Sightings/3f64218d_3ea2_47a4_900c_5c975ffcf0ad?public=true)

Here are some highlights:

* **Washington State has the highest number of sightings** out of all states with around 11% of all sightings.
* The **majority of Bigfoot sightings occur during the summer** in which approximately 34% of sightings take place. Fall is in second place with around 27% of sightings.
* **2012 was the year with the most recorded sightings** with 191. Bigfoot sightings seem to be trending upwards, which might mean we are close to finding them (or maybe we are going crazy!)",36,8,n1nja5h03s,2022-11-17 14:20:05,https://www.reddit.com/r/datasets/comments/yxrjxa/bigfoot_sightings_dataset_and_some_analysis/,0,datasets
v1192a,I don't get the many shady location data providers if there is Google Popular Times and Open Street Map that you can access with ease and drive similar conclusions.,"location data providers are often in the press with negative headlines. Those services aggregate movement data from apps and aggregate the data to derive movement patterns which might be helpful for marketers. In fact, I had two moments in my life where I evaluated a PoC with those location data brokers. 

1. They were all **shady about where the data comes from** which is important to understand the Bias of the data. I never got a good answer. 
2. The data often **just represented < 0.4% of the population** (at least in Europe - different game in the USA). For a big city they might have 20K unique users while in the city were more than 3M users living. 
3. They dismiss **any professional data analytics principle**. The data comes in CSV (if a lot of data they give you like 10 separate files). Data was not always plausible in itself

Those experiences brought me to build certain parts of those data brokers but only with open-source data:

1. If it is about **location data you should know OpenStreetMap**. It's the biggest Database with meta info on location. It's not perfect but big companies like Mapbox, Apple, and Microsoft rely on it. Since the API is kind of messy, you can load with this repository whole cities information smoothly into a PostGres --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/osm-poi/README.md

2. Googe Popular Times: **Movement data can be also found on Google**. When you search a location it is often shown how frequently a place was visited (on an index of 0-100). With this libary you can access all the Popular Times data for location and entire cities --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/google-poi/README.md


3. Global Admin Boundaries: A huge problem that often people feel when working with location data is aggregating the data into different geo-based slices (country level, admin level, or even smaller into sub-districts). Here is a repo that cleaned the data out of Open Street Map for geo boundaries worldwide from very broad to a very small granularity --> https://github.com/kuwala-io/kuwala/blob/master/kuwala/pipelines/admin-boundaries/README.md

I think with those Open Source Tools and some data science magic you can generate similar outcomes as those location data providers but totally anonymized and free. **Would be awesome if anybody is interested in building a case around it :-)**",38,1,kuwala-io,2022-05-30 14:19:32,https://www.reddit.com/r/datasets/comments/v1192a/i_dont_get_the_many_shady_location_data_providers/,0,datasets
prxat9,"[Self Promotion] Free, cleaned database of US Schools","Hi r/datasets,

At DoltHub, we completed our US Schools data bounty last week. We build a free, cleaned database of US Schools.

Dataset: [https://www.dolthub.com/repositories/dolthub/us-schools](https://www.dolthub.com/repositories/dolthub/us-schools)

Blog: [https://www.dolthub.com/blog/2021-09-20-schools-bounty-retrospective/](https://www.dolthub.com/blog/2021-09-20-schools-bounty-retrospective/)

We're also running another US businesses data bounty now if you're interested.",40,7,timsehn,2021-09-20 15:35:08,https://www.reddit.com/r/datasets/comments/prxat9/self_promotion_free_cleaned_database_of_us_schools/,0,datasets
ojrfqk,[self-promotion] Brain Tumor Segmentation Dataset,"Hello frens,

&#x200B;

I uploaded a [dataset of MRI Scans for brain tumor segmentation](https://www.kaggle.com/andrewmvd/brain-tumor-segmentation-in-mri-brats-2015). It is the training set for the BraTS competition for the years 2018, 2019 and 2020. The data contains MRI scans and expert segmentations for HGG and LGG (high grade and low grade gliomas), as well as survival data. 

It can be used for tumor type classification, tumor segmentation and survival analysis.

Hope it helps,

&#x200B;

All credits go to:

>\[1\] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. ""The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694  
>  
>\[2\] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., ""Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features"", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117  
>  
>\[3\] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, et al., ""Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge"", arXiv preprint arXiv:1811.02629 (2018)",39,1,larxel,2021-07-13 23:11:44,https://www.reddit.com/r/datasets/comments/ojrfqk/selfpromotion_brain_tumor_segmentation_dataset/,0,datasets
mejju8,"Looking for any real-time API's/datasets, doesn't matter what the data is so long as it's updated in real time.","Essentially the title, looking for \*any\* datasets that are streamed real time. Any suggestions?",38,9,347247,2021-03-27 18:28:49,https://www.reddit.com/r/datasets/comments/mejju8/looking_for_any_realtime_apisdatasets_doesnt/,0,datasets
lzn1qy,Covid 19 Data. How we got it and how we should save it,,39,3,cavedave,2021-03-07 09:11:41,https://twitter.com/chipx86/status/1368312922754019329,0,datasets
lwdoki,"[OC] What's in your data? Easily extract schema, statistics and entities from a dataset",,38,2,austingwalters,2021-03-02 22:00:07,https://github.com/capitalone/DataProfiler,0,datasets
jdr32z,"Dataset of 21,255 IRC quotes from bash.org with scores.",,38,2,dwrodri,2020-10-18 23:49:26,https://gitlab.com/dwrodri/bash_irc_quotes,0,datasets
j9bbvg,"Food price dataset including honey and the most common foods, from the last 10-20 years",I'd like to decide and show whether honey overperforms other food items or not (which food was 'the best investment' in the last 10-20 years). I couldn't find any datasets about this. I'm mostly interested in Hungary or Europe specific datasets but at this point anything will do. Thanks in advance.,37,6,Ballydon,2020-10-11 19:07:04,https://www.reddit.com/r/datasets/comments/j9bbvg/food_price_dataset_including_honey_and_the_most/,0,datasets
herxfr,Video Game Sales as of 2020,,38,4,PKtheworldisaplace,2020-06-24 01:51:42,https://www.kaggle.com/baynebrannen/video-game-sales-2020,0,datasets
gz1cs0,Drone Shoreline Dataset (Boats/Docks/Lifts annotated for object detection),,36,3,aloser,2020-06-08 15:24:24,https://public.roboflow.ai/object-detection/aerial-maritime,0,datasets
dwe0l9,School shootings by guns owned per capita per state data set,"After today’s shooting in Santa Clarita, I’d like to get a holistic view of school shootings (no other type of shooting) by state in the US",41,7,Tyraniczar,2019-11-14 18:56:37,https://www.reddit.com/r/datasets/comments/dwe0l9/school_shootings_by_guns_owned_per_capita_per/,0,datasets
d6d4dy,Top Python Packages metadata and dependencies (pypi dump),,39,1,amirouche,2019-09-19 12:10:15,https://archive.org/details/pypi21k.20181110.pack,0,datasets
cdjzt3,usa-soccer: CSV data of the USA soccer teams and locations,,38,3,gavreh,2019-07-15 16:56:17,https://github.com/gavinr/usa-soccer,0,datasets
bhnij2,Does AI Get the Joke? Researchers try to teach AI to learn humor,,40,5,Yuqing7,2019-04-26 15:21:16,https://medium.com/syncedreview/does-ai-get-the-joke-43cfe3f0b125,0,datasets
9midyj,Every Sex and the City script,,35,8,snappcrack,2018-10-08 20:05:33,https://www.kaggle.com/snapcrack/every-sex-and-the-city-script,0,datasets
9eyvez,Tencent Open-Sources Its Massive Multi-Labeled Image Dataset,,37,3,trcytony,2018-09-11 16:06:13,https://medium.com/syncedreview/tencent-open-sources-its-massive-multi-labeled-image-dataset-7b0b3dd5373f,0,datasets
98bayf,A dataset of 785 D&D characters,,39,0,mouse_Brains,2018-08-18 12:21:14,https://www.reddit.com/r/dndnext/comments/98b8qp/is_your_dd_character_rare_25_data_release_and/,0,datasets
91vf4q,Every cyclist of the Tour de France in a single CSV file,"A detailed description of how I got the data can be found here: [https://www.camminady.org/every-cyclist-of-the-tour-de-france-in-a-single-csv-file/](https://www.camminady.org/every-cyclist-of-the-tour-de-france-in-a-single-csv-file/)

Source: Official Tour de France website.

Tools: Python, Matlab, R and last fixes had to be done manually with Vim.

Github: [https://github.com/camminady/LeTourDataSet](https://github.com/camminady/LeTourDataSet)

First 11 out out 9041 lines:

    year,rank,name,id,team,time,h,m,s
    1903,1,MAURICE GARIN,1,TDF 1903,94h 33m 14s,94,33,14
    1903,2,LUCIEN POTHIER,37,TDF 1903,97h 32m 35s,97,32,35
    1903,3,FERNAND AUGEREAU,39,TDF 1903,99h 02m 38s,99,2,38
    1903,4,RODOLPHE MULLER,33,TDF 1903,99h 12m 44s,99,12,44
    1903,5,JEAN-BAPTISTE FISCHER,12,TDF 1903,99h 41m 58s,99,41,58
    1903,6,MARCEL KERFF,9,TDF 1903,101h 37m 38s,101,37,38
    1903,7,DIT SAMSON JULIEN LOOTENS,28,TDF 1903,104h 06m 22s,104,6,22
    1903,8,GEORGES PASQUIER,2,TDF 1903,105h 59m 18s,105,59,18
    1903,9,FRANÇOIS BEAUGENDRE,45,TDF 1903,106h 27m 28s,106,27,28
    1903,10,ALOIS CATTEAU,71,TDF 1903,108h 20m 11s,108,20,11

Please let me know if you find errors.",40,12,k1next,2018-07-25 20:16:50,https://www.reddit.com/r/datasets/comments/91vf4q/every_cyclist_of_the_tour_de_france_in_a_single/,0,datasets
8jq5a4,"[dataset] Lines from the scripts of all 9 seasons of The Office, shared by @abhinavr8 on data.world. Includes the line, episode, season, scene, and name of speaker. (I work for data.world). Hope you enjoy exploring.",,39,5,MzAtoz,2018-05-15 23:18:38,https://data.world/abhinavr8/the-office-scripts-dataset,0,datasets
7ihh5e,Darknet Market Cocaine Listings,,38,7,narhasan,2017-12-08 19:51:40,https://www.kaggle.com/everling/cocaine-listings,0,datasets
78bnlv,United States Census Bureau APIs,,38,0,Vardox,2017-10-23 23:08:01,https://www.census.gov/developers/,0,datasets
73afxg,The Complete Pokemon Dataset,,36,0,rony1996,2017-09-29 20:18:43,https://www.kaggle.com/rounakbanik/pokemon,0,datasets
6v685o,Complete Hacker News / Ycombinator data dump,"**Location:**  https://files.pushshift.io/hackernews/

This is the complete API dump for Hacker News / Ycombinator from 2006 to July of 2017.

**sha256sums**

6ea91006618ab7cd6e58a2f42b0907be1e12c3f436c7987b2ff49d86b3c07df5  HN_2006.bz2

53bf6a400cd1cfd716f525ba5be4e1b64062df27112b219f71dddc35b2ca33de  HN_2007.bz2

d0cd72457223d4b2a2e73cd5c45a35a2e0dc0a4cb7a5356d852bac56ba6ced5a  HN_2008.bz2

20c37726ad76fa99e73853c8b332d2aa9d9988989ff77d84c9f256132e0de82e  HN_2009.bz2

c6e8e69e7e17266324d969ba4e2acad85312859d772abee422eac1424193f5bd  HN_2010.bz2

9da4ed0c1e8a626aa8ead983cd53f361316b6075f281f7910be176f6a9f271f2  HN_2011.bz2

96271fbde309abc3ccfaf972642b57dd1b64b0b0ed692f50a82e5e19e46a47ff  HN_2012.bz2

ab42d4f0a96d04d11432ee94d086e3a227d5f82835adcf8356a535e91df63e05  HN_2013.bz2

e866f46931198ddfd61b17db9247cd9ccb71d66a1b568088717a199061979138  HN_2014.bz2

9af11af2302b9062cee7b90a8ff81ede002ab9840844dc56976e71d8881f853f  HN_2015.bz2

f60142708d52b4a11e92dae6e50b74e505768e7edce3b59425c8ead32d705a45  HN_2016.bz2

c6f3dc178faee6086a412dfb8fc26bdac9535bb398b882932df71d3284e0ac92  HN_2017-01.bz2

adbe6fee73c76592b23b7eb81cd9e722b1af02cdb761ce3d4968c96b4903ae15  HN_2017-02.bz2

102b4c7cd7980a69243866029d2bf5d16b5f1840585092e8ed5387c39e3e26b4  HN_2017-03.bz2

e00e793b8f96b26415c72c28ea93ef1a0e8a065cff5f3e31ba9be6febe38de79  HN_2017-04.bz2

90f247bab3b0ecb65f56e9f8739e9202bbb1510d327ed8e8f8a24ac8b2cf783e  HN_2017-05.bz2

258e67ea951f12a74cf9d757c0ea84ae0eb577946f7f01ee2c42bc565b3488f8  HN_2017-06.bz2

c9739641f87e571aaa1c7ed8c95d8783350e796a9a3f36197342d2f440cd3e58  HN_2017-07.bz2


",37,5,Stuck_In_the_Matrix,2017-08-21 21:27:40,https://www.reddit.com/r/datasets/comments/6v685o/complete_hacker_news_ycombinator_data_dump/,0,datasets
5uftxe,Reddit January 2017 Comments now available via Torrent. Other months will move over to Torrents soon.,"magnet:?xt=urn:btih:68eba39a077f82f5df201916b1aafe16384427bb&dn=RC_2017-01.bz2

I read up on Torrent technology and realized that DHT allows for trackerless torrenting (technically the torrent software acts as part of a tracker overall).  

I will be adding the other files over time, but hopefully the speeds will be much better.  If anyone can test this and report back with the speed they received, I'd greatly appreciate it.  ",38,18,Stuck_In_the_Matrix,2017-02-16 15:58:35,https://www.reddit.com/r/datasets/comments/5uftxe/reddit_january_2017_comments_now_available_via/,0,datasets
5qe2m4,Datasets/resources for data that you would not expect to be public,"I like the datasets and dataisbeautiful subreddits and I am always fascinated at the places people find data or how they have mined data. 
What are some interesting resources you have found that you were surprised to find existed at all?",35,10,the_ju66ernaut,2017-01-27 00:38:54,https://www.reddit.com/r/datasets/comments/5qe2m4/datasetsresources_for_data_that_you_would_not/,0,datasets
5jti8w,"Top 1000 GitHub Repos, by Stars Given in 2016",,38,5,minimaxir,2016-12-22 22:28:33,https://docs.google.com/spreadsheets/d/11bGpZq6ixlhrmQnzEUqbgbwTQwQVdtvILjp32vaOKBc/edit?usp=sharing,0,datasets
1k0ubj,database of over 600 battles that were fought between 1600 AD and 1973 AD,,36,2,None,2013-08-09 13:07:05,https://github.com/jrnold/CDB13,0,datasets
z88rts,"""The Stack: 3 TB of permissively licensed source code"", Kocetkov et al 2022 (esp Python)",,36,1,gwern,2022-11-29 22:46:13,https://arxiv.org/abs/2211.15533,0,datasets
xd8pqf,Looking for: dataset of slurs to ban from my Discord server,"Hello,

I run an 18+ Discord server and I would like to set up my bots to auto-delete messages with especially offensive terms (not terms like fuck, shit, cunt, etc. but rather slurs like the N-word) 

I've found a few lists, but they almost always include words that require context to be offensive (eg. apple, black, chief, autism, gay) which I don't want to include on a blacklist because they're often used in inoffensive contexts. 

I also don't want to blacklist general swear words like: shit, fuck, bastard, or cunt (fine in UK/AUS).

Is there a database of slurs that are stand-alone highly offensive slurs, but does not include swear words (again like the N-word, but excludes stuff like cunt)?",37,23,karorom,2022-09-13 14:14:56,https://www.reddit.com/r/datasets/comments/xd8pqf/looking_for_dataset_of_slurs_to_ban_from_my/,0,datasets
ujdhsk,ALL THE VOLCANOES PRESENT ON EARTH (with various details),,33,1,deepcontractor,2022-05-06 02:34:16,https://www.kaggle.com/datasets/deepcontractor/the-volcanoes-of-earth,0,datasets
qud34u,[self-promotion] Spotify Playlists Dataset,"Hello frens  


I published on kaggle a dataset of 1.2 GB of spotify data (user hashes, artists, tracks and playlists) scraped from the #nowplaying hashtag on twitter.  


It is available on this [link](https://www.kaggle.com/andrewmvd/spotify-playlists).  


All credits are due to the original authors:

> Pichl, Martin; Zangerle, Eva; Specht, Günther: ""Towards a Context-Aware Music Recommendation Approach: What is Hidden in the Playlist Name?"" in 15th IEEE International Conference on Data Mining Workshops (ICDM 2015), pp. 1360-1365, IEEE, Atlantic City, 2015. 

  
Cheers",35,9,larxel,2021-11-15 10:01:55,https://www.reddit.com/r/datasets/comments/qud34u/selfpromotion_spotify_playlists_dataset/,0,datasets
qpg8mr,Films and IMDB Info By Actor For 10K Actors,,35,3,darinhq,2021-11-08 15:42:16,https://www.kaggle.com/darinhawley/imdb-films-by-actor-for-10k-actors,0,datasets
lw5f7g,[self-promotion] [Synthetic] face2comics 512x512 2x10000 generated images dataset,"I've taken a survey recently and got positive feedback, so I've decided to share the dataset here.

Here's the repo with download url and description: [https://github.com/Sxela/face2comics](https://github.com/Sxela/face2comics)

and some previews.

10x10 samples (faces on the left, corresponding comics on the right)

https://preview.redd.it/j7ljp7ie1nk61.jpg?width=2560&format=pjpg&auto=webp&s=fd1a3fba9358c472c7c98d780c128a54f9944e62

This is an inference (test) sample on a fastai unet trained on this very dataset:

https://preview.redd.it/4bbrcsgj1nk61.jpg?width=510&format=pjpg&auto=webp&s=7f2606eec192fad21848ceb2dae74c5e61d77303

&#x200B;

This one has already been here:

2x2 generated samples

https://preview.redd.it/ghqfvslh1nk61.jpg?width=1024&format=pjpg&auto=webp&s=f2fe37ac49bc743d4237e518ebb1ea0782bcabb7",37,7,devdef,2021-03-02 16:04:16,https://www.reddit.com/r/datasets/comments/lw5f7g/selfpromotion_synthetic_face2comics_512x512/,0,datasets
ltptkt,Bangladesh Government COVID-19 Testing Data,,38,0,zigazigzig,2021-02-27 14:59:21,/r/bangladesh/comments/lqa2vx/bangladesh_government_covid19_testing_data/,0,datasets
jzjzt4,"Two weeks ago, I used open data from Pennsylvania to investigate two claims of election fraud. Today, Pennsylvania took down the data. [self-promotion]","Two weeks ago I posted a dataset by the Pennsylvania Department of State about mail ballots, and used it to investigate two separate claims of election fraud:

[https://www.reddit.com/r/datasets/comments/jr32vb/debunking\_an\_election\_fraud\_claim\_using\_open\_data/](https://www.reddit.com/r/datasets/comments/jr32vb/debunking_an_election_fraud_claim_using_open_data/)

Over the weekend, Pennsylvania took down the data set. We still have our copy up:

[https://www.dolthub.com/repositories/dolthub/pa\_mail\_ballots\_2020](https://www.dolthub.com/repositories/dolthub/pa_mail_ballots_2020)

In this follow-up post, I make the case that for ""open"" data to be truly open, it needs to be decentralized to prevent this kind of abuse of trust.

[https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/#Update2](https://www.dolthub.com/blog/2020-11-09-debunking-election-fraud/#Update2)

Disclaimer: I'm a software engineer that works for DoltHub. The Dolt tool is free and open source, but we sell additional services for it.",36,23,zachm,2020-11-23 15:46:32,https://www.reddit.com/r/datasets/comments/jzjzt4/two_weeks_ago_i_used_open_data_from_pennsylvania/,0,datasets
hx4yhy,Any advice for scraping Google Trends data," I want to scrape massive amounts of google trends data (i.e. 1800 requests for about 5000 keywords) I've tried using proxy-cheap, but based on its performance so far, the total cost will tally up to be alot (around $5000). Does anyone know a cheaper alternative to this or another method that could help make this solution cheaper or know somewhere I could find an already cleaned dataset online?",36,12,zestybananas,2020-07-24 16:32:11,https://www.reddit.com/r/datasets/comments/hx4yhy/any_advice_for_scraping_google_trends_data/,0,datasets
gjn04r,"Cheapest way to get 10,000 home/rent values?","Short term I need 10,000 home or rent values based on addresses, long term 100k-10M. 

Expensive solutions- Paid APIs, seems like 100-300$.

Mid tier- Scrape, I get an IP address rotator and burn through IPs, (I believe 10$/mo)

Free?

I'm a 12 year programmer, so implementing things are easy.",37,32,canIbeMichael,2020-05-14 14:06:37,https://www.reddit.com/r/datasets/comments/gjn04r/cheapest_way_to_get_10000_homerent_values/,0,datasets
g4wz11,How to UNPIVOT multiple columns into tidy pairs with SQL and BigQuery,,37,2,fhoffa,2020-04-20 17:19:52,https://towardsdatascience.com/how-to-unpivot-multiple-columns-into-tidy-pairs-with-sql-and-bigquery-d9d0e74ce675,0,datasets
g495hu,[Dataset] English - Telugu transliteration data mined from reddit and wikipedia,,34,2,winchester6788,2020-04-19 15:06:46,https://github.com/notAI-tech/Datasets/tree/master/En-Te_Transliteration,0,datasets
g37u83,Most comprehensive bad words dataset on the internet,"We published the most complete bad word dataset on the internet. I wrote a blog post about importing our [bad-words dataset to DoltHub](https://www.dolthub.com/repositories/Liquidata/bad-words/) and using it in a simple python application called [chat-bot-profanity-filter](https://github.com/liquidata-inc/chat-bot-profanity-filter).  Check out the blog here: [https://www.dolthub.com/blog/2020-04-16-f-you-in-4-languages/](https://www.dolthub.com/blog/2020-04-16-f-you-in-4-languages/)

As I mention in the conclusion, if you find any bad words we're missing, email me [katie@liquidata.co](mailto:katie@liquidata.co) and I will give you write permissions to contribute!",38,4,ktmcculloch,2020-04-17 18:41:02,https://www.reddit.com/r/datasets/comments/g37u83/most_comprehensive_bad_words_dataset_on_the/,0,datasets
e021fv,The Indian Movie Database: ranging from 1950-2019,,38,3,pncnmnp,2019-11-22 14:55:23,https://www.kaggle.com/pncnmnp/the-indian-movie-database,0,datasets
cy8s3f,Looking for Uighur Human Rights Data,"Hey everyone,

&#x200B;

I'm hoping to put together some visuals which highlight the human rights attacks from China to the Uighur's in Xinjiang. I know this is a bit broad but here's of examples examples of what would be really helpful:

* Re-education camp location and sizes
* Growth in Uighur related facial recognition academic papers

&#x200B;

Thanks for all help!",36,6,ChemEngandTripHop,2019-09-01 11:21:56,https://www.reddit.com/r/datasets/comments/cy8s3f/looking_for_uighur_human_rights_data/,0,datasets
cow0vr,Looking for a dataset of every Kanye west lyric ever?,Any other rappers would be great too!,35,11,birch-trees,2019-08-11 12:12:58,https://www.reddit.com/r/datasets/comments/cow0vr/looking_for_a_dataset_of_every_kanye_west_lyric/,0,datasets
br0jsp,buffer.com Salaries,,39,7,cavedave,2019-05-20 20:04:02,https://docs.google.com/spreadsheets/d/11s9VSyf4yaYUsqBKLaVH78NL8wdl8gXoj5BGAzjIFuc/edit#gid=671465451,0,datasets
bgq8tc,62683 Google Play App Listings + 10500 Etsy T-Shirt Product Listings + More,"Here are a few of the datasets I've collected over the past year + jupyter notebooks so you can see what's in them!

[https://github.com/schlende/practical-pandas-projects](https://github.com/schlende/practical-pandas-projects)

I collected most of this data to look for product opportunities. Check out the etsy **view count** data and the Google Play **ratings per day** data.

With a bit of analysis you can get a sense of which products are popular and which aren't.

Enjoy!",38,1,schlendeus,2019-04-24 04:19:44,https://www.reddit.com/r/datasets/comments/bgq8tc/62683_google_play_app_listings_10500_etsy_tshirt/,0,datasets
bajxk7,U.S. Census Bureau deanonymization attacks,,41,11,cavedave,2019-04-07 19:16:29,https://twitter.com/john_abowd/status/1114942180278272000,0,datasets
8hy0lf,"Subreddit Traffic - 52 subs sampled every 10 minutes for a month - Source code, charts, and data in comments",,36,4,aaronpenne,2018-05-08 16:23:59,https://github.com/aaronpenne/data_visualization/tree/master/traffic,0,datasets
7j83zg,Dataset Containing ISP Contributions and Net Neutrality Position for all 535 Members of Congress,,38,5,Theriley106,2017-12-12 04:03:52,https://www.kaggle.com/theriley106/net-neutrality-accountability,0,datasets
60oh8r,FiveThirtyEight's tally of how often congresspeople vote in line with Trump,,35,1,None,2017-03-21 16:10:47,https://www.kaggle.com/ctlente/trump-score,0,datasets
550w1a,"Data from the San Francisco Police Department, including traffic stops, officer-involved shootings, and department demographics",,37,5,danwin,2016-09-29 06:22:17,http://sanfranciscopolice.org/data,0,datasets
43im3r,List of high-quality open datasets in public domains,,35,1,Habitual_Emigrant,2016-01-31 10:23:01,https://github.com/caesar0301/awesome-public-datasets,0,datasets
3rnktj,"Academic Torrents, reddit's imgur for data?","We are the founders of Academic Torrents (academictorrents.com), a system for peer-to-peer transfer of scientific data. We're a pending U.S. nonprofit started by data mining PhD students who often faced issues transferring lots of data. 

We want you to use our service to share data on Reddit! Have you heard of us? Have you used us? How can we better solve your problems? 

In July a user of Reddit shared 160GB of historical reddit data using our service and in total 18TB of data has been transferred! Get the data here: http://academictorrents.com/details/7690f71ea949b868080401c749e878f98de34d3d

-Joseph and Henry
",34,19,academictorrents,2015-11-05 17:50:53,https://www.reddit.com/r/datasets/comments/3rnktj/academic_torrents_reddits_imgur_for_data/,0,datasets
3gegdz,1.7 millions Youtube videos' description,"Hi,

I have a small dataset containing various informations about random Youtube videos. I have nothing to do with it, so before I delete it, I'm sharing it with you.

In fact, there's two datasets, one about the videos, and one containing uploaders' custom name. It does **not** contains informations about view count, likes/dislikes, subscribers, ...

There's ~1.7 millions entries in the first dataset, and ~0.4 millions in the second one. Both contains JSON blocks delimited by new lines (\n).

The data has been collected randomly, so do not expect to have every uploaded videos by a uploader. The uploader might have uploaded 10 videos, but only five may be indexed...

**exportVideos.json fields**

* id: the video's id

* uploader: the uploader's id (match uploader in exportUploaders.json)

* upload_date: YYYY-MM-DD

* title

* description

* duration: video's duration in seconds

*Example*

    {""id"":""UEX16lWliQc"",""uploader"":""CGPGrey"",""upload_date"":""2014-02-12"",""title"":""ANNOUNCEMENT: CGP Grey Podcast"",""description"":""http:\/\/hellointernet.fm"",""duration"":""26""}


**exportUploaders.json fields**

* uploader: the uploader's id (match uploader in exportVideos.json)

* uploader_name: the uploader's custom name

*Example*

    {""uploader"":""CGPGrey"",""uploader_name"":""CGP Grey""}


Files size:

* exportUploaders.json.gz: 6.6M (25M uncompressed)

* exportVideos.json.gz: 425M (1.2G uncompressed)

**Magnet**:

    magnet:?xt=urn:btih:410e59c85165326a042100a2de5b3edc8fe3a786&dn=youtube_dataset",33,3,BradPatt,2015-08-09 23:27:35,https://www.reddit.com/r/datasets/comments/3gegdz/17_millions_youtube_videos_description/,0,datasets
3e9zr5,Who's ready for more data? June Reddit comment dump (RC_2015-06),"Yes, submissions are coming.  [Soon.](http://orig15.deviantart.net/f256/f/2013/212/3/3/soon_cat_by_huskyfish-d6g1qoe.jpg)

---------------------------------------------------------


**RC_2015-06 (Reddit June Comments)**

54,919,108 Comments 

33,780 MB / 5,737 MB (83% compression)

MD5: 2a60afe9ef24e78f78e0351cb48423ae

magnet:?xt=urn:btih:49e2d1c63accba9ede6defcaeabc89f108ce37eb&dn=RC%5F2015-06.bz2&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80&tr=udp%3A%2F%2Fopen.demonii.com%3A1337",34,23,Stuck_In_the_Matrix,2015-07-23 02:43:41,https://www.reddit.com/r/datasets/comments/3e9zr5/whos_ready_for_more_data_june_reddit_comment_dump/,0,datasets
38g57i,"220,579 movie conversations (including character data)",,37,1,hufflepuph,2015-06-04 00:09:12,http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html,0,datasets
2xt6ty,Large collection of public data sets,,36,3,rhiever,2015-03-03 18:12:10,https://github.com/caesar0301/awesome-public-datasets,0,datasets
11uj04x,Housing Data: the number of dwellings and people in cities and countries around the world,,34,2,cavedave,2023-03-18 09:05:51,https://github.com/jgleeson/PublicHouse,0,datasets
11os2jl,"270K of oil and gas wells dataset in Pennsylvania, USA","Historical oil and gas drilling activity in PA, with over 270K wells, coordinates locations, oil company who drilled it, etc. etc. Look at the ""Data Dictionaries"" to get a sense of what each column means.

[https://www.depgreenport.state.pa.us/ReportExtracts/OG/OilGasWellInventoryReport](https://www.depgreenport.state.pa.us/ReportExtracts/OG/OilGasWellInventoryReport)",35,4,starwag,2023-03-11 19:01:57,https://www.reddit.com/r/datasets/comments/11os2jl/270k_of_oil_and_gas_wells_dataset_in_pennsylvania/,0,datasets
zfe9lo,Using Open Data to find NIMBY counties in the USA,"Hi all,

This is Tim Sehn CEO of [DoltHub](https://www.dolthub.com), famous on this sub for our [data bounties](https://www.dolthub.com/bounties) and maybe more importantly, [the novel open data they produce](https://www.dolthub.com/profile/bounties).

For our [US Housing prices bounty](https://www.dolthub.com/repositories/dolthub/us-housing-prices-v2) we collected 112GB of schema-ed housing sale data from around the US.

We had a bounty contributor use the data to analyze where the NIMBY and YIMBY counties were in the US using a novel method. 

[https://www.dolthub.com/blog/2022-12-08-nimby-yimby/](https://www.dolthub.com/blog/2022-12-08-nimby-yimby/)

I thought this sub would be interested in the open dataset and use the analysis as an inspiration on how to use it.",36,3,timsehn,2022-12-07 21:21:07,https://www.reddit.com/r/datasets/comments/zfe9lo/using_open_data_to_find_nimby_counties_in_the_usa/,0,datasets
uloglb,[Self Promotion] I scraped Blind App and created a dataset of company reviews for over 25 companies!,"Hi folks,

Over the weekend, I scraped employee reviews of over 25 companies over \[Blind App\]([https://www.teamblind.com/](https://www.teamblind.com/)) to build a dataset that comprises a company's rating, pros, cons, resignation reasons and an overall description of an employee's review about the company.

Blind is an anonymous workplace network where verified professionals connect to discuss about compensation, work-life balance, interviewing, pros/cons and much more. The data was scraped using a Python script and is now available as a CSV and a JSON for each of the 25 companies.

Check it out on Kaggle here: [https://www.kaggle.com/datasets/harshcasper/blind-app-company-reviews](https://www.kaggle.com/datasets/harshcasper/blind-app-company-reviews)

You can also submit feedback and bug/feature reports on GitHub: [https://github.com/HarshCasper/Blind-App-Reviews](https://github.com/HarshCasper/Blind-App-Reviews)",36,2,harshcasper,2022-05-09 10:48:35,https://www.reddit.com/r/datasets/comments/uloglb/self_promotion_i_scraped_blind_app_and_created_a/,0,datasets
sx2d6e,The Open Industrial Data Project - Oil & Gas Industry Live Dataset,"Live stream data provided by AkerBP oil company through Cognite data platform. Tons of time series and maintenance event data. Really useful anyone who wants to create Industrial IoT use cases with real industrial data.

[https://www.openindustrialdata.com/](https://www.openindustrialdata.com/)",36,3,uerten,2022-02-20 14:25:55,https://www.reddit.com/r/datasets/comments/sx2d6e/the_open_industrial_data_project_oil_gas_industry/,0,datasets
of2pkm,An organized dataset of more than 600 soft law programs directed at AI governance from 64 countries classified in up to 107 variables/themes,,39,0,fuck_your_diploma,2021-07-06 19:55:28,https://docs.google.com/spreadsheets/d/1jfSgQ7L8iBcfb7jXJQtmKxjzBx0J27Ky9NhIJvOamCk/edit?usp=sharing,0,datasets
o3n8uo,"Dataset: A global, georeferenced event dataset on electoral violence with lethal outcomes from 1989 to 2017.",,35,1,smurfyjenkins,2021-06-19 18:58:28,https://journals.sagepub.com/doi/full/10.1177/00220027211021620,0,datasets
ndj81e,Indian Food Dataset,"Find Indian food datasets here: [https://www.kaggle.com/kanan275/indian-food-image-dataset](https://www.kaggle.com/kanan275/indian-food-image-dataset)

&#x200B;

It can be used for calorie counting, food detection and recognition problems.",35,1,Kananvyas2,2021-05-16 07:46:30,https://www.reddit.com/r/datasets/comments/ndj81e/indian_food_dataset/,0,datasets
mjr0hj,Need a chinese resident or someone possessing a baidu account for obtaining a fake news dataset.,"Due to the great firewall, a chinese paper has its dataset locked behind a baidu cloud account that is inaccessible without a baidu account. Looking for some help to get my hands on the dataset.",38,16,thescoobynooby,2021-04-04 06:48:03,https://www.reddit.com/r/datasets/comments/mjr0hj/need_a_chinese_resident_or_someone_possessing_a/,0,datasets
maod7t,CATSAD: A Chinese traffic sign recognition dataset (60k+ images) [self-promotion],,32,0,Section7Curse,2021-03-22 14:04:59,https://github.com/icfaust/CATSAD,0,datasets
kpvl0k,Looking for books that heavily use and present data,"I'm hoping to read more data-driven literature this year, especially where the data is publicly available, and I'm also hoping this would be a good place to ask for book recommendations.

I'm open to most topics – but I do love economics and politics – and I don't mind if the book is for more lay (e.g. *Enlightenment Now* or *More From Less*) or technical (e.g. *Capital in the Twenty-First Century*, *The Financial Economics of Privatisation*, or *Politics, work, and daily life in the USSR*) audiences, or something in between (e.g. *The WEIRDest People in the World* or *Why Civil Resistance Works*). I just want it to spend a lot of time justifying its arguments with reference to the graphs, tables, and statistics it presents. Double points if I can access the raw data to scrutinise or recreate the findings.",36,32,ButterscotchLost3829,2021-01-03 23:17:38,https://www.reddit.com/r/datasets/comments/kpvl0k/looking_for_books_that_heavily_use_and_present/,0,datasets
k39ogh,Looking for datasets about pandemic and violence against women,Hi! I'm looking for datasets about the changes in the frequency of domestic violence/violence against women episodes  during Covid-19 pandemic/lockdown periods. Do you know interesting sources?,37,6,Karsupnow11,2020-11-29 13:50:31,https://www.reddit.com/r/datasets/comments/k39ogh/looking_for_datasets_about_pandemic_and_violence/,0,datasets
k0xoqw,Background and business dealings of US political appointees. Revolving Door | OpenSecrets,,36,1,cavedave,2020-11-25 18:25:57,https://www.opensecrets.org/revolving/index.php,0,datasets
jo6zw1,Americal Sign Language Dataset,"**Dataset ->** [**ASL Alphabet Dataset on Kaggle**](https://www.kaggle.com/grassknoted/asl-alphabet/)

(Great for getting started with **Real-time Image Classification**)

Here is a sign language dataset that contains 87,000 images which are 200x200 pixels.There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING.These 3 classes are very helpful in real-time applications and classification.The test data set contains a mere 29 images, to encourage the use of real-world test images. Feel free to split the 87,000 images from the training set as you need.

Also, here a GitHub repository to convert sign-language to speech, in real-time -> [**Unvoiced**](https://github.com/grassknoted/Unvoiced)

&#x200B;

[Snapshot of the Dataset](https://preview.redd.it/d5hvfsgwtfx51.png?width=728&format=png&auto=webp&s=c033b25e9bbf9eaf3bb7ce3a2b8a4176c565ee47)

[ASL Alphabet Dataset](https://preview.redd.it/xsr8zdbo7bx51.jpg?width=242&format=pjpg&auto=webp&s=51229d73d20c7d11d4c6743f03977a7958c05fda)",35,2,grassknoted,2020-11-04 22:58:38,https://www.reddit.com/r/datasets/comments/jo6zw1/americal_sign_language_dataset/,0,datasets
jgaw1n,I made a modern data catalog tool for anyone using a word document or excel sheet as a data dictionary. I’m curious if anyone would like to try it out!,"At my old job, it seemed like every few weeks I’d have a new project with a new dataset. The hardest part in my job was understanding the data not completing the project. I created a tool to solve this and I’m hoping to find beta users to give it a spin! It’s one location to store meta-data, common questions, and analysis associated with a data table. It requires manual uploads in current state so it’s ideal for smaller teams. 

If anyone is interested in trying it please let me know. :)

The website is datalogz.io",35,32,lblip123,2020-10-22 23:08:16,https://www.reddit.com/r/datasets/comments/jgaw1n/i_made_a_modern_data_catalog_tool_for_anyone/,0,datasets
jflqjg,Releasing a Decade of Forex Tick Data I Crawled and Converted,,38,5,jtimperio,2020-10-21 21:08:27,/r/DataHoarder/comments/jflilw/releasing_a_decade_of_forex_tick_data_i_crawled/,0,datasets
hl5edx,[self-promotion] memes with labels & metadata,,36,2,gmorinan,2020-07-04 15:46:03,https://www.kaggle.com/gmorinan/memes-classified-and-labelled,0,datasets
fsm4qh,I'm doing a Microsoft Excel project for my school and I need people to fill out my quiz for data! Thanks <3 (It is to see if your personality dictates who your favorite smash bros character is),,40,11,InfamousJackson,2020-03-31 21:49:04,https://forms.gle/JT4khpWWRVxe3je4A,0,datasets
flr1xy,Open-Access JSTOR Materials Accessible to the Public,,34,8,cavedave,2020-03-20 07:42:58,http://www.universitytimes.ie/2020/03/jstor-makes-database-accessible-to-the-public/,0,datasets
f9a5x2,Auto-updating Corona virus dataset,,36,1,alfa1381,2020-02-25 13:20:56,/r/Coronavirus/comments/f91ske/autoupdating_queryable_corornavirus_dataset_with/,0,datasets
eu4rhx,How to build a simple web crawler,"Three years ago, I was working as a student assistant in the Institutional Statistics Unit.

At first, my job was to copy and paste the web content and save them in excel files.

However, I discovered a way to automate it and here is what I am going to share with you in this article.

I will share with you step by steps on how to automate it, then you will have the skills to do it yourself too.

Link: [https://towardsdatascience.com/how-to-build-a-simple-web-crawler-66082fc82470?source=friends\_link&sk=b7fd5670e6397736f9e038b930ea1607](https://towardsdatascience.com/how-to-build-a-simple-web-crawler-66082fc82470?source=friends_link&sk=b7fd5670e6397736f9e038b930ea1607)

Share with your friends or colleague if you find it helpful.",37,4,weihong95,2020-01-26 09:39:48,https://www.reddit.com/r/datasets/comments/eu4rhx/how_to_build_a_simple_web_crawler/,0,datasets
eo9iuw,Flavor Bible Dataset,"I'm thinking of creating a network visualization of different flavours based on The Flavor Bible by Karen Page and Andrew Dornenburg. Does anyone have access to a tabular dataset from the book? If not, I'll try to scrape it from a pdf and post it here:)",38,9,schweppes-ginger-ale,2020-01-13 20:11:32,https://www.reddit.com/r/datasets/comments/eo9iuw/flavor_bible_dataset/,0,datasets
cbfifc,A live stream of the entire ImageNet dataset,,36,3,z058,2019-07-10 11:26:08,https://www.youtube.com/watch?v=VG3hpuK2TKw,0,datasets
965dkp,Amazon open-sources dataset for understanding names in different languages,,35,0,cavedave,2018-08-10 09:18:44,https://venturebeat.com/2018/08/09/amazon-open-sources-dataset-for-understanding-names-in-different-languages/,0,datasets
8urjwy,Popcorn analytics • r/dataisbeautiful crosspost,,36,1,cavedave,2018-06-29 08:31:36,https://www.reddit.com/r/dataisbeautiful/comments/8uo1q9/popcorn_analytics_oc/e1hjmbe/,0,datasets
8gxkp2,Academic research papers dataset (including full-text),,37,4,rtk25,2018-05-04 07:56:35,https://github.com/ronentk/sci-paper-miner,0,datasets
7k7l4o,List of Billionaires,,38,7,toothbrushguitar,2017-12-16 15:33:30,https://docs.google.com/spreadsheets/d/1IbqxfAlWOiY1zlW0d5UQcPWqeP_FEmPsirMs0q3fcCU/edit?usp=sharing,0,datasets
7jguhq,FCC Net Neutrality Electronic Comment Filings as of November 3 2017 (zipped JSON),,34,6,caustinbrooks,2017-12-13 04:38:01,https://apps.fcc.gov/edocs_public/attachmatch/DA-17-1089A1.pdf,0,datasets
7at8sj,"A more complete FIFA 18 player dataset (~18000 players, 183 fields per player)",,38,0,caoimhin_o_h,2017-11-04 20:56:00,https://www.kaggle.com/kevinmh/fifa-18-more-complete-player-dataset,0,datasets
6p17rr,Created a dataset of a million plus headlines with date from abc news,,37,3,therohk,2017-07-23 13:12:23,https://www.kaggle.com/therohk/million-headlines,0,datasets
6m0pq3,Creating joke dataset,"EDIT: The created dataset can be found on this [GitHub repository](https://github.com/TWinters/JokeJudger-Data).

Hi /r/datasets!

I'm a computer science student currently researching humour theory and how to generate humor with computers.
For this, I'm creating a dataset with human rated ""I like my X like I like my Y, Z"" jokes.
I intend to release this anonymised dataset after I finish my thesis on it next month such that other researchers can also benefit from this dataset!

In order to gather this training data, I created a website called [JokeJudger.com](http://jokejudger.com) where you can **rate and create jokes**. It also aims to help the joke creators by giving them anonymous feedback from other users as a reward. There are also mechanisms in place to [generate challenges](http://www.jokejudger.com/#/create), and even a suggestion system to help with associations.

I'm currently looking for more people to help me out with this. If you'd like to help and [create](http://www.jokejudger.com/#/create)/[judge](http://www.jokejudger.com/#/rate) some jokes on the [site](http://jokejudger.com) and thus improve the quality of this future dataset, that'd be greatly appreciated!

Either way: if you're interested in this dataset, come back in a month or two to this post, as I will probably have linked it here by then!

Thanks for your attention!",35,17,thomaswint,2017-07-08 12:28:08,https://www.reddit.com/r/datasets/comments/6m0pq3/creating_joke_dataset/,0,datasets
6lqbsd,Downloading Reddit,"Hello,

Would you happen to know if it's still possible to ""download Reddit""? I found the following thread from 2015:
https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/

But I'm looking for 2007-present data for a research paper.

I'm also interested in Twitter and Facebook. I know you can obtain data from the prior with tools like Crimson Hexagon (I'm not sure if there are better alternatives) while access to the latter is limited.

Either way, thank you for your help.",36,16,NobleWhale,2017-07-07 01:13:25,https://www.reddit.com/r/datasets/comments/6lqbsd/downloading_reddit/,0,datasets
6gvpsf,California Finally Releases Wiretap Dataset,,37,3,surlyq,2017-06-12 22:33:23,https://www.eff.org/files/2017/06/09/california_wt2_export_2016.xlsx,0,datasets
6aszh9,White House Visitor Logs from Politico,,36,2,DataScienceInc,2017-05-12 18:12:20,https://datalab.politico.com/databases/white-house-visitor-logs/api/docs/,0,datasets
5iqygk,The Best Way to Prepare a Dataset Easily,,37,0,llSourcell,2016-12-16 21:49:48,https://www.youtube.com/watch?v=0xVqLJe9_CY&feature=youtu.be,0,datasets
5bn0v1,"~4,000 accused Scottish witches 1563 - 1736",,36,4,datadotworld,2016-11-07 16:17:20,https://data.world/history/scottish-witchcraft,0,datasets
4z2571,Pennsylvania Launches OpenDataPA,,36,3,endthestory,2016-08-22 18:58:43,https://data.pa.gov/,0,datasets
3jnknt,"A Plethora of Open Data Repositories (i.e., thousands!)",,33,0,nyike,2015-09-04 19:04:14,http://www.datasciencecentral.com/profiles/blogs/a-plethora-of-open-data-repositories-i-e-thousands,0,datasets
3emyfy,"The infamous ""Reddit 2015 Blackout"" dataset is available! Dive in and analyze what happened during that period.","I have made available all public Reddit comments from July 1 - July 8 that covers the Reddit Blackout beginning July 2.  I hope some of you can do interesting analysis on this data!  Specifically:

**How was Reddit comment activity affected?**

**When was the first mention of Victoria's situation?**

**How often was Victoria mentioned in comments over this time span?**

**Exactly how many subreddits went dark and for how long?**

**What words were associated with Pao and Alexis during that period?**

Get to it data lovers!  I hope to see something in /r/dataisbeautiful very soon!

**Link:  http://files.pushshift.io/reddit/RC_blackout.gz**

**Magnet Link: magnet:?xt=urn:btih:9a7444056c8fc51b2f0653dae403cf8e9f4994cf&dn=RC_blackout.gz** *(Thanks to /u/hak8or)*

*(This file is compressed with gzip so that it can more easily be ingested into Google's BigQuery -- thanks again /u/fhoffa!)*

",37,8,Stuck_In_the_Matrix,2015-07-26 06:23:52,https://www.reddit.com/r/datasets/comments/3emyfy/the_infamous_reddit_2015_blackout_dataset_is/,0,datasets
rfil3q,Looking for Spotify Global Dataset 2021,"ISO Spotify's massive dataset with music from all over the world for 2021. Its been easy to find in the past but I'm coming up short this year. 

I teach a university class that involves creating pivot tables in Excel, and for years I've been using Spotify's year end dataset for the last test. It's huge so impossible to find answers any other way, and being about music helps keep their interest in what's usually a pretty boring topic.

Edit: for example, a question in the past has been 'which genre has the highest danceability score?'",37,11,kjosness,2021-12-13 15:44:43,https://www.reddit.com/r/datasets/comments/rfil3q/looking_for_spotify_global_dataset_2021/,0,datasets
q5hlw5,"NLP beginner dataset for text classification, sentiment analysis and/or NER","Hi, everyone! Any NLP experts in the room? As the title says, I'm looking for a good dataset for my first NLP projects. I've been studying and practicing spaCy and someone I know recommended starting with TextCat, sentiment analysis and training a NER, but I'm having trouble finding a dataset. Thanks in advance to anyone who can provide some guidance!",35,8,gotchab003,2021-10-10 21:51:12,https://www.reddit.com/r/datasets/comments/q5hlw5/nlp_beginner_dataset_for_text_classification/,0,datasets
zw6lyy,Advice for finding a detailed dataset on stocks listed in the S&P 500?,"I need to find a dataset for a college course. I'm interested in finance and would like to do some exploratory analysis on S&P 500 stocks. I've already found something that is almost perfect for what I want to do:  [S&P 500 Companies with Financial Information | Kaggle](https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information)

  
However, I would prefer a dataset of the exact same format but that has even more columns. I would especially like to have more categorical columns, since the only truly categorical one from this one is the sector. I know that there are a lot more columns that could make sense for such a dataset, such as historical return over the past N years, or other financials. So I really do feel like there must be some dataset out there that looks exactly like this but has even more columns.

I'm just having trouble with finding such a dataset. So far I've just been googling something like ""dataset of s&p 500 stocks with financial information"" or ""dataset of s&p 500 stocks with many columns,"" but I haven't really found anything better yet by doing so.

I would appreciate any suggestions.",23,2,anonymous_palindrome,2022-12-27 04:30:41,https://www.reddit.com/r/datasets/comments/zw6lyy/advice_for_finding_a_detailed_dataset_on_stocks/,0,datasets
b30288,Every line from every episode of The Office,,21,6,misunderstoodpoetry,2019-03-19 17:13:45,https://docs.google.com/spreadsheets/d/18wS5AAwOh8QO95RwHLS95POmSNKA2jjzdt0phrxeAE0/edit?usp=sharing,0,datasets
y264aj,Does anyone know a good Statista alternative?,"I've never been able to find good competition for Statista.com

My work requires me to use various data/stats for decision-making.

Statista is quite useful, and it is possible to buy an account for not that much (I need to pay from my own pocket...) $468 net per year.

But maybe there's something better?

&#x200B;

Market: Mobile devices, mobile os, mobile apps etc.",19,18,saurgalen,2022-10-12 14:56:34,https://www.reddit.com/r/datasets/comments/y264aj/does_anyone_know_a_good_statista_alternative/,0,datasets
