id,title,selftext,score,num_comments,author,created_utc,url,gilded,subreddit
mkfsfi,"Big Data Pipeline on AWS, Microsoft Azure, and Google Cloud",,262,21,satishcgupta,2021-04-05 08:23:32,https://i.redd.it/82cgd6zaebr61.png,0,bigdata
z0qt73,"Big Data Pipelines on AWS, Azure, and Google Cloud (v2022)",,154,12,satishcgupta,2022-11-21 05:42:24,https://i.redd.it/hd7n9igsr81a1.png,0,bigdata
qq35ab,k-Means clustering: Visually explained,,128,11,Va_Linor,2021-11-09 12:59:42,https://v.redd.it/urt5l3sgiky71,0,bigdata
c5189o,Big Data,,109,6,FunkyDoktor,2019-06-25 02:53:24,https://i.redd.it/i945mic43f631.jpg,0,bigdata
icyh6p,I've compiled a list of the best books on data science (see comments for the list),,95,5,vhpoet,2020-08-19 22:56:23,https://i.redd.it/u1lb24jdh1i51.png,0,bigdata
v69kmi,Saw that today and it made me laugh 😀,,91,8,ioah86,2022-06-06 18:02:31,https://i.redd.it/4iz4ge92j1491.jpg,0,bigdata
q18zde,Do you agree?,,89,16,D3Vtech,2021-10-04 16:29:32,https://i.redd.it/egxczjvwmgr71.png,0,bigdata
ns4lfg,The age-old spinach data problem,,91,1,lblip123,2021-06-04 13:01:48,https://i.redd.it/rg3r6xmuy8371.png,0,bigdata
ca521f,Matt Tuck Big data Landscape,,84,12,inboundmage,2019-07-07 09:30:04,http://mattturck.com/wp-content/uploads/2019/07/2019_Matt_Turck_Big_Data_Landscape_2019v2_Fullsize.png,0,bigdata
mkbfhw,Realities Of Being A Data Engineer,"As the demand continues to grow for data engineers I wanted to cover a few expectations I had going into being a data engineer.

Just in case some people on this thread are thinking about becoming data engineers themselves. Also, it would be great to discuss other challenges you might be facing becoming a data engineer. Also, I added a timestamp to when I reference this in a recent video I put up. But the points are the exact same in the video. So feel free to just discuss here as well!

1. I assumed most companies had already switched over to tools like Hadoop, Bigquery, Spark, etc. You read enough articles and it seems like you will need to be familiar with all of these technologies. However, I have found plenty of companies still store their data in Oracle, SQL Server, and Postgres...and...it works. There is no need for them to switch. In fact when I interviewed at  a large tech company and I told them I wanted to learn more about distributed systems and streaming. They asked me about the downsides of the tech. [0:48](https://www.youtube.com/watch?v=6RiA_Qur2yo&t=48s)​ Hadoop And Spark?
2. Data engineering salary - I assumed I would make 100k after a few years. However, I quickly was able to make a lot more than that by getting hired in big tech. In fact you could make 2x that at the right company. Now SWEs always make more. But 200k is a decent amount even in the bay. [6:15](https://www.youtube.com/watch?v=6RiA_Qur2yo&t=375s)​ What to expect for a data engineering salary?
3. I assumed most of my work would be creating data pipelines. But I have done at least 3 migrations in the last 3-4 years. So you will spend a lot of time just replicating functionality in a different system or design process. [3:04](https://www.youtube.com/watch?v=6RiA_Qur2yo&t=184s)​ Isn't it All About Data Pipelines
4. Data engineers get no fanfare. As much as most of the work the analyst and data data scientist do rely on data engineers. We are always in the background. We don't get to present our findings or talk about our impact that much. Personally I kind of like being behind the scenes [5:02](https://www.youtube.com/watch?v=6RiA_Qur2yo&t=302s)​ All The Focus Is Not On Data Engineers

What were your expectations vs reality for being a data engineer or working with big data.

&#x200B;

Edit: I don't think I have ever gotten a reward before...THANK YOU.

&#x200B;

Edit 2: Thank you for the silver on the BI subreddit!",80,15,nonkeymn,2021-04-05 03:14:22,https://www.reddit.com/r/bigdata/comments/mkbfhw/realities_of_being_a_data_engineer/,0,bigdata
14s0wq1,Data Truth!,,71,3,Sailja_Jain,2023-07-06 06:58:16,https://i.redd.it/2av5hc84maab1.jpg,0,bigdata
gezig6,Big Data Engineers Path,,70,10,TheInsaneApp,2020-05-07 04:05:37,https://i.redd.it/zua58yhyo9x41.png,0,bigdata
8ud9f5,Big Data Landscape 2018,,69,9,rsinghanalytics,2018-06-27 20:53:03,http://mattturck.com/wp-content/uploads/2018/06/Matt-Turck-FirstMark-Big-Data-Landscape-2018.png,0,bigdata
2vl13s,Google BigQuery is ridiculously fast. Just queries 3TB in 7 seconds in plain SQL.,,65,11,tinoter,2015-02-11 22:05:20,http://i.imgur.com/9Q6uZ9D.png,0,bigdata
124rf3g,Big news! LambdaConf returns Sept 16-19th and is better than ever! 🔥,"Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)",59,1,Agataziverge,2023-03-28 14:59:34,https://www.reddit.com/r/bigdata/comments/124rf3g/big_news_lambdaconf_returns_sept_1619th_and_is/,0,bigdata
116ax6a,What People Are Missing About Microsoft’s $10B Investment In OpenAI,"&#x200B;

[Sam Altman Might Have Just Pulled Off The Coup Of The Decade](https://preview.redd.it/daoewwski5ja1.png?width=720&format=png&auto=webp&s=2eec23c4cadb1697314f0225fd4485ddacabc601)

Microsoft is investing $10B into OpenAI!

There is lots of frustration in the community about OpenAI not being all that open anymore. They appear to abandon their ethos of developing AI for everyone, [free](https://openai.com/blog/introducing-openai/) of economic pressures.

The fear is that OpenAI’s models are going to become fancy MS Office plugins. Gone would be the days of open research and innovation.

However, the specifics of the deal tell a different story.

To understand what is going on, we need to peek behind the curtain of the tough business of machine learning. We will find that Sam Altman might have just orchestrated the coup of the decade!

To appreciate better why there is some three-dimensional chess going on, let’s first look at Sam Altman’s backstory.

*Let’s go!*

A Stellar Rise

Back in 2005, Sam Altman founded [Loopt](https://en.wikipedia.org/wiki/Loopt) and was part of the first-ever YC batch. He raised a total of $30M in funding, but the company failed to gain traction. Seven years into the business Loopt was basically dead in the water and had to be shut down.

Instead of caving, he managed to sell his startup for $[43M](https://golden.com/wiki/Sam_Altman-J5GKK5) to the finTech company [Green Dot](https://www.greendot.com/). Investors got their money back and he personally made $5M from the sale.

By YC standards, this was a pretty unimpressive outcome.

However, people took note that the fire between his ears was burning hotter than that of most people. So hot in fact that Paul Graham included him in his 2009 [essay](http://www.paulgraham.com/5founders.html?viewfullsite=1) about the five founders who influenced him the most.

He listed young Sam Altman next to Steve Jobs, Larry & Sergey from Google, and Paul Buchheit (creator of GMail and AdSense). He went on to describe him as a strategic mastermind whose sheer force of will was going to get him whatever he wanted.

And Sam Altman played his hand well!

He parleyed his new connections into raising $21M from Peter Thiel and others to start investing. Within four years he 10x-ed the money \[2\]. In addition, Paul Graham made him his successor as president of YC in 2014.

Within one decade of selling his first startup for $5M, he grew his net worth to a mind-bending $250M and rose to the circle of the most influential people in Silicon Valley.

Today, he is the CEO of OpenAI — one of the most exciting and impactful organizations in all of tech.

However, OpenAI — the rocket ship of AI innovation — is in dire straights.

OpenAI is Bleeding Cash

Back in 2015, OpenAI was kickstarted with $1B in donations from famous donors such as Elon Musk.

That money is long gone.

In 2022 OpenAI is projecting a revenue of $36M. At the same time, they spent roughly $544M. Hence the company has lost >$500M over the last year alone.

This is probably not an outlier year. OpenAI is headquartered in San Francisco and has a stable of 375 employees of mostly machine learning rockstars. Hence, salaries alone probably come out to be roughly $200M p.a.

In addition to high salaries their compute costs are stupendous. Considering it cost them $4.6M to train GPT3 once, it is likely that their cloud bill is in a very healthy nine-figure range as well \[4\].

So, where does this leave them today?

Before the Microsoft investment of $10B, OpenAI had received a total of $4B over its lifetime. With $4B in funding, a burn rate of $0.5B, and eight years of company history it doesn’t take a genius to figure out that they are running low on cash.

It would be reasonable to think: OpenAI is sitting on ChatGPT and other great models. Can’t they just lease them and make a killing?

Yes and no. OpenAI is projecting a revenue of $1B for 2024. However, it is unlikely that they could pull this off without significantly increasing their costs as well.

*Here are some reasons why!*

The Tough Business Of Machine Learning

Machine learning companies are distinct from regular software companies. On the outside they look and feel similar: people are creating products using code, but on the inside things can be very different.

To start off, machine learning companies are usually way less profitable. Their gross margins land in the 50%-60% range, much lower than those of SaaS businesses, which can be as high as 80% \[7\].

On the one hand, the massive compute requirements and thorny data management problems drive up costs.

On the other hand, the work itself can sometimes resemble consulting more than it resembles software engineering. Everyone who has worked in the field knows that training models require deep domain knowledge and loads of manual work on data.

To illustrate the latter point, imagine the unspeakable complexity of performing content moderation on ChatGPT’s outputs. If OpenAI scales the usage of GPT in production, they will need large teams of moderators to filter and label hate speech, slurs, and tutorials on killing people, you name it.

*Alright, alright, alright! Machine learning is hard.*

*OpenAI already has ChatGPT working. That’s gotta be worth something?*

Foundation Models Might Become Commodities:

In order to monetize GPT or any of their other models, OpenAI can go two different routes.

First, they could pick one or more verticals and sell directly to consumers. They could for example become the ultimate copywriting tool and blow [Jasper](https://app.convertkit.com/campaigns/10748016/jasper.ai) or [copy.ai](https://app.convertkit.com/campaigns/10748016/copy.ai) out of the water.

This is not going to happen. Reasons for it include:

1. To support their mission of building competitive foundational AI tools, and their huge(!) burn rate, they would need to capture one or more very large verticals.
2. They fundamentally need to re-brand themselves and diverge from their original mission. This would likely scare most of the talent away.
3. They would need to build out sales and marketing teams. Such a step would fundamentally change their culture and would inevitably dilute their focus on research.

The second option OpenAI has is to keep doing what they are doing and monetize access to their models via API. Introducing a [pro version](https://www.searchenginejournal.com/openai-chatgpt-professional/476244/) of ChatGPT is a step in this direction.

This approach has its own challenges. Models like GPT do have a defensible moat. They are just large transformer models trained on very large open-source datasets.

As an example, last week Andrej Karpathy released a [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) of him coding up a version of GPT in an afternoon. Nothing could stop e.g. Google, StabilityAI, or HuggingFace from open-sourcing their own GPT.

As a result GPT inference would become a common good. This would melt OpenAI’s profits down to a tiny bit of nothing.

In this scenario, they would also have a very hard time leveraging their branding to generate returns. Since companies that integrate with OpenAI’s API control the interface to the customer, they would likely end up capturing all of the value.

An argument can be made that this is a general problem of foundation models. Their high fixed costs and lack of differentiation could end up making them akin to the [steel industry](https://www.thediff.co/archive/is-the-business-of-ai-more-like-steel-or-vba/).

To sum it up:

* They don’t have a way to sustainably monetize their models.
* They do not want and probably should not build up internal sales and marketing teams to capture verticals
* They need a lot of money to keep funding their research without getting bogged down by details of specific product development

*So, what should they do?*

The Microsoft Deal

OpenAI and Microsoft [announced](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/) the extension of their partnership with a $10B investment, on Monday.

At this point, Microsoft will have invested a total of $13B in OpenAI. Moreover, new VCs are in on the deal by buying up shares of employees that want to take some chips off the table.

However, the astounding size is not the only extraordinary thing about this deal.

First off, the ownership will be split across three groups. Microsoft will hold 49%, VCs another 49%, and the OpenAI foundation will control the remaining 2% of shares.

If OpenAI starts making money, the profits are distributed differently across four stages:

1. First, early investors (probably Khosla Ventures and Reid Hoffman’s foundation) get their money back with interest.
2. After that Microsoft is entitled to 75% of profits until the $13B of funding is repaid
3. When the initial funding is repaid, Microsoft and the remaining VCs each get 49% of profits. This continues until another $92B and $150B are paid out to Microsoft and the VCs, respectively.
4. Once the aforementioned money is paid to investors, 100% of shares return to the foundation, which regains total control over the company. \[3\]

What This Means

This is absolutely crazy!

OpenAI managed to solve all of its problems at once. They raised a boatload of money and have access to all the compute they need.

On top of that, they solved their distribution problem. They now have access to Microsoft’s sales teams and their models will be integrated into MS Office products.

Microsoft also benefits heavily. They can play at the forefront AI, brush up their tools, and have OpenAI as an exclusive partner to further compete in a [bitter cloud war](https://www.projectpro.io/article/aws-vs-azure-who-is-the-big-winner-in-the-cloud-war/401) against AWS.

The synergies do not stop there.

OpenAI as well as GitHub (subsidiary of Microsoft) e. g. will likely benefit heavily from the partnership as they continue to develop[ GitHub Copilot](https://github.com/features/copilot).

The deal creates a beautiful win-win situation, but that is not even the best part.

Sam Altman and his team at OpenAI essentially managed to place a giant hedge. If OpenAI does not manage to create anything meaningful or we enter a new AI winter, Microsoft will have paid for the party.

However, if OpenAI creates something in the direction of AGI — whatever that looks like — the value of it will likely be huge.

In that case, OpenAI will quickly repay the dept to Microsoft and the foundation will control 100% of whatever was created.

*Wow!*

Whether you agree with the path OpenAI has chosen or would have preferred them to stay donation-based, you have to give it to them.

*This deal is an absolute power move!*

I look forward to the future. Such exciting times to be alive!

As always, I really enjoyed making this for you and I sincerely hope you found it useful!

*Thank you for reading!*

Would you like to receive an article such as this one straight to your inbox every Thursday? Consider signing up for **The Decoding** ⭕.

I send out a thoughtful newsletter about ML research and the data economy once a week. No Spam. No Nonsense. [Click here to sign up!](https://thedecoding.net/)

**References:**

\[1\] [https://golden.com/wiki/Sam\_Altman-J5GKK5](https://golden.com/wiki/Sam_Altman-J5GKK5)​

\[2\] [https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny](https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny)​

\[3\] [Article in Fortune magazine ](https://fortune.com/2023/01/11/structure-openai-investment-microsoft/?verification_code=DOVCVS8LIFQZOB&_ptid=%7Bkpdx%7DAAAA13NXUgHygQoKY2ZRajJmTTN6ahIQbGQ2NWZsMnMyd3loeGtvehoMRVhGQlkxN1QzMFZDIiUxODA3cnJvMGMwLTAwMDAzMWVsMzhrZzIxc2M4YjB0bmZ0Zmc0KhhzaG93T2ZmZXJXRDFSRzY0WjdXRTkxMDkwAToMT1RVVzUzRkE5UlA2Qg1PVFZLVlpGUkVaTVlNUhJ2LYIA8DIzZW55eGJhajZsWiYyYTAxOmMyMzo2NDE4OjkxMDA6NjBiYjo1NWYyOmUyMTU6NjMyZmIDZG1jaOPAtZ4GcBl4DA)​

\[4\] [https://arxiv.org/abs/2104.04473](https://arxiv.org/abs/2104.04473) Megatron NLG

\[5\] [https://www.crunchbase.com/organization/openai/company\_financials](https://www.crunchbase.com/organization/openai/company_financials)​

\[6\] Elon Musk donation [https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research](https://www.inverse.com/article/52701-openai-documents-elon-musk-donation-a-i-research)​

\[7\] [https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/](https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software-2/)",57,4,LesleyFair,2023-02-19 13:59:31,https://www.reddit.com/r/bigdata/comments/116ax6a/what_people_are_missing_about_microsofts_10b/,0,bigdata
xludlh,What's bad data?,,60,1,Ok-Put-4951,2022-09-23 11:00:25,https://i.redd.it/iyznfwasalp91.png,0,bigdata
eu73vm,Google just published 25 million free datasets,,59,5,superconductiveKyle,2020-01-26 14:09:19,https://link.medium.com/tKSfTtk5y3,0,bigdata
n5ire6,"Researchers found that accelerometer data from smartphones can reveal people's location, passwords, body features, age, gender, level of intoxication, driving style, and be used to reconstruct words spoken next to the device.",,57,1,bayashad,2021-05-05 15:35:38,https://i.redd.it/b8bpryt35bx61.png,0,bigdata
mttt65,Online Courses And Youtube Channels I Used To Become A Data Engineer,"Oddly enough I wrote an article over a year ago about some great data engineering courses and channels that you can use to become a data engineer(Or at least the courses I used). 

The odd part being that when I started a youtube channel, I got asked for similar content. So I put together a video (Without affiliate links ;) ) to cover the youtube channels and courses I used. 

Here were some of my favorites. I didn't include the Google Data Engineer course because I will likely put together a separate video for that. 

Also, feel free to let me know what courses you took or would recommend! 

[0:46](https://www.youtube.com/watch?v=lVj0RlSxTXk&t=46s)​ - Youtube: WiseOwl - When I first started working in data, I worked at a company that was focused on the Microsoft stack. In particular, the older, SSIS, SSRS and standard SQL Server. So WiseOwl provided a lot of learning in terms of specific tooling. I will add that they don't really teach anything in terms of good practices. These videos are generally just specific how-tos for basic concepts like transformations and SQL.

[4:06](https://www.youtube.com/watch?v=lVj0RlSxTXk&t=246s)​ -  Udemy: Data Warehouse Fundamentals for Beginners - As mentioned in the previous section above, WiseOwl only really focuses on how-tos for specific issues. Really, its great for those problems you google. Like ""How do I use pivot on SQL Server"". Something very specific.

When it comes to really going beyond and thinking from a higher level. You will need to learn about data warehousing and ETL design(Even if these are changing in terms of best practices). Understanding what ETLS and data warehouses are can drastically help you understand the big picture. Like, why are companies look to ELTs vs ETLs or Data Lakehouses vs Data Lakes. Understanding these changes requires you know where data engineering started from. 

 [3:16](https://www.youtube.com/watch?v=lVj0RlSxTXk&t=196s)​ - Youtube: Airflow Apache w/Tuan Vu  - I skipped over this channel to discuss the high level data warehousing course. But, if you're looking for another free option. Tuan Vu's apache airflow playlist is great if you're trying to get up to speed on airflow fast. It's a great way to practice setting up airflow on a cloud VM. 

[6:40](https://www.youtube.com/watch?v=lVj0RlSxTXk&t=400s)​ - Udemy: Big Data on Amazon web services (AWS) - Once you have the basics down. You probably want to start  to learn about the big data tools you can use and where they are useful. This is where this course comes in handy. The Udemy Big Data AWS course is very helpful in letting you see what tools you can use and what they do. Also, you can assume that if it exists on AWS there is a similar tool on Azure or GCP. 

[8:43](https://www.youtube.com/watch?v=lVj0RlSxTXk&t=523s)​ - Udemy: Taming Big Data with Apache Spark and Python - Hands On!  - Now if you want more specific big data courses I think any course by Frank Kane is great. For example, I am still working through this course and its been a good mix of hands on and high level work.",59,11,nonkeymn,2021-04-19 05:23:11,https://www.reddit.com/r/bigdata/comments/mttt65/online_courses_and_youtube_channels_i_used_to/,0,bigdata
xw7st9,Apache Iceberg Reduced Our Amazon S3 Cost by 90%,,58,6,dnzprmksz,2022-10-05 11:15:18,https://medium.com/insiderengineering/apache-iceberg-reduced-our-amazon-s3-cost-by-90-997cde5ce931,0,bigdata
dveafo,10 billion/2.1 TB YouTube comments,"I crawled YouTube comments for my friends that are into big data. Well, I got an unexpectedly large dataset together.Because of the enormous size, I haven't had time to analyze video and author coverage yet, unfortunately.

Hope you enjoy. Seeders as always appreciated.

More Info: [https://files.mine.terorie.dev/yt/](https://files.mine.terorie.dev/yt/)

`magnet:?xt=urn:btih:18bc22ee0017fb056794f3d7821a942b5c08cc91&dn=YT%5FCOMMENTS%5FTERORIE%5F2019%5F10.ndjson.zst`

*Update:* Added list of the 576,551,936 unique author IDs who posted all those comments.  
*Update #2*: Direct download link is available!",50,15,terorie,2019-11-12 18:45:29,https://www.reddit.com/r/bigdata/comments/dveafo/10_billion21_tb_youtube_comments/,0,bigdata
11xisa7,"This subreddit is littered with low effort blogs and self promotion, to the point of uselessness. There’s essentially no discussion or learning to be had.",,54,12,theArtOfProgramming,2023-03-21 14:38:56,https://www.reddit.com/r/bigdata/comments/11xisa7/this_subreddit_is_littered_with_low_effort_blogs/,0,bigdata
vsirkq,"Iceberg + Spark + Trino + Dagster: modern, open-source data stack installation","I created a docker-compose based installation of a data stack with Iceberg, Spark, Trino, Dagster, and more. I've already delivered two data projects with it and I love it!  Feel free to use it too. Read this [short description](https://zsvoboda.medium.com/modern-data-stack-demo-5d75dcdfba50) for more details and installation steps. Enjoy!",54,13,zdsvoboda,2022-07-06 05:52:49,https://www.reddit.com/r/bigdata/comments/vsirkq/iceberg_spark_trino_dagster_modern_opensource/,0,bigdata
bbiy00,Top 12 Interesting Career in Big Data Field,,52,10,raj11113,2019-04-10 06:54:55,https://i.redd.it/s0htlvquwdr21.jpg,0,bigdata
6y66vi,"Harvard student noted that search requests on ""iphone slow"" spike when new model announced, not so for other makers.",,50,13,weev1,2017-09-05 06:23:34,https://www.nytimes.com/2014/07/27/upshot/hold-the-phone-a-big-data-conundrum.html,0,bigdata
jucwjc,relatable,,52,1,allende1973,2020-11-15 00:50:59,https://v.redd.it/13z0vvhb38z51,0,bigdata
g130k1,The Different Types of Data Analytics,,52,2,TonTona,2020-04-14 10:20:13,https://sonasoft.com/different-types-of-data-analytics/,0,bigdata
mmtkus,Big Data infrastructure alternatives on cloud,,50,10,satishcgupta,2021-04-08 14:55:57,https://i.redd.it/ksyevsr5ryr61.png,0,bigdata
ecq2mx,Finland is making its online AI crash course free to the world,,51,1,Curio_ser,2019-12-19 07:43:46,https://www.theverge.com/2019/12/18/21027840/online-course-basics-of-ai-finland-free-elements,0,bigdata
k9ve9v,Made these for our BI director. You feel me?,,50,4,PracticallyUncommon,2020-12-09 16:49:20,https://i.redd.it/ukzzqc14y6461.jpg,0,bigdata
mprc34,Data Engineering Hierarchy Of Skill Sets,"For some reason.

Although there are a thousand different articles on what skills a data engineer needs

Here is what I would recommend. 

You learn your skills in layers. 

Starting with a solid base and moving onto more specific skills.

I put together an image and a video to display my thoughts. 

&#x200B;

https://preview.redd.it/jogy9yrqzts61.png?width=1920&format=png&auto=webp&s=8ee129d71203d9da74f4433272d73259c0b54f84

Each of these skills tends to build on each other and you in no way need to master one before moving onto the next one. 

I also created a video to go along with this slide so you can hear me talk a little more about what 

1. [2:08](https://www.youtube.com/watch?v=LgSHaOvNodA&t=128s)​ Python And SQL - Can you technically use drag and drop tools and other low-code no-code options. Sure, but I feel like knowing a solid baseline of SQL and coding is a good first layer for any technologist. Whether you decide to become a programmer, data scientist, or data engineer.

2. [3:25](https://www.youtube.com/watch?v=LgSHaOvNodA&t=205s)​ ETL/ELT and Data Warehousing/Data Lakes - Learning about ETLs, data warehouses, and data lakes tends to start to define a person's skill set as a data engineer. These skills have two aspects. The theoretical/design side and the practical side. There is a lot to learn in this space. Much of it will need to be in a company. However, you can get a good base through reading books and taking a course on Coursera. 

3. [5:14](https://www.youtube.com/watch?v=LgSHaOvNodA&t=314s)​ Cloud, DevOps, and Data Viz - So this next set of skills can be learned pretty much with ETLs and data warehouses. Of course, you might need to break up learning about steps 2 and 3 because it's just a lot of information to take on. 

You can work on developing your data warehouse/ETLs in the cloud while using Git and other tools to improve your deployment process. But that might be a little much.

4. [6:49](https://www.youtube.com/watch?v=LgSHaOvNodA&t=409s)​ Specialize In A Specific Skill Like - Streaming, Azure, Distributed Computing, Etc - What I noticed is that at a certain point a certain percentage of data engineers start to pick a stack they enjoy. 

For example, sometimes I run into engineers that only build on Azure. It can make a lot of sense since so many companies utilize Microsoft. However, I find it confusing as I enjoy having e general skill set. 

But you can still focus on learning other skills like distributed computing and streaming. These two skills are less about specializing and more about waiting until you have finished steps 1,2 and 3 before rushing to them. 

5. I didn't cover this in the video or on the pyramid. But for a fifth skill set, the focus would be softer skills. You can work on this throughout all the steps because you're constantly improving this. 

These would be skills like ownership, project management, communication, and a sense of impact. All of these can help you take the skills from layers 1-4 and amplify them. 

Hopefully, this helps someone in terms of what skills are worth learning as a data engineer.",44,16,nonkeymn,2021-04-13 00:38:53,https://www.reddit.com/r/bigdata/comments/mprc34/data_engineering_hierarchy_of_skill_sets/,0,bigdata
5z1ar6,The Free 'Big Data' Sources Everyone Should Know,,43,0,pmz,2017-03-12 22:08:42,http://www.bigdatanews.com/profiles/blogs/the-free-big-data-sources-everyone-should-know,0,bigdata
wope5l,BigData Hadoop and Spark Analytics Projects (End to End) Tutorials,"Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Bigdata Hadoop Projects:***

1) [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)

2) [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)

3) [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)

4) [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)

5) [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

***Apache Spark Analytics Projects***

1) [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)

2) [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)

3) [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)

4) [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)

5) [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

I hope you'll enjoy these tutorials.",44,3,bigdataengineer4life,2022-08-15 03:47:21,https://www.reddit.com/r/bigdata/comments/wope5l/bigdata_hadoop_and_spark_analytics_projects_end/,0,bigdata
9l58yk,Cloudera and Hortonworks announce merger,,45,2,PineappleOnPizzaSin,2018-10-03 20:53:51,https://www.streetinsider.com/dr/news.php?id=14671788&gfv=1,0,bigdata
kxwns5,Modern Data Engineer Roadmap 2021,,43,14,alexandraabbas,2021-01-15 15:24:16,https://github.com/datastacktv/data-engineer-roadmap,0,bigdata
dtdkep,Hadoop Ecosystem,,45,20,mrcihandemir,2019-11-08 10:29:21,https://i.redd.it/lsvgmvycwfx31.png,0,bigdata
c0a0ro,Is Apache Hadoop dying? Is it already dead?,"I know this is a tired question and it's been discussed to death. But, please bear with me. I have a few pointed questions. I'm an undergraduate student trying to decide where I should look for my first job and I'm trying to understand the enterprise big data landscape now and going forward.

&#x200B;

* Should I invest time to learn the Hadoop (Cloudera/Hortonworks) ecosystem? Will there be use cases for it in the next couple years, or is there a world where businesses transition entirely to other stacks?
* Will Hadoop transition successfully to the cloud (like Cloudera Data Platform)?",42,14,them_russians,2019-06-13 19:12:37,https://www.reddit.com/r/bigdata/comments/c0a0ro/is_apache_hadoop_dying_is_it_already_dead/,0,bigdata
578y25,Machine Learning Algorithm Identifies Tweets Sent Under the Influence of Alcohol,,40,0,steccami,2016-10-13 06:35:05,http://www.datasciencecentral.com/profiles/blogs/machine-learning-algorithm-identifies-tweets-sent-under-the-1?utm_content=buffer8f67f&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,0,bigdata
134a27f,How MotherDuck attempted to astroturf r/bigdata with a manufactured controversy,"In a now-deleted post at
https://www.reddit.com/r/bigdata/comments/12l00df/debate_is_big_data_dead/ (still available via https://www.unddit.com/r/bigdata/comments/12l00df/debate_is_big_data_dead/ for you to judge yourself), a user named ryguyrgrg posted about some monumental shifts in the future of Big Data. Apparently a senior Google Big Data engineer had published an article proclaiming that Big Data was Dead, and had gotten a lot of traction with it. It was certainly seen as an important enough paper for UC Berkeley to take notice, with one of their professors penning a public response. But what to believe? University of Chicago to the rescue, sending a professor to moderate a public debate between the two. 

Except the whole affair was staged. A fake. A  ploy. 

 The supposed Google engineer was actually the ex-Google founder of a company called MotherDuck. The original article wasn’t an impartial analysis of the Big Data landscape, but an piece of marketing material for MotherDuck’s DuckDB-related commercial product. The UC Chicago professor moderator was also a MotherDuck employee. The Berkeley professor wasn’t an outraged bystander but a long-time friend, who incidentally had a company of his own to shill. And as for reddit user 
ryguyrgrg? He hadn’t identified himself or declared any interest, but a little digging online based on some older reddit posts he’d made from a previous job revealed him to be a co-founder of… MotherDuck. 

Now, are MotherDuck liars? Nothing in the wording of their press release is technically false, and most of what I revealed above I know because they publicly declared it elsewhere. Yet, they clearly expended a lot of effort making sure every sentence of that reddit press release was carefully worded to conceal their involvement in the whole circus. 

In conclusion, don’t take any anything you read online or product demos you see about MotherDuck at face value - spinning fantastical tales and lying by omission is their founders’ modus operandi. Stay safe people.",42,20,throwaway073847,2023-05-01 02:19:26,https://www.reddit.com/r/bigdata/comments/134a27f/how_motherduck_attempted_to_astroturf_rbigdata/,0,bigdata
9vjnt8,Spark 2.4 Released!,,38,1,ggeorgios11,2018-11-09 11:26:34,https://spark.apache.org/releases/spark-release-2-4-0.html,0,bigdata
naqvhx,Which of these is better?,,40,19,PracticallyUncommon,2021-05-12 15:05:14,https://i.redd.it/y14y52dxfpy61.jpg,0,bigdata
klv5eg,Anybody want a sticker or 3? DM me.,,39,4,PracticallyUncommon,2020-12-28 17:49:23,https://i.redd.it/04tsay45uy761.jpg,0,bigdata
yb6xkb,Bigdata Hadoop and Spark Analytics Projects (End to End)," Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1) [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)

2) [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)

3) [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)

4) [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)

5) [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)

6) [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)

7) [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)

8) [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1) [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)

2) [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)

3) [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)

4) [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)

5) [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.

&#x200B;

Thanks and Regards,

Bigdata Engineer",38,6,bigdataengineer4life,2022-10-23 03:32:57,https://www.reddit.com/r/bigdata/comments/yb6xkb/bigdata_hadoop_and_spark_analytics_projects_end/,0,bigdata
vi364v,PySpark Cheat Sheet: Basics of Initialising Spark in Python by DataCamp,,39,7,yashasbansal,2022-06-22 11:38:36,https://i.redd.it/npz1b81vs5791.jpg,0,bigdata
cwka81,The 4 V's of Big Data,,37,0,Sonaliiiii,2019-08-28 12:42:56,https://i.redd.it/mxwezzvlq6j31.jpg,0,bigdata
6u4w74,You can practice Big Data exploration with public data sets (including StackOverflow and Github) in BigQuery and 1TB free Queries per Month.,,36,1,dave-taylor,2017-08-16 20:32:23,https://cloud.google.com/bigquery/public-data/,0,bigdata
x8yrko,👀,,34,0,Ok-Put-4951,2022-09-08 12:06:05,https://i.redd.it/8skje02zkmm91.png,0,bigdata
wxuenq,ODD Platform - An open-source data discovery and observability service for data-driven enterprises looking to democratize data,,37,5,firig1965,2022-08-26 00:32:43,https://github.com/opendatadiscovery/odd-platform,0,bigdata
s3y56b,Uber Releases V1.1 of Orbit: A Python Package to Perform Bayesian Time-Series Analysis and Forecasting,"Last year, the Uber team introduced Orbit, a Bayesian time series modeling user interface which is simple to use, adaptable, interoperable, and high-performing (fast computation). Orbit uses probabilistic programming languages (PPL) for posterior approximation. So far, it is the only tool that enables simple model specification and analysis without being restricted to a small number of models.

Uber Team has recently released version 1.1 of Orbit, which includes changes in the syntax of calling models, the new classes design, and the KTR (Kernel Time-varying Regression) model. [***Continue Reading***](https://www.marktechpost.com/2022/01/14/uber-releases-v1-1-of-orbit-a-python-package-to-perform-bayesian-time-series-analysis-and-forecasting/)

Github: https://github.com/uber/orbit",39,0,techsucker,2022-01-14 17:44:37,https://www.reddit.com/r/bigdata/comments/s3y56b/uber_releases_v11_of_orbit_a_python_package_to/,0,bigdata
momk4r,Data Lakes: The Definitive Guide,,38,2,armagoei,2021-04-11 07:30:40,https://lakefs.io/data-lakes/,0,bigdata
8bj2vf,Data Engineers vs. Data Scientists and why these differences are so important,,37,10,eljefe6a,2018-04-11 17:51:58,https://www.oreilly.com/ideas/data-engineers-vs-data-scientists,0,bigdata
1rwq95,Free Data Science/Big Data Books,,36,1,chaconnewu,2013-12-02 13:15:34,https://github.com/chaconnewu/free-data-science-books,0,bigdata
147jmca,I've built a tool to let people interact with their data without SQL,,38,12,None,2023-06-12 09:21:59,https://v.redd.it/rvwi76sn1k5b1,0,bigdata
fydokq,How Big Data Affects Business,,35,2,Freeda_Freeda,2020-04-10 10:47:13,https://sonasoft.com/how-big-data-affects-business/,0,bigdata
5w9gux,Robert Mercer: the big data billionaire waging war on mainstream media,,36,3,BearsNecessity,2017-02-26 09:52:18,https://www.theguardian.com/politics/2017/feb/26/robert-mercer-breitbart-war-on-media-steve-bannon-donald-trump-nigel-farage?CMP=twt_gu,0,bigdata
8wqzh7,Build your own mini Apache Spark to understand how it works,,33,0,szelvenskiy,2018-07-07 05:38:36,https://towardsdatascience.com/fundamentals-of-data-processing-for-scifi-geeks-part-ii-apache-spark-rdd-3d4b2c6f39f,0,bigdata
663bme,Steve Ballmer has been building and is about to release a database of US Government spending,,36,0,scott_dsgn,2017-04-18 14:56:59,https://www.nytimes.com/2017/04/17/business/dealbook/steve-ballmer-serves-up-a-fascinating-data-trove.html?_r=1,0,bigdata
xy10sd,Data Quality Comparison on AWS Glue and Great Expectations/Updated with V3 API,,34,1,Witer1945,2022-10-07 14:46:01,https://towardsdatascience.com/data-quality-comparison-on-aws-glue-and-great-expectations-70af5bdfe39c,0,bigdata
byjctq,Create your first ETL Pipeline in Apache Spark and Python,,32,0,pknerd,2019-06-09 11:36:28,http://blog.adnansiddiqi.me/create-your-first-etl-pipeline-in-spark-and-python/?utm_source=r_bigdata_sparkpthon&utm_medium=reddit&utm_campaign=c_r_bigdata_sparkpthon,0,bigdata
9w2n4u,Uber's Big Data Platform: 100+ Petabytes with Minute Latency,,34,2,None,2018-11-11 09:50:26,https://eng.uber.com/uber-big-data-platform/,0,bigdata
843j0t,"The tech unicorns aren't software companies, they're data companies",,31,7,trickyanswers,2018-03-13 11:50:14,https://medium.com/initialcommit/data-the-resource-that-will-power-the-ai-future-21eafbb415da,0,bigdata
nl0xhd,First there were Data Warehouses. Then Data Lakes. I give you the next great technology you probably already have in your organization...,,34,3,Thriven,2021-05-25 21:47:56,https://i.redd.it/42i881zk7c171.jpg,0,bigdata
efp9th,"Tesla's Neural Net can now identify red and green traffic lights, garbage cans and detailed road markings",,34,5,Curio_ser,2019-12-26 02:07:54,https://www.teslarati.com/tesla-holiday-update-fsd-preview-neural-net-improvements/,0,bigdata
cogj89,Top 5 Data Science Projects to kick-start your Career [Code Included] - DataFlair,,33,2,Aakashdata,2019-08-10 10:41:49,https://data-flair.training/blogs/data-science-projects-code/,0,bigdata
cd7wnc,Good reference sheet for anyone working with data,"&#x200B;

**Download Link:** [https://www.geckoboard.com/assets/data-fallacies-to-avoid.pdf](https://www.geckoboard.com/assets/data-fallacies-to-avoid.pdf)

&#x200B;

https://preview.redd.it/vv6crjq3yba31.jpg?width=900&format=pjpg&auto=webp&s=6338888720698828523b0420454beadac3f60a89",33,4,ai-lover,2019-07-14 20:36:45,https://www.reddit.com/r/bigdata/comments/cd7wnc/good_reference_sheet_for_anyone_working_with_data/,0,bigdata
8tzzej,Practical Apache Spark in 10 minutes. Part 3 - DataFrames and SQL,,35,0,viktoriia_shulga,2018-06-26 13:25:20,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-3-dataframes-and-sql/?utm_source=reddit&utm_medium=bigdata&utm_campaign=spark_part3,0,bigdata
81pnyz,"Why do we need 9 different data processing engines? Comparison of Spark, Flink and 8 others.",,32,5,Krever,2018-03-03 11:39:19,https://youtu.be/sMEfvnIX4Co,0,bigdata
5vig3n,Finding Radiohead's most depressing song. With R,,31,4,Everything_Cyber,2017-02-22 12:42:53,http://blog.revolutionanalytics.com/2017/02/finding-radioheads-most-depressing-song-with-r.html,0,bigdata
n78jky,Connecting big data and blockchain with Constellation Network,,30,11,pratimacrt,2021-05-07 20:46:43,https://blockonomi.com/constellation-network-guide/,0,bigdata
f9808s,Py [package](https://github.com/kotartemiy/newscatcher) to collect news data from news websites,,30,0,kotartemiy,2020-02-25 09:54:49,https://i.redd.it/k1rjr5efl1j41.gif,0,bigdata
cam0h7,I Built A Raspberry Pi Hadoop / Spark Cluster,,29,0,_awwsmm,2019-07-08 14:51:47,https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/calhzs/i_built_a_raspberry_pi_hadoop_spark_cluster/,0,bigdata
bk86cj,"20 TB of ecology / honey bee behavior data, data storage, community analysis","We, a team of 3 scientists, build a device to measure honey bee dances in their colonies. These Dances are extremely telling concerning the bees health. There is already a body of literature on what the dance is, what information are communicated and how stress and pesticides change some aspects of the dances. There is, however, also a lot to be investigated since nobody ever collected this data in a scale as we did. We used 20 hives all over Germany that carry our sensor system. Additional sensor, besides honey bee dancing sensors, are inside temperature and humidity, outside temperature and humidity, a scale for the hives wight changes, an activity sensor at the entrance and a GPS mostly for accurate time stamps. One of the problems is that the dances are extracted offline in the lab for 6 channel 5000 samples per second analog to digital converters (16bit). This set of sensors generate per hive and per day  5 GB of data. This data is written to a SD card and send per post to us. The hives are out in the filed without internet connection and most of our apiarists are old and not very tecky. In 2018 we accumulated around 20 TB of data in binaries. To work with the data and extract the dancing events we convert them into CSVs, this process multiplies the volume of data by 7 fold. The converting software takes 3h per day of data on a new i/ with a SSD hard drive. Also we have no storage to keep all the inflated data. Our group is super small, very broke and will be closed soon (Professor is 80 years old). It would be a shame to toss the data in the trash / stop working on it. My question is were to store 150 TB for free. And most importantly how to get people involved to analyze the data without pay. The only thing we can offer is authorship to resulting articles. This data is insanely informative, in many aspects / from the point of view of different fields. Ecology, the bee is representing most of the pollinating insects in their niche as well. The influence of pesticides on insects could be tracked by this technique. Once we figure out how the signaling changes within the hive due to swarming, a dead queen or sicknesses like varroa, we can do those computations online (powerful STM32 on board) and apiarists would be much more at ease. And there is a wealth of unknown-unknown! This type of data did not exist before. I thought you know how to handle big data. Hopefully know how to analyze data in an exploratory manner. And most importantly know how to get in contact with people that are just as curious as we are, but capable in analyzing such monster. We work with the data, we poke it with a stick and see what falls out. But so far our questions need more data then our PCs can handle and to be honest, we are lacking experience with dimensionality reduction and others. Is there anything you could suggest?",31,27,bhp91,2019-05-03 12:56:38,https://www.reddit.com/r/bigdata/comments/bk86cj/20_tb_of_ecology_honey_bee_behavior_data_data/,0,bigdata
b64yed,Hadoop: The end of an Era,"It is not a secret that Hadoop is rapidly loosing the momentum, which became even more clear with the recent Cloudera acquisition of Hortonworks. Here is my take on the Hadoop and Big Data industry in general: [https://0x0fff.com/hadoop-the-end-of-an-era/](https://0x0fff.com/hadoop-the-end-of-an-era/).",33,31,0x0FFF_,2019-03-27 14:10:55,https://www.reddit.com/r/bigdata/comments/b64yed/hadoop_the_end_of_an_era/,0,bigdata
a84txp,"I wrote a brief guide on how to become a data scientist based on my own experience including learning R, SQL, stats, crafting a resume, and preparing for interviews.",,32,2,IndependentFI,2018-12-21 01:11:00,https://fourpillarfreedom.com/how-to-become-a-data-scientist/,0,bigdata
9nsxf1,Learn Hadoop in 10 Minutes,,32,5,poojagandhi456,2018-10-13 10:55:07,https://youtu.be/nuPp-TiEeeQ,0,bigdata
7anl39,Big Data SQL,,29,6,cohensar,2017-11-04 00:37:53,https://imgflip.com/i/1yrvgt,0,bigdata
5zbd1f,This startup uses machine learning and satellite imagery to predict crop yields,,32,3,dunkin1980,2017-03-14 10:31:09,http://www.theverge.com/2016/8/4/12369494/descartes-artificial-intelligence-crop-predictions-usda,0,bigdata
13tazup,UI for Apache Kafka - An open-source tool for monitoring and managing Apache Kafka Clusters - v0.17 release,,30,2,dahmedahe,2023-05-27 15:56:45,https://github.com/provectus/kafka-ui,0,bigdata
89k6bz,Data Science Interview Guide,,30,1,snazrul,2018-04-03 23:58:01,https://medium.com/@sadatnazrul/data-science-interview-guide-4ee9f5dc778,0,bigdata
5ylv1d,"Alphabet's Eric Schmidt: 'Big data is so powerful, nation states will fight' over it",,32,0,kingsleyelbert,2017-03-10 12:47:08,http://www.businessinsider.com/google-eric-schmidt-countries-will-fight-over-big-data-alphabet-cloud-2017-3,0,bigdata
46rtih,Apache Spark rises to become most active open source project in big data,,30,0,piterpolk,2016-02-20 20:34:01,http://www.techrepublic.com/article/apache-spark-rises-to-become-most-active-open-source-project-in-big-data/,0,bigdata
173lj8o,PySpark Tutorial for Beginners: 1-Hour Full Course,"🚀 Exciting News! Just released my latest YouTube video - ""PySpark Tutorial for Beginners: 1-Hour Full Course"" 🐍💡

Are you ready to dive into the world of PySpark and harness the power of distributed data processing with ease? In this comprehensive 1-hour tutorial, I'll guide you through the fundamentals of PySpark, from installation to hands-on coding examples.

🔥 What You'll Learn:
✅ Spark Introduction
✅ Spark Installation
✅ Setting Up Your PySpark Environment
✅ Spark RDD
✅ DataFrame Operations
✅ Spark SQL
✅ And much more!

Whether you're a beginner looking to kickstart your journey into big data or an experienced data engineer aiming to refresh your skills, this video has something for you!

Watch it now 👉 https://youtu.be/EB8lfdxpirM
GitHub Repo 👉 https://github.com/coder2j/pyspark-tutorial

Don't forget to like, subscribe, and share with your network. Let's spread the knowledge together! 📚💪
#PySpark #DataScience #BigData #Tutorial #LinkedInLearning #YouTubeTutorial",30,0,Coder2j,2023-10-09 07:04:58,https://youtu.be/EB8lfdxpirM,0,bigdata
oiszko,"Google Open-Sources A First-Of-Its-Kind, General-Purpose Transpiler For Fully Homomorphic Encryption (FHE), Enabling Developers To Compute On Encrypted Data Without Being Able To Access Any Personally Identifiable Information","Google has formulated [a guide of coding utilities that allow encrypted data to be fully homomorphic encrypted (FHE)](https://developers.googleblog.com/2021/06/our-latest-updates-on-fully-homomorphic-encryption.html). The open-source set of libraries and tools enables computational operations to be performed on encrypted data without first decrypting it, resulting in increased security and privacy. **Secure multi-party computing** and **homomorphic encryption** are well-known technologies. Rather than rewriting the foundation for the technologies, FHE focuses on improving and making them appropriate for broader deployment.

It’s the first-of-its-kind general-purpose transpiler for Fully Homomorphic Encryption (FHE), which will allow developers to perform computations on encrypted data without gaining access to personally identifying information. Developers can now build safe solutions by default, private by design, and put consumers in control. It will help developers to keep their users secure online and protect their data.

Full Story: [https://www.marktechpost.com/2021/07/12/google-open-sources-a-first-of-its-kind-general-purpose-transpiler-for-fully-homomorphic-encryption-fhe-enabling-developers-to-compute-on-encrypted-data-without-being-able-to-access-any-personally/](https://www.marktechpost.com/2021/07/12/google-open-sources-a-first-of-its-kind-general-purpose-transpiler-for-fully-homomorphic-encryption-fhe-enabling-developers-to-compute-on-encrypted-data-without-being-able-to-access-any-personally/) 

Google Blog: https://developers.googleblog.com/2021/06/our-latest-updates-on-fully-homomorphic-encryption.html",29,1,techsucker,2021-07-12 15:00:02,https://www.reddit.com/r/bigdata/comments/oiszko/google_opensources_a_firstofitskind/,0,bigdata
fe11g6,How Netflix uses Druid for Real-time Insights to Ensure a High-Quality Experience,[https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06](https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06),31,2,wochiquan,2020-03-05 20:00:25,https://www.reddit.com/r/bigdata/comments/fe11g6/how_netflix_uses_druid_for_realtime_insights_to/,0,bigdata
cqkvor,FREE Data Science and Machine Learning Course with the path (IBM Cognitive classes )," Learning Path:  
**1)Introduction to Data Science**https://cognitiveclass.ai/courses/data-science-101/  
**2)Data Science Tools**https://cognitiveclass.ai/courses/data-science-hands-open-source-tools-2/  
**3)Data Science Methodology**https://cognitiveclass.ai/courses/data-science-methodology-2/  
**4)Statistics 101**https://cognitiveclass.ai/courses/statistics-101/  
**5)Predictive Modeling Fundamentals I**https://cognitiveclass.ai/courses/predictive-modeling-fundamentals/  
**6)Python for Data Science**https://cognitiveclass.ai/courses/python-for-data-science/  
**7)Data Analysis with Python**https://cognitiveclass.ai/courses/data-analysis-python/  
**8)Data Visualization with Python**https://cognitiveclass.ai/courses/data-visualization-with-python/  
**9)Machine Learning with Python**https://cognitiveclass.ai/courses/machine-learning-with-python/  
**10)Deep Learning Fundamentals**https://cognitiveclass.ai/courses/introduction-deep-learning/  
**11)Deep Learning with TensorFlow**https://cognitiveclass.ai/courses/deep-learning-tensorflow/ 

&#x200B;

https://preview.redd.it/od61lddwdjg31.jpg?width=1920&format=pjpg&auto=webp&s=1f454d1a2b5547af0462dc6450513f74f0a41e29",29,3,ai-lover,2019-08-15 04:02:23,https://www.reddit.com/r/bigdata/comments/cqkvor/free_data_science_and_machine_learning_course/,0,bigdata
8pmeih,Apache Beam: a unified programming model for data processing pipelines,,30,1,dworms,2018-06-08 18:40:05,http://www.adaltas.com/en/2018/05/24/apache-beam-unified-programming-model/,0,bigdata
3omb8l,"Google Launches Cloud Datalab, An Interactive Tool For Exploring And Visualizing Data",,27,0,fhoffa,2015-10-13 18:42:54,http://techcrunch.com/2015/10/13/google-launches-cloud-datalab-an-interactive-tool-for-exploring-and-visualizing-data/,0,bigdata
2hrz20,Mining Massive Datasets on Coursera starts Today!!!,,32,8,petrux,2014-09-29 10:16:41,https://class.coursera.org/mmds-001/wiki/Syllabus,0,bigdata
cni9p5,Study of SIM cards in Slovakia finds there are a lot more residents than what official census showed. New information will now aid transport planning,Slovak big data scale-up **Instarea** teamed up with Slovakia’s top 3 telco providers and found that there are 50% more inhabitants in Bratislava than the official census showed. This data is now going to aid transport planning in the city. Check out the article to see how telco data was used for good. [https://instarea.com/the-value-of-mobile-phones-for-responsible-policies/](https://instarea.com/the-value-of-mobile-phones-for-responsible-policies/),30,5,ema_st,2019-08-08 07:24:41,https://www.reddit.com/r/bigdata/comments/cni9p5/study_of_sim_cards_in_slovakia_finds_there_are_a/,0,bigdata
6vazaf,Big data analytics platform Databricks raises $140M Series D round led by Andreessen Horowitz,,29,0,steccami,2017-08-22 13:24:20,https://techcrunch.com/2017/08/22/big-data-analytics-platform-databricks-raises-140m-series-d-round-led-by-andreessen-horowitz,0,bigdata
6j0ppe,"Introduction to web scraping with Python - Data, what now?",,28,0,trumtra,2017-06-23 11:55:39,https://datawhatnow.com/introduction-web-scraping-python/,0,bigdata
66gf43,My Top 10 Big Data & Data Analytics Books,,28,1,KristianBalding,2017-04-20 08:28:00,http://www.datapine.com/blog/best-big-data-and-data-analytics-books,0,bigdata
hc77dl,Spark Summit is now Free and Online!,"Just a reminder to everyone here that Spark AI Summit is happening next week, with training happening on Monday and Tuesday and keynotes/breakout sessions for the following three days. It's also for the first time 100% online and free! If you want to learn more about ML, Big Data solutions, Spark, or Databricks check it out!

Note that sessions will also be recorded and made available to view after the fact for those of you in different timezones. The only aspect you will miss out on is live discussion and asking Databricks TA's questions about the session as it progresses.

https://databricks.com/sparkaisummit/north-america-2020",29,1,Blayzovich,2020-06-19 19:35:11,https://www.reddit.com/r/bigdata/comments/hc77dl/spark_summit_is_now_free_and_online/,0,bigdata
9ayh3k,Practical Apache Spark in 10 minutes. Part 6 - GraphX,,27,0,viktoriia_shulga,2018-08-28 12:27:40,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-6-graphx/?utm_source=reddit&utm_medium=bigdata&utm_campaign=spark_part6,0,bigdata
815l1y,Naive Bayes Classification: how does it work?,,29,0,martmull,2018-03-01 14:24:28,https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44,0,bigdata
3ow3fp,Survey finds Big Data experts spend up to 90% of their time preparing the data for analysis.,,30,0,yourbasicgeek,2015-10-15 18:54:52,http://www.datamation.com/data-center/big-data-mostly-prep-work.html,0,bigdata
36ctq4,In-database R coming to SQL Server 2016,,26,1,eberkut,2015-05-18 10:42:40,http://blog.revolutionanalytics.com/2015/05/r-in-sql-server.html,0,bigdata
2j5fnh,Startup Crunches 100 Terabytes of Data in a Record 23 Minutes,,27,2,huitseeker,2014-10-13 20:47:44,http://www.wired.com/2014/10/startup-crunches-100-terabytes-data-record-23-minutes/,0,bigdata
18yo1z0,"Germany & Switzerland IT Job Market Report: 12,500 Surveys, 6,300 Tech Salaries","Over the past 2 months, we've delved deep into the preferences of jobseekers and salaries in Germany (DE) and Switzerland (CH).

The results of over 6'300 salary data points and 12'500 survey answers are collected in the Transparent IT Job Market Reports. If you are interested in the findings, you can find direct links below (no paywalls, no gatekeeping, just raw PDFs):

[https://static.swissdevjobs.ch/market-reports/IT-Market-Report-2023-SwissDevJobs.pdf](https://static.swissdevjobs.ch/market-reports/IT-Market-Report-2023-SwissDevJobs.pdf)

[https://static.germantechjobs.de/market-reports/IT-Market-Report-2023-GermanTechJobs.pdf](https://static.germantechjobs.de/market-reports/IT-Market-Report-2023-GermanTechJobs.pdf)",27,2,One-Durian2205,2024-01-04 21:10:58,https://www.reddit.com/r/bigdata/comments/18yo1z0/germany_switzerland_it_job_market_report_12500/,0,bigdata
ta7eoh,Apache Airflow: Create DAGs dynamically in the most reliable and scalable way possible,,26,0,marclamberti,2022-03-09 13:07:42,https://youtu.be/HuMgMTHrkn4,0,bigdata
qb6z8b,Free Data Science Month! All courses unlocked for free.,,28,0,365DS,2021-10-19 08:14:34,https://365datascience.com/free-data-science-month/,0,bigdata
hx33dm,Searchable drop down list covid - 19 analytics dashboard in Microsoft Excel,,27,19,SevereMark,2020-07-24 14:49:21,https://i.redd.it/e17x6pkpitc51.gif,0,bigdata
ehtavz,An interview with a DataDog engineer about how they build reliable and highly available systems for processing timeseries data in real time and at massive scale,,27,0,blarghmatey,2019-12-30 22:29:23,https://www.dataengineeringpodcast.com/datadog-timeseries-data-episode-113/,0,bigdata
akkvhe,"A Book Review of ""Architecting Modern Data Platforms""",,27,0,marklit,2019-01-28 06:44:29,https://tech.marksblogg.com/architecting-modern-data-platforms-book-review.html,0,bigdata
9zdsd8,Learn Apache Spark in 20 Minutes - By Experts,,29,2,poojagandhi456,2018-11-22 12:41:42,https://youtu.be/8Kcu63H0d8c,0,bigdata
9we3wm,Teradata's Lawsuit Against SAP - Big Data Warfare,,27,3,hyperindexed,2018-11-12 13:43:05,http://www.hyperindexed.com/blog/2018/8/14/teradatas-lawsuit-against-sap-2x2de,0,bigdata
99v048,Stanford Big Data Course Now Open to the World!,,27,4,datavizu,2018-08-24 06:12:30,http://anand.typepad.com/datawocky/2014/09/stanford-big-data-course-now-open-to-the-world.html,0,bigdata
8xqxfm,Best Open-Source Data Visualization Tools,,27,7,happyremoteworker,2018-07-10 16:40:27,https://blog.kolabtree.com/11-best-open-source-data-visualization-tools/,0,bigdata
518omo,Academic Torrents: Making 15.50TB of research data available,,27,1,eberkut,2016-09-05 09:40:55,http://academictorrents.com/,0,bigdata
4cyb7q,Kaggle's 1st April joke,,29,1,dostre,2016-04-01 21:42:30,http://imgur.com/BPeI4vp,0,bigdata
4bxbc2,Study List for Data Engineers and Aspiring Data Architects,,28,0,kiyoto,2016-03-25 17:01:11,https://blog.treasuredata.com/blog/2016/03/15/self-study-list-for-data-engineers-and-aspiring-data-architects/,0,bigdata
49ijg6,Announcing Apache Flink 1.0.0,,29,5,stadtlegende,2016-03-08 13:45:12,http://flink.apache.org/news/2016/03/08/release-1.0.0.html,0,bigdata
2p4co5,IBM Open Sources Spark Kernel,"We are happy to announce a developer preview of the Spark Kernel which enables remote applications to dynamically interact with Spark. You can think of the Spark Kernel as a remote Spark Shell that uses the IPython notebook interface to provide a common entrypoint for any application. The Spark Kernel obviates the need to submit jars using spark-submit, and can replace the existing Spark Shell. 

You can try out the Spark Kernel today by installing it from our github repo at https://github.com/ibm-et/spark-kernel. To help you get a demo environment up and running quickly, the repository also includes a Dockerfile and a Vagrantfile to build a Spark Kernel container and connect to it from an IPython notebook. 

We have included a number of documents with the project to help explain it and provide how-to information: 

* A high-level overview of the Spark Kernel and its client library (https://issues.apache.org/jira/secure/attachment/12683624/Kernel%20Architecture.pdf). 

* README (https://github.com/ibm-et/spark-kernel/blob/master/README.md) - building and testing the kernel, and deployment options including building the Docker container and packaging the kernel. 

* IPython instructions ( https://github.com/ibm-et/spark-kernel/blob/master/docs/IPYTHON.md) - setting up the development version of IPython and connecting a Spark Kernel. 

* Client library tutorial ( https://github.com/ibm-et/spark-kernel/blob/master/docs/CLIENT.md) - building and using the client library to connect to a Spark Kernel. 

* Magics documentation ( https://github.com/ibm-et/spark-kernel/blob/master/docs/MAGICS.md) - the magics in the kernel and how to write your own. 

We think the Spark Kernel will be useful for developing applications for Spark, and we are making it available with the intention of improving these capabilities within the context of the Spark community ( https://issues.apache.org/jira/browse/SPARK-4605). We will continue to develop the codebase and welcome your comments and suggestions. ",26,5,Lull3rSkat3r,2014-12-12 22:32:01,https://www.reddit.com/r/bigdata/comments/2p4co5/ibm_open_sources_spark_kernel/,0,bigdata
13i6hyp,I've built a tool to let people interact with their data instead of having to write SQL,,26,8,None,2023-05-15 12:13:30,https://v.redd.it/pcndohcflzza1,0,bigdata
xx6trl,The Arrival of the Data Lakehouse,"The term Data Lakehouse has become a real buzzword in the past 1 year+, with mostly Databricks using it as a marketing term for their product. I wanted to make sense of this new technology and compare it to its predecessors (warehouse and lake) to bring the hype into perspective. In this article, I give a brief history of big data storage and where it is heading with the new lakehouse architectures. 

I aimed to write it in a way that is understandable for both newcomers and veterans within the field; I hope you can get something out of it. I really think data lakehouses will bring the data field to the next stage, so I'm pretty hyped to see where this is going. 

Let me know what you think!

[https://medium.com/@axel.westeinde/the-arrival-of-the-data-lakehouse-fc28faf4dd95](https://medium.com/@axel.westeinde/the-arrival-of-the-data-lakehouse-fc28faf4dd95)",25,4,axelwest36,2022-10-06 14:36:16,https://www.reddit.com/r/bigdata/comments/xx6trl/the_arrival_of_the_data_lakehouse/,0,bigdata
dmkvte,FastSpark: A New Fast Native Implementation of Spark from Scratch,,27,1,None,2019-10-24 18:52:58,https://medium.com/@rajasekar3eg/fastspark-a-new-fast-native-implementation-of-spark-from-scratch-368373a29a5c,0,bigdata
b3ouyn,Big Data Project : Data Processing Pipeline using Kafka-Spark-Cassandra,"Videos Link :  [https://www.edyoda.com/course/1430](https://www.edyoda.com/course/1430)    
Github Code link :  [https://github.com/zekelabs/Big-Data-Analytics-Pipeline](https://github.com/zekelabs/Big-Data-Analytics-Pipeline)   


This course is a Big Data Project : Data Processing Pipeline using Kafka-Spark-Cassandra to bring up your own big data analytics pipeline. A very similar pipeline is common across many organizations. Data comes from many sources & kafka is used as a scaleable streaming framework. Gathered data then needs to be subjected for processing which a framework like Spark does amazing work. Finally, data is persisted in highly scale-able database like cassandra ",28,2,prithvi45,2019-03-21 10:22:52,https://www.reddit.com/r/bigdata/comments/b3ouyn/big_data_project_data_processing_pipeline_using/,0,bigdata
8opodz,What's new in Apache Spark 2.3 ?,,26,2,dworms,2018-06-05 10:20:08,http://www.adaltas.com/en/2018/05/23/whats-new-in-apache-spark-2-3/,0,bigdata
8l9dcx,Top 20 R Libraries for Data Science in 2018 [Infographic],,25,1,viktoriia_shulga,2018-05-22 11:49:59,https://www.activewizards.com/blog/top-20-r-libraries-for-data-science-in-2018-infographic/?utm_source=reddit&utm_medium=bigdata&utm_campaign=r_libraries,0,bigdata
7mqkee,Are there any practical examples of big data pipelines using real data?,"EDIT: so this post has gotten a decent amount of upvotes (for this small subreddit) with no answers, so I'm going to crosspost it to other related subs and report back.  BTW: this is the closest I've been able to come: https://aws.amazon.com/big-data/use-cases/

I am able to Google for sample datasets to use for big data experimentation and learning.  What I am unable to find are any tutorial, blog posts, etc that cover setting up and using an actual big data workflow/data pipeline using one of those data sets.

Have you seen any resources like this in the big data community ecosystem?  Something that starts w a large public dataset, covers olap and oltp domains, data warehouse, etl, reporting, etc, but in a practical sense, actually implementing and using the components.

Looking for an actual big data model deployment resource, even if it's just hello world level.

Thanks!

",26,4,ezeeetm,2017-12-29 00:05:59,https://www.reddit.com/r/bigdata/comments/7mqkee/are_there_any_practical_examples_of_big_data/,0,bigdata
48slve,Why Spotify migrated from Kafka to Google Pub/Sub for event delivery,,25,0,thetinot,2016-03-03 16:07:30,https://labs.spotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/,0,bigdata
45lvk5,Big Data: 35 Brilliant And Free Data Sources For 2016,,26,1,lokator9,2016-02-13 18:32:30,http://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#331791d16796,0,bigdata
40y641,Yahoo Offers Data on 20 Million of Its Users to Spur Machine-Learning Research,,26,0,stormforce7916,2016-01-14 15:59:45,http://www.technologyreview.com/news/545421/giant-yahoo-data-dump-aims-to-help-computers-know-what-you-want/,0,bigdata
3tkdwg,"5 things we hate about Spark: Spark has dethroned MapReduce and changed big data forever, but that rapid ascent has been accompanied by persistent frustrations",,28,0,robdoo,2015-11-20 13:46:41,http://www.infoworld.com/article/3004460/application-development/5-things-we-hate-about-spark.html,0,bigdata
11citk3,3D maps of worldwide weather,,25,0,nevermindever42,2023-02-26 14:40:30,https://v.redd.it/fsfvs481gdka1,0,bigdata
10isuna,Why Neural Nets Underperform Tree-Based Models on Tabular Data,,25,4,Personal-Trainer-541,2023-01-22 20:08:43,https://youtu.be/e62CBva4TYc,0,bigdata
y8lftu,Four Horsemen of AI Project Failure and How to Deal with Them,,24,0,Prearp22,2022-10-20 02:13:00,https://thesequence.substack.com/p/dmitrii-evstiukhinprovectus-four,0,bigdata
ukqx0y,Pros and Cons of SnowFlake?,"Our company is looking at SnowFlake as a potential solution for a Data Lake or to anchor a Data Lake I guess.  what are some pros and cons of going with SnowFlake versus other solutions.  

&#x200B;

Any advice would be greatly appreciated.  


TIA.",23,19,CyclonusDecept,2022-05-08 01:11:29,https://www.reddit.com/r/bigdata/comments/ukqx0y/pros_and_cons_of_snowflake/,0,bigdata
u5fdyp,Microsoft Research Introduces Open Data for Social Impact Framework,"The Open Data for Social Impact Framework, developed by Microsoft, is a roadmap to assist companies in using data to get new insights, make better decisions, and enhance efficiency while addressing urgent social concerns. “The ability to access data to enhance outcomes entails much more than technological tools and the data itself,” the framework relies on the fundamental learning from the Open Data Campaign. It is a framework that allows leaders to use data to solve the problems that matter most to them. Finding that not all data can be made available publicly, enormous benefits can be gained by promoting more open data, whether in the form of trusted data collaborations or completely open and public data.

It is a platform that leaders may use to use data to address significant social issues like lowering carbon emissions, closing the broadband gap, improving job skills, and expanding accessibility and inclusiveness. The methodology below is intended to bring organizational leaders from all sectors of the data ecosystem — governments, nonprofits, and multilateral organizations – to insights and solutions that can be used to address pressing social concerns.

[Continue Reading](https://www.marktechpost.com/2022/04/16/microsoft-research-introduces-open-data-for-social-impact-framework/) | [Microsoft Blog](https://news.microsoft.com/open-data-social-impact-framework/)",26,0,No_Coffee_4638,2022-04-17 04:31:34,https://www.reddit.com/r/bigdata/comments/u5fdyp/microsoft_research_introduces_open_data_for/,0,bigdata
oufugf,"Researchers found that accelerometer data from smartphones & -watches can reveal people's location, passwords, body features, age, gender, level of intoxication, driving style, and potentially be used to reconstruct words spoken next to the device.",,25,1,bayashad,2021-07-30 07:14:20,https://twitter.com/JL_Kroger/status/1420681035617116163,0,bigdata
cgqdmw,Band Protocol — A Protocol for Decentralized Data Governance,,26,1,SilentEternalOne,2019-07-23 09:29:56,https://medium.com/bandprotocol/band-protocol-a-protocol-for-decentralized-data-governance-1ab7f5e2a7c2,0,bigdata
b8qbsf,"Snowflake: The Good, The Bad and The Ugly","Snowflake or SnowflakeDB is a cloud SaaS database for analytical workload and batch data ingestion, typically used for building a data warehouse in the cloud. However, it appears to be so cool and shiny that people are getting mad at praising it all around the internet. Seeing that, I could not resist the urge to take a closer look at this technology and poke into some of its pain points. You can read my take on Snowflake DB at [https://0x0fff.com/snowflake-the-good-the-bad-and-the-ugly/](https://0x0fff.com/snowflake-the-good-the-bad-and-the-ugly/).",24,8,0x0FFF_,2019-04-02 23:06:55,https://www.reddit.com/r/bigdata/comments/b8qbsf/snowflake_the_good_the_bad_and_the_ugly/,0,bigdata
8xmqlr,Play Your Charts Right | An Illustrated Collection of Mistakes People Often Make When Visualizing Data,,24,2,Geckoboard,2018-07-10 07:47:41,https://i.redd.it/fbyedz1mtz811.jpg,0,bigdata
6w2695,An interactive GPU-powered deep dive into 11.6 billion rows of US shipping data,,24,0,marcmapd,2017-08-25 23:25:02,https://www.mapd.com/blog/2017/08/22/an-interactive-deep-dive-into-11-6b-rows-of-us-shipping-data/,0,bigdata
638e1l,DNA Sequencing of 1000 Cannabis Strains publicly available in Google BigQuery,,25,2,fhoffa,2017-04-03 17:46:41,https://medium.com/@allenday/dna-sequencing-of-1000-cannabis-strains-publicly-available-in-google-bigquery-a33430d63998,0,bigdata
4tryzc,Machine Learning over 1M hotel reviews finds interesting insights,,24,2,wildcodegowrong,2016-07-20 16:26:24,https://blog.monkeylearn.com/machine-learning-1m-hotel-reviews-finds-interesting-insights/,0,bigdata
3m4h2n,"Google Launches Cloud Dataproc, A Managed Spark And Hadoop Big Data Service: Per minute billing, 90 seconds cluster deploy",,26,0,fhoffa,2015-09-23 22:17:00,http://techcrunch.com/2015/09/23/google-launches-cloud-dataproc-a-managed-spark-and-hadoop-big-data-service/,0,bigdata
1gsz2d,Big Data starting resources for beginers,"Hi, I am a teenager interested in learning more about the Big Data field but am unsure where to start learning the fundamentals for the field. 

Are there any recommended prerequisites with regards to certain programming or math skills? and what are recommended books, videos, websites, etc for a beginner.

Thanks for your help",27,15,broshed,2013-06-21 17:08:39,https://www.reddit.com/r/bigdata/comments/1gsz2d/big_data_starting_resources_for_beginers/,0,bigdata
v0thpa,Data Engineers are in Greater Demand than Data Scientists,"&#x200B;

[ Globally, many think that data scientist is the best job after Harvard declared it to be one of the hottest jobs of the decade.  And since then, many have been choosing it as their career path. ](https://preview.redd.it/xhuksfw4zj291.jpg?width=1000&format=pjpg&auto=webp&s=9d40367b0d171b6e8390f024434d39cf9ec6ea5b)

 https://www.datasciencecentral.com/why-data-engineers-are-in-greater-demand-than-data-scientists/",24,0,None,2022-05-30 05:57:15,https://www.reddit.com/r/bigdata/comments/v0thpa/data_engineers_are_in_greater_demand_than_data/,0,bigdata
och65o,SQL Interviews Be Like (Parody),,25,0,nonkeymn,2021-07-02 19:16:41,https://www.youtube.com/watch?v=z7Ymn3Uuuc4&t=1s,0,bigdata
l8kgb0,Advanced SQL Questions From Amazon (Handling complex logic in data science interviews),,24,5,boyzone87seo,2021-01-30 12:12:41,https://youtube.com/watch?v=VYeevsVj4fU&feature=share,0,bigdata
evlkap,Hey r/bigdata. I downloaded a Reddit dataset and built a recommendation algorithm for subreddits. Here is what it recommends for users of r/bigdata:,"Come over to r/RedditRecommender and get personal or subreddit-based recommendations.

Enjoy these recommendations for r/bigdata readers!

r/bigdata : no. 1 score: 618.6216473791695

r/dataengineering : no. 2 score: 74.4637168141593

r/devops : no. 3 score: 70.07912051303406

r/googlecloud : no. 4 score: 68.73573859768551

r/aws : no. 5 score: 61.544763019309535

r/datascience : no. 6 score: 61.007450372518626

r/scala : no. 7 score: 42.01423043315442

r/Database : no. 8 score: 40.33748801534036

r/datasets : no. 9 score: 33.940762936498786

r/MachineLearning : no. 10 score: 33.108007082431634

r/analytics : no. 11 score: 31.51067282486581

r/kubernetes : no. 12 score: 30.253116011505274

r/PostgreSQL : no. 13 score: 28.63989108236896

r/BusinessIntelligence : no. 14 score: 26.088466308391897

r/LanguageTechnology : no. 15 score: 25.210930009587724

r/Python : no. 16 score: 24.782945301542778

r/statistics : no. 17 score: 24.52769217956192

r/java : no. 18 score: 24.52769217956192

r/learnpython : no. 19 score: 22.95645056781366

r/linuxadmin : no. 20 score: 22.25835464244776

r/AWSCertifications : no. 21 score: 21.918207866631935

r/SQL : no. 22 score: 21.02373615391022

r/learnmachinelearning : no. 23 score: 19.796259263616044

r/compsci : no. 24 score: 19.408888205443642

r/docker : no. 25 score: 19.109738372093023

r/MLQuestions : no. 26 score: 18.677913429522754

r/golang : no. 27 score: 17.519780128258514

r/apachekafka : no. 28 score: 17.183934649421378

r/Rlanguage : no. 29 score: 16.438655899973952

r/ProgrammingLanguages : no. 30 score: 16.438655899973952

r/SideProject : no. 31 score: 16.27228775865403

r/rust : no. 32 score: 16.174073504536366

r/node : no. 33 score: 15.386190754827384

r/linuxquestions : no. 34 score: 15.143799868018478

r/deeplearning : no. 35 score: 15.126558005752637

r/PowerBI : no. 36 score: 14.546041258499482

r/rstats : no. 37 score: 14.546041258499482

r/SQLServer : no. 38 score: 14.008435072142065

r/startup : no. 39 score: 14.008435072142065

r/InsightfulQuestions : no. 40 score: 14.008435072142065

r/javascript : no. 41 score: 13.522901376761132

r/vuejs : no. 42 score: 13.509151236219628

r/computervision : no. 43 score: 13.509151236219628

r/startups : no. 44 score: 13.388371141519295

r/AskComputerScience : no. 45 score: 11.823512880562062

r/foreignpolicy : no. 46 score: 11.455956432947584

r/M1Finance : no. 47 score: 11.455956432947584

r/Electroneum : no. 48 score: 11.455956432947584

r/Terraform : no. 49 score: 11.455956432947584

r/AWS_Certified_Experts : no. 50 score: 11.455956432947584

r/DataScienceJobs : no. 51 score: 11.455956432947584

r/coding : no. 52 score: 11.12917732122388

r/baduk : no. 53 score: 10.959103933315967

r/ansible : no. 54 score: 10.959103933315967

r/HoloLens : no. 55 score: 10.959103933315967

r/softwaredevelopment : no. 56 score: 10.959103933315967

r/cs50 : no. 57 score: 10.959103933315967

r/GraphicsProgramming : no. 58 score: 10.959103933315967

r/cogsci : no. 59 score: 10.959103933315967

r/openwrt : no. 60 score: 10.959103933315967

r/diysound : no. 61 score: 10.959103933315967

r/programming : no. 62 score: 10.652937204591492

r/vim : no. 63 score: 10.51186807695511

r/AskProgramming : no. 64 score: 10.51186807695511

r/crypto : no. 65 score: 10.503557608288604

r/desktops : no. 66 score: 10.503557608288604

r/canadacordcutters : no. 67 score: 10.503557608288604

r/htpc : no. 68 score: 10.503557608288604

r/macbookpro : no. 69 score: 10.228200972447327

r/SelfDrivingCars : no. 70 score: 10.228200972447327

r/mysql : no. 71 score: 10.08437200383509

r/chch : no. 72 score: 10.08437200383509

r/expats : no. 73 score: 10.08437200383509

r/indianews : no. 74 score: 9.704444102721821

r/raisingkids : no. 75 score: 9.697360838999654

r/visualization : no. 76 score: 9.697360838999654

r/HomeNetworking : no. 77 score: 9.563122243942354

r/cscareerquestions : no. 78 score: 9.413484486873509

r/commandline : no. 79 score: 9.338956714761377

r/berlinsocialclub : no. 80 score: 9.338956714761377

r/typescript : no. 81 score: 9.338956714761377

r/ruby : no. 82 score: 9.338956714761377

r/linux : no. 83 score: 9.241346488257582

r/trackers : no. 84 score: 9.231714452896432

r/Chattanooga : no. 85 score: 9.006100824146419

r/redhat : no. 86 score: 9.006100824146419

r/apachespark : no. 87 score: 9.0

r/webdev : no. 88 score: 8.956825342757385

r/askTO : no. 89 score: 8.80290138094574

r/chrome : no. 90 score: 8.80290138094574

r/marriedredpill : no. 91 score: 8.696155436130631

r/Motorrad : no. 92 score: 8.696155436130631

r/violinist : no. 93 score: 8.696155436130631

r/reactjs : no. 94 score: 8.603094540249472

r/psychology : no. 95 score: 8.5300677182596

r/learnjava : no. 96 score: 8.406833849535417

r/NYCbike : no. 97 score: 8.406833849535417

r/IKEA : no. 98 score: 8.406833849535417

r/django : no. 99 score: 8.406833849535417

r/awardtravel : no. 100 score: 8.406833849535417",25,5,needDataInsights,2020-01-29 11:16:11,https://www.reddit.com/r/bigdata/comments/evlkap/hey_rbigdata_i_downloaded_a_reddit_dataset_and/,0,bigdata
dwizux,How LinkedIn customizes Apache Kafka for 7 trillion messages per day,,23,0,ekoutanov,2019-11-15 00:54:43,"https://engineering.linkedin.com/blog/2019/apache-kafka-trillion-messages##targetText=Apache%20Kafka%20is%20a%20core%20part%20of%20our%20infrastructure%20at%20LinkedIn.&targetText=While%20many%20other%20companies%20and,%2C%20metric%20gathering%2C%20and%20more.",0,bigdata
c6jxgp,Is Hadoop Dead?,,23,7,marklit,2019-06-28 13:02:16,https://tech.marksblogg.com/is-hadoop-dead.html,0,bigdata
bt59t2,"The China International Big Data Industry Expo 2019 opened today in China's ""Data Valley"", Guiyang, Guizhou Province [OC]",,22,0,particleacclr8r,2019-05-26 07:42:49,https://i.redd.it/h28dg3odfi031.jpg,0,bigdata
9n7kvv,Cloudera and Hortonworks has announced $5.2 billion merger,"On October 3, 2018, big data rivals [Cloudera and Hortonworks announced their mutual agreement to combine in an all-stock merger](https://www.whizlabs.com/blog/cloudera-and-hortonworks-merger/). As per the terms of the transaction agreement, Hortonworks stockholders will own around 40% of the combined equity while Cloudera stockholders will have 60%. Moreover, Hortonworks shareholders are about to receive 1.305 shares of Cloudera for 1 share of Hortonworks they possess. 

Can anyone tell, what's the actual reason behind this merger?",23,5,amitarya011,2018-10-11 06:47:41,https://www.reddit.com/r/bigdata/comments/9n7kvv/cloudera_and_hortonworks_has_announced_52_billion/,0,bigdata
8w9wdw,AMA with ex-Chief Data Scientist for Candy Crush Saga,,25,0,leaning_over,2018-07-05 12:04:08,https://www.crowdcast.io/e/making-use-of-big-data?utm_campaign=VinceDarleyHangout&utm_source=RedditBigData,0,bigdata
8juvm0,The Best of AI: best articles published last month,,26,0,pierremarcenac,2018-05-16 13:24:31,https://blog.sicara.com/04-2018-best-ai-new-articles-this-month-4b8c5b9ab1ab,0,bigdata
7peszu,TIL autonomous cars are predicted to generate up to 25GB of data per hour.,,24,2,LittleMilton,2018-01-10 10:41:58,https://www.forbes.com/sites/julesschroeder/2018/01/09/millennials-heres-how-cryptocurrency-could-transform-your-future/2/#2c4715ec1dc7,0,bigdata
69dovs,Introducing Machine Learning for the Elastic Stack,,25,2,steccami,2017-05-05 09:35:38,https://www.elastic.co/blog/introducing-machine-learning-for-the-elastic-stack,0,bigdata
487oal,"Do NOT consider the ""Big Data"" specialization on Coursera - it is a waste of time","I gave the first 3 courses a try (Introduction to Big Data, Hadoop Platform and Application Framework, and Introduction to Big Data Analytics). I hoped that by the 3rd course that the material would have me diving into actual big data analysis. Instead, it had me banging my head on the keyboard trying to figure out syntax issues just to complete a quiz that taught me nothing about Big Data analysis. 

Maybe I was naive for thinking Coursera could deliver a quality set of courses on Big Data - but I was hopeful. Do not waste your time with these lectures or the specialization. ",25,15,xyphanite,2016-02-29 01:42:18,https://www.reddit.com/r/bigdata/comments/487oal/do_not_consider_the_big_data_specialization_on/,0,bigdata
3uhm2q,Big Data Visualization: Review of the 20 Best Tools,,26,23,eddyinblu,2015-11-27 18:02:30,http://inspire.blufra.me/big-data-visualization-review-of-the-20-best-tools/,0,bigdata
2x4w3e,Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 1),,23,0,mhausenblas,2015-02-25 18:08:26,http://blog.confluent.io/2015/02/25/stream-data-platform-1/,0,bigdata
2trt0a,The Free 'Big Data' Sources Everyone Should Know (by Bernard Marr),,24,0,urinec,2015-01-26 22:35:20,http://www.datasciencecentral.com/profiles/blogs/the-free-big-data-sources-everyone-should-know?xg_source=activity,0,bigdata
1ii6dn,U.C. Berkeley announces a 100% Online Data Science Master's Program,,25,4,Edutainer,2013-07-17 19:02:29,http://www.informationweek.com/education/online-learning/uc-berkeley-data-science-masters-program/240158400/,0,bigdata
1gc8jo,Big Data using Python tutorial,,24,1,joshir,2013-06-14 14:35:58,http://nealcaren.web.unc.edu/big-data/,0,bigdata
13fg8lk,Software Mistakes and Tradeoffs: Released! (Some time ago),"After almost two years and countless hours of work, I would like to announce to the Reddit community that my book (co-authored with Jon Skeet), ""Software Mistakes and Tradeoffs: How to make good programming decisions,"" has been released - in fact, it was released exactly one year ago. Still, it was not announced to the Reddit community since then, and I wanted to share that with you now 🎉.

*Software Mistakes and Tradeoffs take you on a journey through the real-world problems and tradeoffs you may encounter in your day-to-day job. The book's first part focuses more on low-level tradeoffs (code patterns, APIs, performance optimization, etc.). The second part will allow you to learn more about higher-level tradeoffs, such as data locality in big data processing, consistency vs. availability, delivery semantics, etc.*

The physical book, e-book, and audiobook are available on Manning's website: [http://mng.bz/wvxP](http://mng.bz/wvxP), and many other platforms. 

When I was writing the book, I wanted to make it very practical. Because of that, almost every concept discussed in the book is backed up with actual code and tests; I've prepared a GitHub repo containing the source code (Java and Scala): [https://github.com/tomekl007/manning\_software\_mistakes\_and\_tradeoffs](https://github.com/tomekl007/manning_software_mistakes_and_tradeoffs) 

Feel free to fork, contribute or use for your own experiments 🙂

I've dedicated this book to all of the open-source community. Most of the tools and architectures emerge from your devotion and contributions. You are the reason why software is progressing and meeting today's world demands.

**Note**: I am sharing my personal discount code for -35%: **relelek40**.",23,10,tomekl007,2023-05-12 09:41:06,https://www.reddit.com/r/bigdata/comments/13fg8lk/software_mistakes_and_tradeoffs_released_some/,0,bigdata
nlndh7,Databricks vs Snowflake - new releases,Seems these guys are converging on each other's turf. SQL analytics and Delta Sharing from Databricks and Unstructured data support + Snowpark from Snowflake are obviously their attempts to encroach each other's spaces. Wondering if anyone has seen these in private previews and what they think of each?,24,18,Electric_pokemon,2021-05-26 18:04:53,https://www.reddit.com/r/bigdata/comments/nlndh7/databricks_vs_snowflake_new_releases/,0,bigdata
fqmkpb,Is it interesting how to reach a high salary in your career in the shortest time possible?,"I am thinking of writing a blog / YouTube videos on the subject of maximising your IT career to learn everything required to reach a senior / high salary role in the shortest time span.

I have done this myself, where in 5 years have gone from complete noob to senior dev, on a high salary (~150K €)

I see so many people much more skilled than me which are on much lower salaries, and worse jobs with less freedom etc

I would like to share with other IT folk and people getting into this industry how to fast track your first few years to get to the top fastest.  

Some topics would be:

- Researching the market: Knowing what skills are in demand.  
- Your first 1.5 years - Type of company to learn the most. 
- Start to teach others (before you are ready) - git/blog/stackoverflow
- 1 company for 5 years vs 5 companies for 1 year each 
- Being in demand - & staying up to date
- Job App Process - CV, Interviews, Negotiation 
- Permentant vs Contract work

This is not just about salary, but getting to a point where you are in demand for jobs, so you can work less, more freedom, work remotely, be your own boss etc

Would any of you be interested in any of this type of content?

Any feedback?  Or other areas to focus on?",21,13,databoy1234,2020-03-28 15:46:47,https://www.reddit.com/r/bigdata/comments/fqmkpb/is_it_interesting_how_to_reach_a_high_salary_in/,0,bigdata
ey2eh4,IBM Data Science and AI Programs on Coursera Free for 30 Days,,23,5,awsconsultant,2020-02-03 05:54:52,https://pythoncoursesonline.com/IBMDataScienceAI,0,bigdata
933wz7,Practical Apache Spark in 10 minutes. Part 5 - Streaming,,22,0,viktoriia_shulga,2018-07-30 13:08:57,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-5-streaming/?utm_source=reddit&utm_medium=bigdata&utm_campaign=spark_part5,0,bigdata
6uoc42,Real time analytics: Divolte + Kafka + Druid + Superset,,24,4,Fokko,2017-08-19 10:21:32,https://blog.godatadriven.com/divolte-kafka-druid-superset,0,bigdata
5kzc29,How to execute 500 instantaneous queries on 18 billion rows? Which database should I use?,"So, here's the task my boss has given me:

We have a database of around 18 billion rows, 20 columns. (For the moment, let's assume this is a SQL-esque data table, or a `.csv` file). I have indexed two of these columns, `column_A` and `column_B`.

Now, I need to implement the infrastructure such that you can make a query on `column_A`, i.e.  give me all rows such that `column_A == ""value1""`. This query should take place as quickly as possible. In the words of my boss, financial transactions and internet queries take place instantaneously, so we should be able to implement the following system with ""instantaneous queries"". Ditto with the second indexed column, column_B: give us all rows with `columnB == 'value2'`. 

Furthermore, we would like to execute possibly 100-500 queries at once. 

We have an unlimited number of cores to work with, and a ridiculous about of RAM. 

What is the best database for this task? What should I try here? I imagine we need several parallel workers to grab each of these rows and then concatenate them at the end, a ""mapreduce"" approach. However, this also needs to be exceptionally quick. 

Any ideas? What do internet companies/financial firms do? 

EDIT: would a simple parallel SQL query do? https://wiki.postgresql.org/wiki/Parallel_Query",22,86,Zeekawla99ii,2016-12-29 21:20:11,https://www.reddit.com/r/bigdata/comments/5kzc29/how_to_execute_500_instantaneous_queries_on_18/,0,bigdata
5e6nld,"The Tableau Drag Race Results - tested with Zillow's 75 billion clickstream records (spoiler, results sorted by speed: BigQuery, MemSQL, Snowflake, Redshift, Presto, Jethro)",,21,0,fhoffa,2016-11-21 21:26:31,https://cmtoomey.github.io/speed/2016/11/04/tableaudragrace.html,0,bigdata
5d3non,Steven Hawking: Big data is the key to future scientific advances,,22,1,jkestelyn,2016-11-15 17:11:13,http://www.businessweekly.co.uk/news/academia-research/managing-big-data-vital-future-says-hawking,0,bigdata
3reqkd,Using Jupyter on Apache Spark: Step-by-Step with a Terabyte of Reddit Data,,23,0,Re_JenA,2015-11-03 21:49:31,http://blog.insightdatalabs.com/jupyter-on-apache-spark-step-by-step/,0,bigdata
26jd4q,Fault-tolerance,,23,0,Battle4Seattle,2014-05-26 18:07:37,http://i.stack.imgur.com/MV8HX.png,0,bigdata
xpvgs8,"LinkedIn Open-Sources ‘Venice,’ LinkedIn’s Derived Data Platform that Powers more than 1800 Datasets","Recently, LinkedIn announced that they would soon open-source Venice, their derived data platform. Venice is the engine behind more than 1800 LinkedIn datasets and is used by more than 300 separate applications. As of late 2016, the production of Venice has begun, and it has been steadily scaling to take the place of several existing systems.

An important architectural feature that sets Venice apart from conventional databases is how data is entered. Anyone testing out a new database must first write data before reading it. Since Venice is a derived data store supplied by offline and nearline sources, all writes to it are asynchronous. To rephrase, you cannot issue highly consistent online write requests like you can in MySQL or Apache HBase. Thanks to its design, Venice can reach unprecedented write throughputs.

[Continue reading](https://www.marktechpost.com/2022/09/27/linkedin-open-sources-venice-linkedins-derived-data-platform-that-powers-more-than-1800-datasets/) | [Github link](https://github.com/linkedin/venice) |[Reference Article](https://engineering.linkedin.com/blog/2022/open-sourcing-venice--linkedin-s-derived-data-platform)",21,0,ai-lover,2022-09-27 22:56:39,https://www.reddit.com/r/bigdata/comments/xpvgs8/linkedin_opensources_venice_linkedins_derived/,0,bigdata
v11cnp,"My open-source project: there shall be no difference between BI, Data Analysts and Data Engineer",,22,6,kuwala-io,2022-05-30 14:24:22,https://i.redd.it/g7sd3bgkhm291.gif,0,bigdata
quifbt,"Here is a detailed explanation on inverted indexes, the data structures powering Big data search technologies like Apache Solr and Elasticsearch",,21,6,exploreWithWonder,2021-11-15 15:04:30,https://www.ishanupamanyu.com/inverted-index-data-structure,0,bigdata
pse4gb,Clickhouse and Apache Pinot,"**NOTE**: This will soon be moved into a github repo so that updates/accessibility/collaboration is easier for all. Link will be shared here onceI am done with it.

&#x200B;

This is coming from my investigation on building an analytics platform. I have  really tried to be not partial towards any technology.  I did not find much resource on the internet related to comparison between these  (especially when they are competing for the same space ) technology except for:

1. [https://leventov.medium.com/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7](https://leventov.medium.com/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7)
2. [https://devopsprodigy.com/blog/chose-the-right-time-series-database/](https://devopsprodigy.com/blog/chose-the-right-time-series-database/)

The first post mentioned above is really good to understand from the technical point of view, but is bit dated considering how fast these technologies are evolving, but the basic architecture would not have changed.

Post #2 is also great but doesn't really do the job well of explaining when to choose what and why in detail.

I am here trying to fill some gap, but at the same time looking for advise on finding the differences between the two technologies. Even most of the blogs which says they chose Pinot or Clickhouse does not explain their decision on why that particular OLAP ?

At the same time, I am trying to learn and identify which one suit me the best from my use case. So please share your opinions if you have worked on one or the other!!

NOTE:

1. I could be wrong in some of the differences. So, lets collabrate and make it as accurate as possible.
2. If you are resharing the data, please be nice enought to point to this reddit post. I am expecting most of the goodies from the comment section :)

&#x200B;

&#x200B;

||ClickHouse|Apache Pinot|
|:-|:-|:-|
|Designed by|Yandex. Now Clickhouse, Inc|LinkedIn. Now StarTree Inc|
|Purpose|Realtime OLAP queries|Realtime OLAP queries|
|Storage|Columnar like|Columnar like|
|Query Language|SQL Dialect|SQL Dialect|
|Written in|C++|Java|
|Inception|Been there for quite some time. 2015/2016. Kind of time tested.|New to the community, but already being used by some big names.|
|Default Installation|Single node. Easy to install and deploy|Multi node. Different components are required. Not much of a problem since a complete working Helm chart is provided.|
|Architecture|Simple MPP.|More components like Broker, Controller, Helix/ZK involved. Chances of going something wrong in any of these components are more and can be a debuggability issue. Also needs to have different scaling mechanisms for these components individually. More DevOps!!|
|Zookeeper dependency|Needed, but not for standalone installation. As per the roadmap, zookeeper dependency might get removed.|Required|
|Performance|Known to be the fastest|Should be fast owing to its columnar architecture. C++ can always squeeze out more performance when compared to Java application. Some basic test shows good performance, but I had created really big offline segments which could have resulted in slow scans for some queries.|
|Concepts|More traditional like an RDBMS. Has the concept of primary key and ordered key.|Based on the concept of ""segments"". Can provide better performance for some range based queries. Also has a concept of ""OFFLINE"" and ""REALTIME"" tables. Offline for data which are inserted via batch for eg: csv file and Realtime for data ingested via a stream for eg: Kafka|
|Ingestion|Has direct Kafka read ability. Also can directly view MySQL data via MySQL data engine and many more cool things.|Realtime ingestion works with Kafka/Kinesis. Offline tables are usually from files which needs to be broken down into segments.|
|Multi tenancy|Could not find much about how to store or allocate servers to specific tenants.|Has the concept of tenants. The brokers and the servers can be tagged with tenant ID and the data will get stored only on those servers.|
|Scalability|One would think performance = scalability, but the above row makes me think otherwise. For many tables, the scalability could be less.|Since the servers and the brokers (query engine) can be isolated, we can expect better scalability with more number of tables (?)|
|Aggregation|Lots of options SummingMergeTree, AggregatingMergeTree MV|Has Star Tree Index for fast aggregation queries|
|As Time series database|Yes. Altinity has put lots of cool videos and blog posts comparing its performance with other time series database. Sadly not with Pinot.|Technically it does. But very less info on the net w.r.t benchmarking. The segment partition itself is based on the time, so I expect time range queries to be especially faster in Pinot than in clickhouse due to the nature of segments. Since the metadata server would know which servers to query, there is no read amplification. This is not true for clickhouse since each and every server will need to be queried unless you mention the shard key in the query filter.  Pinot partitions on a dimension column will result in segments having records for only that partition key.|
|User Interface|Has both SQL CLI and UI. Tradition DB like workflow to create database/tables etc.|OpenAPI REST endpoints. UI available to query and monitor. Table/Schema creation done via REST APIs. Not very comfortable to work with. It may improve in future.|
|High Availability|Supports both replication and sharding to a mutinode cluster. All the nodes have the same code running. Metadata handled by the distribution engine.|Supports both replication and sharding. Helix/Broker maintains the metadata related to how data is distributed.|
|Partitioning|Partitioning on dimesions is possible|Partitioning on dimesions is possible|
|Data Placement|Clickhouse has very simple data placement architecture and AFAIK does not allow data to be placed in specific set of servers. Data routing is internally taken care of. UPDATE: As per comment from dbcicero, there are ways to get this using distributed table instead of local table.|Data placement vis replica groups and tenants.|
|Adding nodes|Clickhouse is said to be needing lots of babysitting for its cluster management since it is not easy to add/remove nodes from a cluster. The data balancing needs to be done manually. There does not seem to be any plan to address this in the near future roadmap.|Cluster management is said to be easy though some manual work needs to be done. Need to study further.|
|Deep Storage|The data needs to reside physically. There is option to move cold data to s3 storage based on the storage policy.|Can move old data to cloud storage and thereby reduce storage cost on the compute cluster. This is a very good feature to have especially is we want to move the system to a cloud deployment.|
|Trino Integration|Trino clickhouse connector is available.|Available and seems more work is going into it than for Clickhouse|
|Join support|Yes. Somewhat minimal|No. Supported via Trino.|
|Heterogenous data (No idea what this means! This if from post #2 above)|Not good for such use cases|Good for such use cases|
|Geo support|Yes. To what level one needs it needs to be investigated based on the application|Yes. To what level one needs it needs to be investigated based on the application|
||||

&#x200B;

&#x200B;",24,10,ArunMu,2021-09-21 07:47:35,https://www.reddit.com/r/bigdata/comments/pse4gb/clickhouse_and_apache_pinot/,0,bigdata
mh2t8m,Apache Kafka Made Simple: A First Glimpse of a Kafka Without ZooKeeper,,21,0,rmoff,2021-03-31 10:02:16,https://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek,0,bigdata
kgu4lq,How exactly Hadoop is used in real life?,"Hello, I read a lot of articles about how Hadoop is used by companies, [one link](https://medium.com/@suchitmajumdar/how-twitter-facebook-ny-times-use-hadoop-782b5d8940cf), but I wonder how it is used under the hood. I understand that Hadoop is something like storage for log analytics (?) etc, but how these logs are formatted (is it JSON, CSV, any binary files)? Are they split when they reach certain size?  How is it different from Elastic Search that also stores logs and enables free text search? Maybe Hadoop is used by other frameworks like Apache Hive?

&#x200B;

I am new to this topic and this is why I ask for clarification. Could you provide some example open-source project that uses Hadoop so that I can analyze its source code?

&#x200B;

Thank you :)",23,17,h3wro,2020-12-20 13:38:21,https://www.reddit.com/r/bigdata/comments/kgu4lq/how_exactly_hadoop_is_used_in_real_life/,0,bigdata
jzkec3,Kafka IDE is released as Open Beta today! 🎉,"[Kafka IDE](https://kafkaide.com/?utm_source=Reddit&utm_medium=social&utm_campaign=open-beta&utm_term=bigdata) is a desktop client similar to Tableau or Looker that queries Apache Kafka directly. It aims to be a better alternative to kafkacat, kafka manager or similar.

You can query particular time windows using offsets (you can use natural language to specify dates) as well as smart schema inference for those folks who deal with plain JSON messages.

It also let you peek into some Apache Kafka specific configurations for your cluster and topics.

Let us know if you find it useful or if you encounter any issue while trying it out! You can leave your comment here or send us an email at [team@kafkaide.com](mailto:team@kafkaide.com).",22,4,kafkaide-com,2020-11-23 16:07:22,https://www.reddit.com/r/bigdata/comments/jzkec3/kafka_ide_is_released_as_open_beta_today/,0,bigdata
irxuvs,"The versatility of ROW_NUMBER, one of SQL’s greatest function",,23,2,linkerzx,2020-09-13 13:01:30,https://medium.com/analytics-and-data/the-versatility-of-row-number-one-of-sqls-greatest-functions-53ec78e74096,0,bigdata
ed74xx,How to Use Airflow without Headaches,,21,1,None,2019-12-20 07:57:06,https://towardsdatascience.com/how-to-use-airflow-without-headaches-4e6e37e6c2bc,0,bigdata
cie0ry,Introducing Spark-Kafka integration for realtime Kafka SQL queries,,22,0,allwefantasy,2019-07-27 06:12:46,https://medium.com/@williamsmith_74955/introducing-spark-kafka-integration-for-realtime-kafka-sql-queries-a54e051a79cb,0,bigdata
ccvfu9,AI smokes 5 poker champs at a time in no-limit Hold’em with ‘ruthless consistency’,,22,0,craigbrownphd,2019-07-13 22:31:45,https://www.reddit.com/r/DataIntelligence/comments/cc4vf4/ai_smokes_5_poker_champs_at_a_time_in_nolimit/,0,bigdata
bvklv6,41 hours free course for big data hadoop with spark,"Direct [link here](https://www.udemy.com/big-data-harish/?couponCode=HARISH)

more big data free courses are collected [here](https://www.real.discount/search-page/?keyword=big+data)",22,0,saadmerie,2019-06-01 13:07:13,https://www.reddit.com/r/bigdata/comments/bvklv6/41_hours_free_course_for_big_data_hadoop_with/,0,bigdata
arjyvg,For all the SQL enthusiasts out there. SQL is a language which is becoming a pre-requisite for people who want a good career in data science. This tutorial contains all the topics to give you a brief about this language.,"https://www.youtube.com/playlist?list=PLVHgQku8Z936E51WaMbEHML6LutrGCSYr
",20,5,yashica_,2019-02-17 12:02:37,https://www.reddit.com/r/bigdata/comments/arjyvg/for_all_the_sql_enthusiasts_out_there_sql_is_a/,0,bigdata
a1jf1b,Useful resource bundle,I got [this](https://twitter.com/skjfhsjfldnlvcb/status/1067806126635331584) and realized others may want to get it too. I grabbed the 3rd tier but lower tiers are also valuable too in my opinion.,21,1,Oldmajenkins,2018-11-29 17:32:00,https://www.reddit.com/r/bigdata/comments/a1jf1b/useful_resource_bundle/,0,bigdata
9oxb7t,"Here are some awesome video's I found on YouTube related to Big Data and Hadoop, hope it helps anyone who is interested in learning it.",,22,5,sa-shahzan,2018-10-17 10:09:43,https://www.youtube.com/playlist?list=PLr0GuAb3MR-wEDZwetSvzvc-CKVSGruhq,0,bigdata
989mw7,Why Big Data is in Trouble: They Forgot About Applied Statistics,,24,1,IamArchit,2018-08-18 06:21:46,https://www.kdnuggets.com/2016/07/big-data-trouble-forgot-applied-statistics.html,0,bigdata
9401id,Things you should know when working as a Big Data Engineer,,21,1,Krever,2018-08-02 15:04:54,https://w.pitula.me/2018/data-engineer-must-know/,0,bigdata
86kn6g,Top 9 Data Science Use Cases in Banking,,22,0,viktoriia_shulga,2018-03-23 13:32:54,https://www.activewizards.com/blog/top-9-data-science-use-cases-in-banking/?utm_source=reddit&utm_medium=bigdata&utm_campaign=banking,0,bigdata
5vbc25,How I Cracked 11 Big Data Interviews. Hadoop & Spark Interview Experience & Questions,,20,2,anishp1313,2017-02-21 12:58:15,https://youtu.be/BuREt8e3_ok,0,bigdata
5rnmvc,The Data That Turned the World Upside Down,,23,2,BigLeafLittleRainbow,2017-02-02 16:11:38,https://motherboard.vice.com/en_us/article/how-our-likes-helped-trump-win,0,bigdata
5ra52h,Learn data science: 44 essential resources for developers,,22,0,piedpiperpivot,2017-01-31 18:53:18,https://techbeacon.com/learn-data-science-44-essential-resources-developers,0,bigdata
5jxucs,"More data will be created in 2017 than the previous 5,000 years of humanity",,22,4,AppDeveloperMagazine,2016-12-23 16:14:14,"https://appdevelopermagazine.com/4773/2016/12/23/More-data-will-be-created-in-2017-than-the-previous-5,000-years-of-humanity-/",0,bigdata
5h54ff,Data Engineering at Slack,,23,0,dianamp,2016-12-08 05:11:13,https://slack.engineering/data-wrangling-at-slack-f2e0ff633b69#.wbdaxmvg4,0,bigdata
4jyb6p,We are Giving Away Global Maritime Vessel Traffic Data For One Year,"Over the last two years, we have been designing, building, and testing a payload to be flown to the International Space Station. Our project is called GLobal AIS on Space Station (GLASS). Here are some details:

* The purpose of the GLASS project is to track maritime vessel traffic worldwide from the International Space Station.
* Most vessels over 300 gross tons must transmit an [Automatic Identification Signal \(AIS\)](https://en.wikipedia.org/wiki/Automatic_Identification_System), which is detectable from space.
* We are looking for evaluators to participate in a one-year applied research initiative. 
* We are specifically looking to work with businesses, government organizations, and academic researchers and institutions.
* The hardware is complete, and will launch on a SpaceX vehicle this summer.
* When the hardware is installed, checked out, and operational, we will begin offering the data to evaluators.
* After the one-year evaluation period is over, we intend to commercialize the platform.


Anyone interested in participating in the project should visit [our website](http://www.jamssamerica.com/glass-overview/) for details. Thanks!",21,8,houstonspace,2016-05-18 19:00:35,https://www.reddit.com/r/bigdata/comments/4jyb6p/we_are_giving_away_global_maritime_vessel_traffic/,0,bigdata
3zfnra,Announcing Spark 1.6,,21,0,AccidentalComplexity,2016-01-04 17:21:25,https://databricks.com/blog/2016/01/04/announcing-spark-1-6.html,0,bigdata
2o06ac,"Big data is all blue, shiny, and full of 0s and 1s. A tumblr collection.",,21,2,fhoffa,2014-12-02 02:22:37,http://bigdatapix.tumblr.com/,0,bigdata
2benif,Awesome Big Data: GitHub List,,23,1,ilikebigdata,2014-07-22 16:25:08,https://github.com/onurakpolat/awesome-bigdata,0,bigdata
wppu8n,"A Python Library to OCR, Archive, Index and Search any documents with ease","Hey there, 

We wanted to share a library that we've been working on for a while. It's called [`OcrPy`](https://github.com/maxent-ai/ocrpy) - a Python library for doing OCR++. 

At its core the library aims to unify a multitude of OCR systems (commercial & open-source) and provides a consistent and unified interface, along with a lot of additional functionalities that are usually really useful in practice - such as identifying the type of documents, the different types of layout's they come in, etc and in turn processing them in different approaches.

Once you parse these docs, it also lets you index and enables searching over these collections via semantic search. 

with that said, here are the links to the python library, documentation & tutorials.

1. Github: https://github.com/maxent-ai/ocrpy
2. Documentation: https://maxentlabs.com/ocrpy
3. Tutorial: https://maxentlabs.com/ocrpy/examples

Tl;dr of the library:

![](https://maxentlabs.com/ocrpy/_images/ocrpy-workflow.png)

- It supports various types of ocr cloud backends like AWS-textract, google-cloud-vision, azure-document-recognition, etc & open-source OCR backends like - tesseract. 
- It supports async processing of docs on various kinds of cloud storage like s3, GCS, azure storage, etc.
- Due to the abstraction of various types of these cloud/open-source OCR parser backends & storage backends - if required you can just switch from one ocr vendor to the other without any changes to the codebase or the output formats.
- You can also index and search over the parsed docs - currently we support indexing it to Elastisearch and OpenSearch backends. 
- In case you are interested in processing different types of docs with different types of parsers you can automatically identify the type of the doc and then use the appropriate parser (or write your own custom parser) - this is useful if you want to for instance use some deep learning model for handwritten docs or say table extraction etc.. vs using off-the-shelf OCR for regular docs. 
- Document type classification and layout parsing within the docs is built on top of Huggingface transformers - as such you can easily swap out and play with different types of models or add your own custom models into the pipeline as well.",21,1,xen-m-rph,2022-08-16 09:33:10,https://www.reddit.com/r/bigdata/comments/wppu8n/a_python_library_to_ocr_archive_index_and_search/,0,bigdata
v3l8a1,"What if AB testing is impossible to setup? I wrote a blog to measure impact using backdoor adjustment, a type of causal analysis","To ensure that every feature has a measurable impact on the broader platform my team will set up and run A/B testing on each new feature or product change, but what happens when a new feature needs to be released quickly and there is not enough time for a traditional testing approach? To make sure that these quick changes could still be measured I found a way to perform accurate pre-post analysis using a back-door adjustment of causal analysis. I wanted to share my findings with the community as it was able to help my team at DoorDash make quick bug fixes and still be able to measure the impact. Please check out the article to get the technical details and provide any feedback on my approach. [https://doordash.engineering/2022/06/02/using-back-door-adjustment-causal-analysis-to-measure-pre-post-effects/](https://doordash.engineering/2022/06/02/using-back-door-adjustment-causal-analysis-to-measure-pre-post-effects/)",21,0,tripleespresso7,2022-06-02 22:55:04,https://www.reddit.com/r/bigdata/comments/v3l8a1/what_if_ab_testing_is_impossible_to_setup_i_wrote/,0,bigdata
u4rygq,"LinkedIn Open-Sources ‘Feathr’, It’s Feature Store To Simplify Machine Learning (ML) Feature Management And Improve Developer Productivity","LinkedIn research team has recently open-sourced feature store, [Feathr](https://github.com/linkedin/feathr), created to simplify machine learning (ML) feature management and increase developer productivity. Feathr is used by dozens of LinkedIn applications to define features, compute them for training, deploy them in production, and share them across consumers. Compared to previous application-specific feature pipeline solutions, Feathr users reported significantly reduced time required to add new features to model training and improved runtime performance.

Hundreds of ML models run on LinkedIn in Search, Feed, and Ads applications. Thousands of features about entities in the Economic Graph, such as companies, job postings, and LinkedIn members, power the models. The most time-consuming aspects of handling the ML applications at scale have been preparing and managing features.

[Continue reading the summary](https://www.marktechpost.com/2022/04/15/linkedin-open-sources-feathr-its-feature-store-to-simplify-machine-learning-ml-feature-management-and-improve-developer-productivity/)

Github: https://github.com/linkedin/feathr

LinkedIn Blog: https://engineering.linkedin.com/blog/2022/open-sourcing-feathr—linkedin-s-feature-store-for-productive-m",21,0,No_Coffee_4638,2022-04-16 06:45:26,https://www.reddit.com/r/bigdata/comments/u4rygq/linkedin_opensources_feathr_its_feature_store_to/,0,bigdata
qn2oo2,"70 billion rows, which dbms to choose?","I need to train a model on tensorflow with 70 billion entries, and also I need to be capable of reading (no update necessary) those rows swiftly. Which dbms should I choose? I'm open to both SQL and NoSQL.",20,18,Ded-Smoke,2021-11-05 03:53:45,https://www.reddit.com/r/bigdata/comments/qn2oo2/70_billion_rows_which_dbms_to_choose/,0,bigdata
i5gld8,4 SQL Tips For Data Scientists And Data Analysts,,20,0,nonkeymn,2020-08-07 16:08:51,https://www.youtube.com/watch?v=kSt9NV-qZkc,0,bigdata
hf3map,Redash has been acquired by Databricks,,21,2,WesleyBatista,2020-06-24 16:02:09,https://databricks.com/blog/2020/06/24/welcoming-redash-to-databricks.html,0,bigdata
epmhdo,The Greatest Hedge Fund of All Time,"Renaissance Technologies is the Greatest Hedge Fund of All Time.

Founded by math genius Jim Simons, it's flagship fund Medallion has an average gross return of 66.1% since 1988.

With an average net return of 39.1% after fees.

The Medallion Fund is available only to current and past employees and their families, closing to outside investors in 1993.

Since, 1988, the Medallion Fund has racked up trading profits of more than $100 billion.

Now, I mentioned a net return of 39.1% after fees: well, the fees have been greater than the usual '2 and 20' structure (which means a 2% management fee and a 20% performance fee).

Medallion has had a 5% management fee, and from 1988 to 2001, a 20% performance fee and from 2002 until now, a 44% performance fee.

Notice in particular the return in 2007 and 2008, a time when many were completely REKT.

In 2008, a return (after these monstrous fees) of 82.4%

**To make us all feel terrible, if you had invested $1000 into Medallion in 1988 you would have today, after fees, around $23MM**.

That certainly beats inflation...

So, how did they do it and what can we take away from this story (aside from searing jealousy)?

&#x200B;

**PART 1: The Early Stages**

Early on, Simons had a goal of algorithmic investing.

Remember, this was the late 1980s before the phrase big data became a household name and most investment decisions were made over the phone based on gut with the likes of Jordan Belfort trying to scam you!

“I don’t want to have to worry about the market every minute. I want models that will make money while I sleep,” Simons said. “A pure system without humans interfering.”

Simons hired Sandor Straus to help him collect historic commodity information

Straus’ was essential to Renaissance Technologies early success in commodities trading.

He became somewhat of a data guru ensuring pricing was consistent and accurate, checking his numbers matched with yearbook data provided by commodity exchanges, Wall Street Journal, other newspapers and anything else he could get his hands on.

Over time, Straus and his colleagues discovered additional historical pricing data, helping the development of new predictive models.

In fact, some of the stock market data they'd later find went back as far as the 1800s!

At the time, the team couldn't do much with the data, BUT the ability to search modern history to see how markets reacted to unusual events would later help Simon's team build models to profit from market collapses and so called 'Black Swan events'.

The return in 2008 is a prime example of that.

Commodity markets were relatively simple and RenTec found success in deploying simple trading strategies.

The fund wasn't bothered as to why these trading patterns existed - the only thing that mattered is that they occurred in a predictable and actionable way.

&#x200B;

**PART 2: Intellectual Capital**

Now in order to build these quantitative models, RenTec is composed of mathematicians and physicists of the highest order and it has even been described as the ""best math department in the world"".

Therefore, their quantitative researchers are well aware of the problems with data mining, over-fitting and spurious signals.

We are taking A LOT of data: 9TB per day in fact.

RenTec originally focussed on trading commodities, currencies and futures.

The strategies were mainly trending (i.e. price will continue to move in same direction) and mean reversion (i.e. price will return to original value).

Simons was experimenting in the stock market (equities) since the late 1980s but the strategy that had worked well on futures was not working on equities.

In 1995, David Magerman, an early employee, spotted a line of simulation code used for the equity trading system showing the S&P 500 at an unusually low level.

This test code appeared to use a figure from back in 1991 that was roughly half the current number.

It had been written as a static figure, rather than as a variable that updated with each move in the market.

Magerman also spotted an algebraic error elsewhere in the code.

Finally, the simulator’s algorithms could finally recommend an ideal portfolio for the trading system to execute.

The resulting portfolio seemed to generate big profits, at least according to Magerman’s calculations.

Only then did Renaissance commit significant capital into the equity markets, and since then...well, pretty good....

&#x200B;

**PART 3: Infrastructure**

Now I mentioned before about the sheer amount of data RenTec is utilising.

Big data has obviously caught on, but many hedge funds continue to under-perform the market and even some hedge funds focussed on quant methods haven't fared too well.

The problem, and one of the reasons RenTec is so special, is the barrier to entry is so incredibly high:

Building a data pipeline and the infrastructure required to process that data is no trivial matter.

To then get profitable trading signals from that processed data is a mammoth task.

RenTec has been in the game for over 30 years, constantly refining their algorithms and improving the efficiency of their data processing pipeline.

They have completely automated the process of signal discovery:

They don't hire researchers to manually derive novel insights or trading models from data, and they don't really bother with exclusive sources of data. Instead, they hire researchers to improve methods for automatically processing vast amounts of arbitrary data and extracting profitable trading signals from it.

RenTec has automated the data processing and feature extraction pipeline end to end.

The data is a pure abstraction to them. They don't bother with forming hypotheses and trying to find data to test them, they allow their algorithms to actively discover new correlations from the ground up. So many quantitative funds advertise how much data they work with, and how they have all these exotic sources of data at their disposal - but the data does not matter. The models for the data do not matter.

The mathematics of *efficiently processing* that data are what matters.

&#x200B;

**CONCLUSION:**

**The takeaway from this is the following: do not day trade, you will get REKT.**

**You are competing with immense infrastructure and intellectual capital of the highest level.**

[https://www.youtube.com/watch?v=jcy8QaILDJI](https://www.youtube.com/watch?v=jcy8QaILDJI)

&#x200B;

BRAVE BROWSER: [https://brave.com/fin894](https://brave.com/fin894)",20,7,financeoptimum,2020-01-16 17:29:13,https://www.reddit.com/r/bigdata/comments/epmhdo/the_greatest_hedge_fund_of_all_time/,0,bigdata
923w2x,I still don't understand why Spark is so popular as a ETL solution. Could someone explain why Spark is so commonly chosen?,"I understand that Spark is good for streaming and ML. But with regards to *just* ETL tools/solutions, why should I choose Spark over a typical ETL solution or using a database to process the data? 

For maybe more context, let's say we're doing batch processing every day on no more than 10 GB of data. I've seen companies go for Spark in this scenario, which boggles my mind. What's likely the reasoning here? 

If the incoming data was much much larger at say 1 TB/day, would Spark be the preferred solution, generally speaking?

How much does the ability to use the streaming and ML packages play into decisions to use Spark? If a team wants to also do ML, I'm guessing having that ability on top of a data set is pretty convenient, so Spark is ideal? 

To be honest, as someone still learning its inner workings and coming from a traditional ETL developer background, I just don't understand the hype around Spark. ",20,15,iarcfsil,2018-07-26 16:43:17,https://www.reddit.com/r/bigdata/comments/923w2x/i_still_dont_understand_why_spark_is_so_popular/,0,bigdata
8r3f5f,Metacat: Making Big Data Discoverable and Meaningful at Netflix,,22,0,mfrw1,2018-06-14 16:53:08,https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520,0,bigdata
83kyxs,Data Science vs. Data Analysis: What's the Difference?,,21,0,suthukrish,2018-03-11 05:50:00,https://dzone.com/articles/data-science-vs-data-analysis-whats-the-difference,0,bigdata
82xplv,"Could we talk about low effort posts, particularly links to articles?","I could be in the minority, in which case feel free to downvote this to oblivion. However, this sub is filled with low effort links to articles with no input by the poster, no discussion, and many are clearly bots.

I think this drives off potential subscribers that want to actually discuss big data technology, and not read ten articles on “the next big thing”.

What do you think? Do you enjoy the current state of the sub? What are other subs doing to prevent these low effort posts? I’d love to get some opinions.",22,11,brantaylor,2018-03-08 14:07:28,https://www.reddit.com/r/bigdata/comments/82xplv/could_we_talk_about_low_effort_posts_particularly/,0,bigdata
7sxqkb,"[video] Engineers from Reddit, Thumbtack, Tapjoy, Twitter and Google Cloud discuss big data engineering",,21,1,fhoffa,2018-01-25 17:34:43,https://www.youtube.com/watch?v=2AIO0szCfXg,1,bigdata
7cltq3,1.1 Billion Taxi Trips on BrytlytDB 2.0,,20,3,marklit,2017-11-13 08:34:43,http://tech.marksblogg.com/billion-nyc-taxi-rides-p2-16xlarge-brytlytdb-2.html,0,bigdata
759iac,"7 articles handpicked by data scientists: tutorials about RL, graph generation and sentiment analysis",,22,1,oliviercha,2017-10-09 14:45:25,https://blog.sicara.com/09-2017-best-ai-new-articles-this-month-df0f2088543d,0,bigdata
5rqh31,Google releases massive visual databases for machine learning,,20,1,psangrene,2017-02-02 23:53:45,http://www.datasciencecentral.com/profiles/blogs/google-releases-massive-visual-databases-for-machine-learning,0,bigdata
5maq4c,Why Dataproc — Google’s managed Hadoop and Spark offering — is a game changer,,22,25,fhoffa,2017-01-06 01:53:18,https://hackernoon.com/why-dataproc-googles-managed-hadoop-and-spark-offering-is-a-game-changer-9f0ed183fda3,0,bigdata
4cqk04,Caravel: Airbnb’s data exploration platform for Druid and SQL databases,,19,0,fjpower,2016-03-31 15:39:56,https://medium.com/airbnb-engineering/caravel-airbnb-s-data-exploration-platform-15a72aa610e5#.gr2tr1h3b,0,bigdata
3vg1xk,"Machine learning works spectacularly well, but mathematicians aren’t quite sure why.",,21,6,Megadragon9,2015-12-04 18:35:27,https://www.quantamagazine.org/20151203-big-datas-mathematical-mysteries/,0,bigdata
3pnls9,Elections Canada makes 42nd election vote data available in CSV for analysis,,20,4,atriou,2015-10-21 17:00:26,http://enr.elections.ca/National.aspx?lang=e,0,bigdata
3ob9hw,20 top free Data science and Big Data courses for newcomers,,21,0,hamzamu,2015-10-11 08:35:40,http://medevel.com/20-top-free-data-science-courses-for-newcomers.html?utm_campaign=shareaholic&utm_medium=reddit&utm_source=news,0,bigdata
27qr3d,Data Science Stack Exchange is now public,,20,0,dostre,2014-06-09 23:57:52,http://datascience.stackexchange.com/,0,bigdata
1t3yfa,"""You will never sell flour and eggs for as much as you can sell a cake,"" said a speaker at the Big Data conference. ""...If you package [the data] and sell some useful insight instead, then it has a higher value and a sustained market.""",,22,1,yourbasicgeek,2013-12-17 19:33:44,http://www.fiercebigdata.com/story/secrets-monetizing-data/2013-12-16,0,bigdata
1h1pie,I am trying to put together a list of subreddits that are related to big data.,,23,17,janhen10,2013-06-25 16:54:20,https://www.reddit.com/r/bigdata/comments/1h1pie/i_am_trying_to_put_together_a_list_of_subreddits/,0,bigdata
1c1jas,Key Big Data Terms You Should Know,,23,4,None,2013-04-10 04:14:05,http://hkotadia.com/archives/5427,0,bigdata
17kn5k9,I've built a tool so that non-tech people can explore data,,20,12,None,2023-10-31 15:05:41,https://v.redd.it/7g7ao76ozjxb1,0,bigdata
z70vlk,Stream Processing vs. Batch Processing: What to Know,[https://medium.com/p/eb8b0480ae6a](https://medium.com/p/eb8b0480ae6a),21,0,Glittering_Bug105,2022-11-28 16:26:50,https://www.reddit.com/r/bigdata/comments/z70vlk/stream_processing_vs_batch_processing_what_to_know/,0,bigdata
yblsxg,P-Values Explained,,20,0,Personal-Trainer-541,2022-10-23 16:17:54,https://youtu.be/IZUfbRvsZ9w,0,bigdata
s3m7me,What is Reddit's tech stack to handle big data?,"I know  Reddit uses Cassandra and PostgreSQL to store data, what other tools does it use for processing and analytics?",20,11,BinodBoppa,2022-01-14 07:11:10,https://www.reddit.com/r/bigdata/comments/s3m7me/what_is_reddits_tech_stack_to_handle_big_data/,0,bigdata
kphm7h,A Book That Analyzes The Most Honest Data Possible - Google Searches,,19,1,sheriffmclawdawg,2021-01-03 09:20:40,https://www.goodreads.com/en/book/show/28512671-everybody-lies,0,bigdata
gwn59m,Apache Hudi graduates to a top level project,,20,1,None,2020-06-04 18:29:56,https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces64,0,bigdata
fl4v3v,Good time to upskill and master Data Science from scratch,,21,1,awsconsultant,2020-03-19 05:47:44,https://medium.com/@codinggeeks/complete-data-science-bootcamp-636b425415d6,0,bigdata
eezxza,The Age of A.I. - An inside view into the diverse applications of AI,,20,0,Curio_ser,2019-12-24 10:56:08,https://www.youtube.com/watch?v=UwsrzCVZAb8,0,bigdata
bd7hue,How Convolutional Neural Networks Work: An Intuitive Approach,,21,0,None,2019-04-14 20:56:54,https://www.youtube.com/watch?v=LaSLR8f0Ws0,0,bigdata
b8i4aa,"Kafka Monitoring with Prometheus, Telegraf, and Grafana",,20,0,techgig11,2019-04-02 11:43:02,https://www.activewizards.com/blog/kafka-monitoring-with-prometheus-telegraf-and-grafana/?utm_source=reddit&utm_medium=bigdata&utm_campaign=kafka,0,bigdata
b6xyk2,Free Big Data Course from Microsoft Available in 2 Days,"This course provides an introduction to data formats, technologies, and techniques, including fundamentals of databases and basic principles for working with Big Data.

[https://www.edx.org/course/introduction-to-big-data-2](https://www.edx.org/course/introduction-to-big-data-2)

&#x200B;

&#x200B;",22,1,edxsocial,2019-03-29 14:29:23,https://www.reddit.com/r/bigdata/comments/b6xyk2/free_big_data_course_from_microsoft_available_in/,0,bigdata
9ykh2n,Humble Book Bundle: Big Data & Infographics,,20,3,jackmayson,2018-11-19 20:15:30,https://twitter.com/BigDataBundle/status/1064608456852697088,0,bigdata
9sdgo6,Practical Apache Spark in 10 minutes. Part 7 - GraphX and Neo4j,,20,0,viktoriia_shulga,2018-10-29 14:16:27,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-7-graphx-and-neo4j/?utm_source=reddit&utm_medium=bigdata&utm_campaign=spark_part7,0,bigdata
9hgq7b,Apache NiFi: Custom Web Scraper Processor – Powered by Selenium,,20,0,J3diMindTricks,2018-09-20 16:30:25,http://blog.davidvassallo.me/2018/09/20/apache-nifi-custom-web-scraper-processor-powered-by-selenium/,0,bigdata
8t0skn,A simple introduction to Apache Flink,,18,10,chemicalX91,2018-06-22 12:03:24,https://medium.com/archsaber/a-simple-introduction-to-apache-flink-2a603119041e,0,bigdata
82707m,Getting started with Data Engineering,,20,0,riky_tree534,2018-03-05 16:09:51,https://medium.com/@richard534/getting-started-with-data-engineering-3d2e728d0c1f,0,bigdata
7zzoph,Apache Kafka foundation of modern data stream processing,,20,3,jak_sky,2018-02-24 21:29:10,https://jakubstransky.com/2016/11/02/introduction-to-apache-kafka-destilled/,0,bigdata
7xj9l5,Data Tokenization is Coming - Here's What You Need to Know,,21,2,Yotaru,2018-02-14 16:35:42,https://tokentops.com/blog/data-tokenization?data,0,bigdata
7vizia,List of online courses to learn Hadoop for beginners,,19,7,golu2017,2018-02-05 23:23:01,https://medium.com/quick-code/top-tutorials-to-learn-hadoop-for-big-data-3fa31f399063,0,bigdata
693nq8,"Spotify open sources Spydra – Ephemeral Hadoop Clusters Using Google Compute Platform (their needs: 2500 nodes and over 100 PBs of capacity, running about 20,000 independent jobs per day)",,20,2,fhoffa,2017-05-03 22:24:20,https://labs.spotify.com/2017/04/28/spydra-ephemeral-hadoop-clusters-using-google-compute-platform/,0,bigdata
5seazp,Making Google Data Studio Free for Everyone,,21,4,fhoffa,2017-02-06 13:45:10,https://analytics.googleblog.com/2017/02/making-google-data-studio-free-for.html,0,bigdata
5n6ayx,Apache Beam graduates from ASF Incubator; first Beam-based Cloud Dataflow SDK available,,20,7,jkestelyn,2017-01-10 17:23:59,https://cloud.google.com/blog/big-data/2017/01/apache-beam-graduates-from-incubation-try-it-today-on-google-cloud-dataflow,0,bigdata
5b12zv,Google's Bigtable paper earns the SIGOPS 2016 Hall of Fame Award (most influential papers >decade),,19,3,fhoffa,2016-11-04 02:16:30,https://cloudplatform.googleblog.com/2016/11/Bigtable-paper-earns-the-SIGOPS-2016-Hall-of-Fame-Award.html,0,bigdata
5alqvz,"Google: Practical advice for analysis of large, complex data sets",,22,0,fhoffa,2016-11-01 21:08:37,http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html,0,bigdata
4z723e,Big data and hidden cameras are emerging as dangerous weapons in the gentrification wars,,20,0,cer1ckson,2016-08-23 15:34:07,http://qz.com/763900/surveillance-and-gentrification/,0,bigdata
4ipnue,Real Time Credit Card Fraud Detection with Apache Spark and Event Streaming,,19,2,caroljmcdonald,2016-05-10 15:11:23,https://www.mapr.com/blog/real-time-credit-card-fraud-detection-apache-spark-and-event-streaming#.VzH57hONa1g.reddit,0,bigdata
474ht4,"Google Cloud Dataproc (managed Spark 1.6.0 and Hadoop 2.7.2) out of beta: Per minute billing, clusters in 90 seconds, custom and preemptive VMs",,21,0,fhoffa,2016-02-23 02:35:03,http://googlecloudplatform.blogspot.com/2016/02/Google-Cloud-Dataproc-managed-Spark-and-Hadoop-service-now-GA.html,0,bigdata
46hbul,Sorting 50 Petabytes at Google,,21,2,thetinot,2016-02-18 22:18:00,https://cloud.google.com/blog/big-data/2016/02/history-of-massive-scale-sorting-experiments-at-google,0,bigdata
3xy9zy,R is the fastest-growing language on StackOverflow,,18,3,steccami,2015-12-23 11:13:02,http://www.r-bloggers.com/r-is-the-fastest-growing-language-on-stackoverflow/,0,bigdata
3i10jw,Vagrant + Spark + Zeppelin a Toolbox to the Data Analyst (or Data Scientist),,19,0,arjones,2015-08-23 00:28:49,http://arjon.es/2015/08/23/vagrant-spark-zeppelin-a-toolbox-to-the-data-analyst/,0,bigdata
3529ya,"Google Cloud launches BigTable, a managed NoSQL database to compete with Hbase and Cassandra",,18,9,thetinot,2015-05-06 14:37:11,http://googlecloudplatform.blogspot.com/2015/05/introducing-Google-Cloud-Bigtable.html,0,bigdata
33b8an,"Hadoop (on Azure) vs BigQuery w/ 63GB of data: Monthly costs $4840 vs $1, while magnitudes faster",,19,17,fhoffa,2015-04-21 02:41:26,http://www.christoolivier.com/hdinsight-hive-vs-bigquery-a-detailed-comparison/,0,bigdata
1bfa89a,Postgres is eating the database world,,20,3,evecryago,2024-03-15 09:51:45,https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4,0,bigdata
15lschp,Virtual Data Builds: A data warehouse environment for every Git commit,,20,0,Pleasant-Guidance599,2023-08-08 19:49:30,https://www.y42.com/blog/virtual-data-builds-one-data-warehouse-environment-for-every-git-commit/,0,bigdata
15griyo,A Comparison of Large Language Models (LLMs) in Biomedical Domain,,19,0,DarronFeldstein,2023-08-03 02:27:47,https://provectus.com/blog/comparison-large-language-models-biomedical-domain/,0,bigdata
12bdeq0,Five big data books that I strongly recommend,"I encourage everyone to take a look at a list of big data books that I've created:

[https://shepherd.com/best-books/big-data-processing-ecosystem](https://shepherd.com/best-books/big-data-processing-ecosystem) 

I am sure that everyone will find something interesting in their domain.  


https://preview.redd.it/0jo0l8fi2ura1.png?width=800&format=png&auto=webp&s=6bbb0b224267652b9ca4961635bd4f3b9cffc523",19,15,tomekl007,2023-04-04 09:10:16,https://www.reddit.com/r/bigdata/comments/12bdeq0/five_big_data_books_that_i_strongly_recommend/,0,bigdata
123hz7h,What challenges do you face with ETL pipelines?," Hello together,

My job is about automating data processing, essentially ETL, for subsequent data science purposes. I'm curious to exchange knowledge with you and learn from each other!

So, I'm curious to learn what data processing techniques you use in your job? What data pipeline tools do you use/ dont use? What are the biggest challenges you face?

Thank you for your help!",18,1,Puzzled-Lime141,2023-03-27 09:51:39,https://www.reddit.com/r/bigdata/comments/123hz7h/what_challenges_do_you_face_with_etl_pipelines/,0,bigdata
11q5zs9,"Release 0.11 of OpenDataDiscovery Platform w/ metrics, search explanations & new dataset structure","Hey, I'm one of the contributors to OpenDataDiscovery platform and would like to share a release update and/or introduce you to ODD as well!

&#x200B;

For the ones who're not familiar, the ODD Platform is an open-source data discovery and observability tool that enables businesses to standardize data collection, improve the compatibility of different data catalogs, augment and expand their data lineage capabilities, and improve the way data quality and observability are implemented.

&#x200B;

So, the v0.11 release of the ODD Platform introduces the following enhancements:

\- Metrics: With the implementation of Open Metrics Specification, ODD Platform introduces enhanced Metrics support. Users can ingest metrics for their data entities or dataset fields and observe the last metric values, right within the platform.

\- Search explanations: It is crucial to understand why a certain entity has appeared in search results. The ODD Platform can now display all user query entries in a specific entity in the Search API.

\- New Dataset Structure UI: The Dataset Structure UI has been reworked and enhanced, to provide a better experience for users in the Data Discovery phase.

&#x200B;

Get to know about OpenDataDiscovery: [https://opendatadiscovery.org/](https://opendatadiscovery.org/)

Source code: [https://github.com/opendatadiscovery/odd-platform](https://github.com/opendatadiscovery/odd-platform)",18,1,Haarolean,2023-03-13 10:26:27,https://www.reddit.com/r/bigdata/comments/11q5zs9/release_011_of_opendatadiscovery_platform_w/,0,bigdata
vaeoyn,Researchers From China Introduce ‘FedPerGNN’: A New Federated Graph Neural Network (GNN) Framework For Both Effective And Privacy-Preserving Personalization,"👉  A privacy-preserving user-item graph extension protocol to expand local graphs and convey high-order information while maintaining privacy 🔒

👉  FedPerGNN yields 📉 4.0% – 9.6% reduced errors than state-of-the-art federated customization algorithms under adequate privacy protection, according to experimental results on six datasets for personalization in diverse circumstances. 

👉  Furthermore, this method is not restricted to the customization scenario. It may be used as a fundamental strategy for privacy-preserving data mining on decentralized graph data, thus facilitating research in various domains involving graph-structured data.

[Continue reading](https://www.marktechpost.com/2022/06/11/researchers-from-china-introduce-fedpergnn-a-new-federated-graph-neural-network-gnn-framework-for-both-effective-and-privacy-preserving-personalization/) | *Check out the*[ ](https://www.prnewswire.com/news-releases/introducing-the-digital-upcycling-project-by-tilda-the-first-ai-artist-by-lg-ai-research-301561017.html)[*paper*](https://www.nature.com/articles/s41467-022-30714-9#citeas) *and* [*github*](https://github.com/wuch15/FedPerGNN)",21,0,No_Coffee_4638,2022-06-12 04:59:34,https://www.reddit.com/r/bigdata/comments/vaeoyn/researchers_from_china_introduce_fedpergnn_a_new/,0,bigdata
l380mc,Databricks Raising Funds At $27B Valuation: Report,,18,0,The-Techie,2021-01-23 07:56:28,https://www.thetechee.com/2021/01/databricks-raising-funds-at-27b.html,0,bigdata
kii481,GCP vs AWS for data engineering?,"I've been using GCP for data engineering and like it a lot. From my experience, dataflow, BigTable, BigQuery, and PubSub work well and make it easy to be productive. I very briefly used AWS, but not in any real capacity. Didn't like it too much, but maybe it would have grown on me. 

How do other people like using GCP/AWS for data engineering?

For people who have used both, do you have a preference and why?",19,4,fake_actor,2020-12-23 00:37:04,https://www.reddit.com/r/bigdata/comments/kii481/gcp_vs_aws_for_data_engineering/,0,bigdata
kbjm6t,Which is better Apache Nifi Vs Apache Airflow,"I am getting started with workflows and had a usecase , reding the data from json sources , avro format and keep the data in kafka and further picked up spark streaming to do some stream processing, which tool is better with pros and cons ? thanks",19,14,nag9s,2020-12-12 05:35:04,https://www.reddit.com/r/bigdata/comments/kbjm6t/which_is_better_apache_nifi_vs_apache_airflow/,0,bigdata
glyfow,Kafka Removing Zookeeper Dependency,,19,0,rmoff,2020-05-18 09:41:17,https://www.confluent.io/blog/removing-zookeeper-dependency-in-kafka/,0,bigdata
g48pi4,Python Data Engineering Tools: The Next Generation,,19,0,will_flwrs,2020-04-19 14:38:39,https://medium.com/@will_flwrs/python-data-engineering-tools-the-next-generation-354e00f2f060?source=friends_link&sk=69097c6f649cf114c9000e7268d45015,0,bigdata
ej5ag0,ELI5 Apache Pulsar vs Apache Kafka,"Doing a bit of google searching and not finding a simple explanation of what the real differences are, and why to choose Pulsar over Kafka and vice versa.  Any insights?",19,14,shufflingshuffler,2020-01-02 22:15:34,https://www.reddit.com/r/bigdata/comments/ej5ag0/eli5_apache_pulsar_vs_apache_kafka/,0,bigdata
edg5yw,"If Data is the New Oil, How to Determine Its Value?",,19,0,Yuqing7,2019-12-20 21:13:32,https://medium.com/syncedreview/if-data-is-the-new-oil-how-to-determine-its-value-9b1d14cd5a57,0,bigdata
dh6f2a,List of Best Online Courses to Learn Big Data,,19,0,singhpankaj99,2019-10-13 05:19:27,https://blog.coursesity.com/learn-big-data-tutorials-for-beginner/,0,bigdata
dek6r2,How to Get Certified in Spark by Databricks," 

I recently passed the databricks pyspark certification (*Databricks Certified Developer - Apache Spark 2.x for Python*).

I gathered some tips for those who would like to take and pass the certification.

Tell me what you think and do not hesitate to ask me anything!

[https://www.sicara.ai/blog/2019-10-07-how-to-get-certified-in-spark-by-databricks](https://www.sicara.ai/blog/2019-10-07-how-to-get-certified-in-spark-by-databricks)",18,5,florian_c,2019-10-07 14:28:21,https://www.reddit.com/r/bigdata/comments/dek6r2/how_to_get_certified_in_spark_by_databricks/,0,bigdata
c30fq6,Cool Big Data Study on Why Microsoft Employees are Miserable,,18,1,savorymonk,2019-06-20 19:34:04,https://www.nytimes.com/2019/06/15/upshot/how-to-win-neil-irwin.html,0,bigdata
bi3jgj,"Delta Lake a new open source project - ACID transactions, versioning, and schema enforcement to Apache Spark","My take on Delta Lake  is a new open source project from Databricks  with some sample Scala examples to get started with Delta Lake

This adds ACID transactions, versioning, schema enforcement and lot of new features to Spark data sources that don't have them already!

[https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda](https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda)",18,3,akhilanandbv003,2019-04-27 20:59:15,https://www.reddit.com/r/bigdata/comments/bi3jgj/delta_lake_a_new_open_source_project_acid/,0,bigdata
aujydb,An interview about what data engineers need to know about deep learning,,18,0,blarghmatey,2019-02-25 10:49:32,https://www.dataengineeringpodcast.com/deep-learning-data-engineers-episode-71/,0,bigdata
ah7qe6,"Comparing Hadoop, MapReduce, Spark, Flink, and Storm",,19,2,Big_Data_Path,2019-01-18 06:39:14,https://bigdatapath.wordpress.com/2019/01/18/comparing-hadoop-mapreduce-spark-flink-and-storm/,0,bigdata
agcpc1,The solutions for data the size of which is close to the limits of modern hardware! (Developer's description with code examples),,20,4,beezwild,2019-01-15 20:11:03,https://atomicuschart.com/features/bigdata/,0,bigdata
abhrt5,How Web Scraping is Transforming the World with its Applications,,19,0,hiren_p,2019-01-01 12:14:23,https://towardsdatascience.com/https-medium-com-hiren787-patel-web-scraping-applications-a6f370d316f4,0,bigdata
a7lubu,What should I do after the Stanford SQL course?,"With help of the Reddit community I came across one of the most engaging course I have ever done, the Stanford Database course taught by professor Jennifer Widom : [link to the course](https://lagunita.stanford.edu/courses/Home/Databases/Engineering/about). The specialisation contains many courses, I completed the Relational Algebra course, and then the SQL one. My main goal was to learn the basics of SQL.

After completing the Stanford course, I know a fair bit about basic querying in SQL(thanks to the large amount of practice problems the course provides) but that's about it. Not much about creating tables, just a little bit about indexing.

About me : I am a Computer Science graduate(2018) who wants to be a data engineer in the future, currently I am not looking for specifically data engineer jobs, but any data related jobs that will get me started.

I want to up my knowledge on SQL, and there are just so many pathways and courses on the web it is really overwhelming. Will love to know from the community what should I do next to learn and practice more and more SQL?

Few options that come to my mind are :

1. Complete the full Stanford database course(because I have only done Relational Algebra and SQL)
2. A SQL related project.
3. SQL challenges on Leetcode or other websites.
4. A new course related to SQL that contains as much practicing SQL as this Stanford course had (would love to know such course)
5. Something else.

I will really love to know the next step, feeling really stuck right now.",19,18,None,2018-12-19 12:08:09,https://www.reddit.com/r/bigdata/comments/a7lubu/what_should_i_do_after_the_stanford_sql_course/,0,bigdata
9l9jxt,Cloudera and Hortonworks shares skyrocket as rivals merge,,19,3,nikhilaggarwal0711,2018-10-04 06:52:31,https://www.cnbc.com/2018/10/03/cloudera-and-hortonworks-announce-all-stock-merger.html,0,bigdata
9aku1l,How can I prepare for my Big Data technical interview?,"Hello fellow programmers,

On Thursday, August 30th, I have a technical interview for a Jr. Software Engineer position for a company in Atlanta, GA. They want me to review the following bullet points prior to my interview:

• Hive Bucketing & Partioning

• Spark RDDs

• Dataframes

• Dataset differences

• Spark vs MapReduce

• Data warehousing

• Star Schema

• Table Join basics

• AWS

• AWS Lambda Functions

• Kinesis vs Kafka

I have a few questions for you guys with experience with Big Data companies.

1.) When it comes to partitioning and bucketing, does the company want me to learn the basic concepts (ie. what they do) or learn how to make queries with the two?

2.) When it comes to the technical part of the interview (ie. code in Python), what exact concepts should I review exactly? I'm aware I have to know basic join tables, AWS Lambda functions in Python, but what else?

3.) Where can I review AWS Lambda functions online?",19,13,Hegemon1984,2018-08-27 01:26:45,https://www.reddit.com/r/bigdata/comments/9aku1l/how_can_i_prepare_for_my_big_data_technical/,0,bigdata
95hcns,"Checkout these tutorials that I found, Big Data explained in a very simplified way.",Big Data & Hadoop Tutorials: https://www.youtube.com/playlist?list=PLr0GuAb3MR-wEDZwetSvzvc-CKVSGruhq,19,1,sa-shahzan,2018-08-08 00:44:55,https://www.reddit.com/r/bigdata/comments/95hcns/checkout_these_tutorials_that_i_found_big_data/,0,bigdata
8tf691,Kafka Tutorial for Fast Data Architecture,,19,0,admintome,2018-06-24 03:33:15,http://www.admintome.com/blog/kafka-tutorial-for-fast-data-architecture/,0,bigdata
8lrmgu,Practical Apache Spark in 10 minutes. Part 2 - RDD,,18,0,viktoriia_shulga,2018-05-24 10:37:07,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes-part-2-rdd/?utm_source=reddit&utm_medium=bigdata&utm_campaign=rdd,0,bigdata
8cwc36,Big Data Cheat Sheet: Batch Storage,,21,0,mniejiki,2018-04-17 13:01:49,https://mindfulmachines.io/blog/2018/4/10/series-big-data-batch-storage,0,bigdata
6vtign,The Hadoop Ecosystem Table,,17,1,liranbh,2017-08-24 20:13:39,https://hadoopecosystemtable.github.io,0,bigdata
6fswtr,The 9 Best Free Online Big Data And Data Science Courses,,21,1,maribethharrold,2017-06-07 11:50:10,https://www.forbes.com/sites/bernardmarr/2017/06/06/the-9-best-free-online-big-data-and-data-science-courses/,0,bigdata
6bll1x,"Google Cloud Spanner is now production-ready (horizontally-scalable, strongly-consistent, relational database service)",,17,3,fhoffa,2017-05-17 01:07:48,https://cloudplatform.googleblog.com/2017/05/Cloud-Spanner-is-now-production-ready-let-the-migrations-begin.html,0,bigdata
60ocij,Inside Nest's “smart home” data pipeline,,20,2,jkestelyn,2017-03-21 15:48:34,https://nest.tech/google-cloud-dataflow-in-the-smart-home-data-pipeline-5ae71781b856#.ta15w593b,0,bigdata
46atj7,New Columnar In-Memory Analytics Framework Apache Arrow,,19,1,None,2016-02-17 20:35:04,http://www.dremio.com/blog/apache-arrow/,0,bigdata
42qmm9,Deep Learning with Spark and TensorFlow,,18,0,alpyhp,2016-01-26 08:25:40,https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html,0,bigdata
3f9v0r,Big Data Analytics Youtube & TED Video Tutorials for Beginners,,19,1,john_philip,2015-07-31 10:14:32,http://www.analyticsvidhya.com/blog/2015/07/big-data-analytics-youtube-ted-resources/,0,bigdata
2xey45,"""Data scientist"" added to UK skills shortage list; data scientists fast tracked for work visa",,18,2,None,2015-02-28 00:49:30,http://www.ft.com/cms/s/0/5a2f76e4-bcf0-11e4-9902-00144feab7de.html,0,bigdata
2hcm4c,20 Big Data Repositories You Should Check Out,,17,1,urinec,2014-09-24 17:53:48,http://www.datasciencecentral.com/profiles/blogs/20-free-big-data-sources-everyone-should-check-out,0,bigdata
26oqdy,Comparison: Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase vs Couchbase vs Neo4j vs Hypertable vs ElasticSearch vs Accumulo vs VoltDB vs Scalaris,,18,3,lukadremelj,2014-05-28 11:20:16,http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis,0,bigdata
1mjosx,"Upcoming courses in Statistics, Data Analysis and R (repost from /r/datascience)","I figured these courses start this week might be interesting to some people here:

[Statistics One](https://class.coursera.org/stats1-002/auth/auth_redirector?type=login&subtype=normal)
Starts Sept 22nd.
This one will be taught using R to learn the basics of statistics

[Computing for Data Analysis](https://www.coursera.org/course/compdata)
Starts Sept 23rd.
In this course you will learn how to program in R and how to use R for effective data analysis.  Some of my co-workers did this one last year and really enjoyed it.

Finally, after the ""Computing for Data Analysis"" course above ends, there is a course which appears to be a continuation with applications in analysis of real world data:

[Data Analysis](https://www.coursera.org/course/dataanalysis)
Starts Oct 13th.
This course is an applied statistics course focusing on data analysis. The course will begin with an overview of how to organize, perform, and write-up data analyses.

Anyone else going to follow along in any of these courses?  I figured it's time that I finally learn R this Fall...",18,7,burgersmoke,2013-09-17 02:29:10,https://www.reddit.com/r/bigdata/comments/1mjosx/upcoming_courses_in_statistics_data_analysis_and/,0,bigdata
16ez6nl,Polars’ New ‘Sink_CSV’ Function Capable of Handling Inner Join of Ten Billion Rows,,17,3,100GB-CSV,2023-09-10 12:33:32,https://i.redd.it/5sss9t90afnb1.jpg,0,bigdata
14hlzad,"TIL that — through the lens of modern data analytics — a simple voice recording can yield thousands of speech parameters that may reveal a user’s biometric identity, body shape, personality traits, mental & physical health, emotions, drug habits, geographical origin, and socioeconomic status.",,19,1,VarunTossa5944,2023-06-24 07:00:16,https://rd.springer.com/chapter/10.1007/978-3-030-42504-3_16,0,bigdata
y5x5ij,The 5-minute guide to using bucketing in Pyspark,,17,2,luminoumen,2022-10-17 00:58:43,https://luminousmen.com/post/the-5-minute-guide-to-using-bucketing-in-pyspark,0,bigdata
xv0go5,"Alex Merced makes the case for Data Lakehouses, Apache Iceberg and Dremio in 3 minutes",,17,3,amdatalakehouse,2022-10-04 00:39:27,https://v.redd.it/x6e5vs8ypor91,0,bigdata
wdmryr,Python Data Visualisation,,18,0,marklit,2022-08-01 17:12:07,https://tech.marksblogg.com/python-data-visualisation-charts-graphs-plots.html,0,bigdata
scepbf,Apache Kafka 3.1 has been released,,17,0,rmoff,2022-01-25 14:14:32,/r/apachekafka/comments/scep2r/apache_kafka_31_has_been_released/,0,bigdata
qix13u,Python Financial Stock analysis - Stock Data using Yahoo Finance and Jupyter,,16,0,Best_Fold_2554,2021-10-30 08:17:53,https://medium.com/vinsloev-academy/python-financial-stock-analysis-algo-trading-4d5304d07416,0,bigdata
onuwh1,Data Scientists and ML Engineers Are Luxury Employees,,18,0,yiriwer,2021-07-20 04:30:41,https://www.thinkdataanalytics.com/data-scientists-and-ml-engineers-are-luxury-employees/,0,bigdata
o2wrly,Machine Learning is like p*rn,,17,2,RRR777R7,2021-06-18 18:27:49,https://www.datascienceirl.com/machine-learning-is-like-p-rn/,0,bigdata
isl3gl,Diary of a Data Engineer,,17,0,ozzyboy,2020-09-14 13:43:28,https://lakefs.io/2020/09/14/diary-of-a-data-engineer/,0,bigdata
hei5yk,We've built a data engineering tool to make writing Spark code much easier,"Hi all!

We're a small team of ex-spark/hadoop/hive engineers & compilers experts, who has been hard at work to develop a ‘Code-First Data Engineering’ product. We’re super excited to announce the Public beta of Prophecy Data Engineering today!

It includes a ‘Code=Visual’ editor, where you can toggle between a fully-featured coding IDE and a visual graph editor, to author Spark code that is standardized, performant and maintainable.

Let us know what we can do better and what you’re loving! You can sign up for beta at: [https://www.prophecy.io/blogs/prophecy-public-beta](https://www.prophecy.io/blogs/prophecy-public-beta)",19,4,several27,2020-06-23 16:49:00,https://www.reddit.com/r/bigdata/comments/hei5yk/weve_built_a_data_engineering_tool_to_make/,0,bigdata
glbdff,Hadoop Distributed File System - A comprehensive guide,"From a computing perspective, there are essentially 2 types of scaling — vertical and horizontal. In vertical scaling, we simply add more RAM and storage to a single computer/machine aka “node”. In horizontal scaling, we add more nodes connected through a common network, thereby increasing the overall capacity of the system. With that in mind, let’s dive in.

**Block Size**

&#x200B;

[ File split into blocks](https://i.redd.it/qmd4qw5y2az41.gif)

When a file is saved in HDFS, the file is broken into smaller chunks or “blocks”, as can be seen in the GIF above. The number of blocks is dependent on the “Block Size”. The default is *128 MB* but can be changed/configured easily.

In our example, a 500 MB file needs to be broken into blocks of 128 MB. *500/128 = 3 blocks of 128 MB and 1 block of 116 MB.* The residual block space of 12 MB is returned back to the name node for usage elsewhere, thus preventing any wastage. This is true of any file system really, for example, Windows NTFS has a block size between 4 KB and 64 KB depending on file size (up to 256 TB). Considering petabytes and above for Big Data processing, KBs would be highly inefficient, as you can imagine. This is why HDFS has a block size of 128 MB.

**Replication Factor**

&#x200B;

[ Replication](https://i.redd.it/grt1zi433az41.gif)

HDFS is a fault-tolerant and resilient system, meaning it prevents a failure in a node from affecting the overall system’s health and allows for recovery from failure too. In order to achieve this, data stored in HDFS is automatically replicated across different nodes.

How many copies are made? This depends on the “replication factor”. By default, it is set to 3 i.e. 1 original and 2 copies. This is also easily configurable.

In the GIF to the left, we see a file broken into blocks and each block replicated across other data nodes for redundancy.

**Storage and Replication Architecture**

&#x200B;

[ Storage and Replication Architecture](https://preview.redd.it/wks8jpj63az41.png?width=2500&format=png&auto=webp&s=ba698b9e9537064fb591e975f53d6367258fc9ff)

Hadoop Distributed File System (HDFS) follows a *Master — Slave* architecture, wherein, the ‘Name Node’ is the master and the ‘Data Nodes’ are the slaves/workers. This simply means that the name node monitors the health and activities of the data node. The data node is where the file is actually stored in blocks.

Let's continue with the same example of a file of size = 500 MB in the image above. With HDFS’ default block size of 128 MB, this file is broken into 4 blocks B1 — B4. Please note that A — E are our Data Nodes. With HDFS’ default replication factor of 3, the blocks are replicated across our 5 node cluster. Block B1 (in yellow) is replicated across Nodes A, B and D and so on and so forth (follow the coloured lines).Here, the Name Node maintains the metadata, i.e. data about data. *Which replica of which block of which file is stored in which node* is maintained in NN — replica 2 of block B1 of file xyz.csv is stored in node B.

So a file of size 500 MB requires a total storage capacity in HDFS of 1500 MB due to its replication. This is abstracted from the end users’ perspective and the user can only see 1 file of size 500 MB stored within HDFS.

Continue reading here - [Hadoop Distributed File System](https://towardsdatascience.com/hadoop-distributed-file-system-b09946738555)",17,4,ppnimkar,2020-05-17 07:34:55,https://www.reddit.com/r/bigdata/comments/glbdff/hadoop_distributed_file_system_a_comprehensive/,0,bigdata
g1tnmv,Ten cheatsheets for data science and machine learning,,20,0,TheTesseractAcademy,2020-04-15 15:05:34,https://thedatascientist.com/data-science-machine-learning-cheatsheets/,0,bigdata
f7camr,Is SMACK the most popular data engineering tech stack?,"I have recently heard of SMACK (Spark, Meso, Akka, Cassandra, Kafka), tech stack in data engineering. 

Is SMACK the most popular tech stack right now and in the foreseeable future? Does a data engineer need or are better to be familiar with all the five components? 

Is there other competitive/promising alternative tech stack?

Thanks.",17,14,timlee126,2020-02-21 15:02:30,https://www.reddit.com/r/bigdata/comments/f7camr/is_smack_the_most_popular_data_engineering_tech/,0,bigdata
e6gxa2,What is Canonical URL and why it is so Important?,,17,0,powhound84,2019-12-05 13:06:02,https://medium.com/hexact/what-is-canonical-url-and-why-it-is-so-important-e13d438f7567,0,bigdata
dqxi2m,Spark tips. Don't collect data on driver,,18,1,luminoumen,2019-11-03 08:02:01,https://luminousmen.com/post/spark-tips-dont-collect-data-on-driver,0,bigdata
c26bhd,Cloudera's Data Platform (CDP) Webinar,"What did you guys think of the webinar on CDP today? My company is looking into it tentatively. I'd love to hear from y'all, especially others who are considering it, what you thought.",19,6,SrirachaPizza,2019-06-18 19:02:10,https://www.reddit.com/r/bigdata/comments/c26bhd/clouderas_data_platform_cdp_webinar/,0,bigdata
b0er2b,Spark join internals,"Checkout my article on the internals of joins in spark. It covers how joins work and how we can impact the join performance.  
[https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c](https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c)

&#x200B;

Any comments or suggestions to improve this blog are really appreciated. I am just a month old to  blogging , and I am still learning.",16,3,akhilanandbv003,2019-03-12 23:27:41,https://www.reddit.com/r/bigdata/comments/b0er2b/spark_join_internals/,0,bigdata
amq8ww,Data engineer getting up to the speed,"Hi all, 

I have recently started working as a data engineer at a fintech company. The company is investing heavily in data driven decisions and everything is very fast-paced. I am good at coding. I know quite a lot of Apache Spark but what I lack is the exposure of building blocks of a data pipeline. Specifically, if you would ask me to build a pipeline from scratch then I might come up with the tools but might not be able to compare different options. I wouldn't not be able to list pros and cons. I know in many of these cases you need solid hands-on experience. But I see a great opportunity in my company to climb up the ladder if I can prove myself in the short period of time. I also don't have hands-on experience of AWS tools. I am willing to put effort and time but I want to have a clear road map before. 

So, I would like to ask what sample applications, blogs, books or tutorial I should start off immediately? I would like to get deeper insight of data pipeline architecture. And, if any of you could point to a resource explaining how to use machine learning in real-time analysis then that would be great. 
Thanks in advance 🙂",16,4,muddasser-h,2019-02-03 15:00:01,https://www.reddit.com/r/bigdata/comments/amq8ww/data_engineer_getting_up_to_the_speed/,0,bigdata
a0xxqx,Three billion that's some big data,,18,8,mikeff8,2018-11-27 19:09:01,https://i.redd.it/3ji0ul6r9x021.jpg,0,bigdata
9ws6p2,"5 Must-read Books on Data Science, Analytics and Big Data",,17,5,developFFM,2018-11-13 19:25:11,https://www.aisoma.de/must-read-books-on-big-data-analytics/,0,bigdata
9aopyq,Fast Stream Processing On Kafka In Python With Faust (Interview),,19,0,blarghmatey,2018-08-27 13:00:15,https://www.podcastinit.com/fast-stream-processing-in-python-using-faust-with-ask-solem-episode-176/,0,bigdata
97w4c3,Why I’m sick of ETL tools and what I use instead (hint: it’s a different ETL tool),,17,5,gavlaaaaaaaa,2018-08-16 20:59:31,https://medium.com/@lewisdgavin/why-im-sick-of-etl-tools-and-what-i-use-instead-hint-it-s-a-different-etl-tool-c2e9c961a3f,0,bigdata
91h5w0,Kafka on Kubernetes - using etcd (instead of Zookeeper),,18,0,matyix_,2018-07-24 12:59:15,https://banzaicloud.com/blog/kafka-on-etcd/,0,bigdata
8tu6w8,Kafka Python Tutorial for Fast Data Architecture,,17,0,admintome,2018-06-25 21:09:15,http://www.admintome.com/blog/kafka-python-tutorial-for-fast-data-architecture/,0,bigdata
8l5b26,Why would I want to use kubernetes over Yarn or Mesos for Spark?,What are the tradeoffs that I am getting for deploying it via kubernetes? ,17,1,None,2018-05-21 23:32:05,https://www.reddit.com/r/bigdata/comments/8l5b26/why_would_i_want_to_use_kubernetes_over_yarn_or/,0,bigdata
8bz38w,Shattering the Trillion-Rows-Per-Second Barrier With MemSQL,,18,0,furqanasghar85,2018-04-13 13:19:08,http://www.dataengconf.com/blog/shattering-the-trillion-rows-per-second-barrier-with-memsql,0,bigdata
7u64f2,A comprehensive Apache Spark Streaming tutorial with examples.,,17,0,lawrencejones617,2018-01-31 01:26:02,http://www.irfanelahi.com/data-science/apache-spark-streaming-tutorial-examples/,0,bigdata
7luq8l,DataScience & Big Data Job Market Trends in 2018,,19,3,tanmoyray01,2017-12-24 11:28:06,https://www.stoodnt.com/blog/285/how-to-get-data-science-and-machine-learningai-jobs-how-to-become-a-data-scientist,0,bigdata
74xm1u,"Gorilla: A Fast, Scalable, In-Memory Time Series Database (PDF)",,18,3,based2,2017-10-07 21:28:40,http://www.vldb.org/pvldb/vol8/p1816-teller.pdf,0,bigdata
70tks5,1.1 Billion Taxi Trips on 3 Raspberry Pis running Spark 2.2,,18,2,marklit,2017-09-18 08:01:06,http://tech.marksblogg.com/billion-nyc-taxi-rides-spark-raspberry-pi.html,0,bigdata
6ug1z6,"Game Of Thrones S07 E05-Tweet Analysis with Kafka, BigQuery and Tableau",,19,0,fhoffa,2017-08-18 06:22:53,https://www.rittmanmead.com/blog/2017/08/how-was-game-of-thrones-s07-e05-tweet-analysis-with-kafka-bigquery-and-tableau/,0,bigdata
65kgiy,Do you guys recommend any resources for learning Spark as a beginner?,"I am an experienced developer but new to the big data ecosystem. Looking to get into Spark. I am familiar with Java and would consider learning Scala also, Is there a benefit to using Scala vs Java with Spark?",16,4,zergUser1,2017-04-15 18:01:39,https://www.reddit.com/r/bigdata/comments/65kgiy/do_you_guys_recommend_any_resources_for_learning/,0,bigdata
5qf3vh,Top 20 Big Data Experts to Follow,,18,1,psangrene,2017-01-27 04:13:59,http://www.datasciencecentral.com/profiles/blogs/top-20-big-data-experts-to-follow-1,0,bigdata
5b4zxz,"Kafka, meet Bruce: How one service boosts app messaging reliability | TechBeacon",,18,2,amirshk,2016-11-04 17:41:28,http://techbeacon.com/when-kafka-met-bruce-how-one-service-can-boost-app-messaging-reliability,0,bigdata
4xk1hf,Discovering ketosis: how to effectively lose weight at master 路 machine learning 路 GitHub,,17,0,iamondemand,2016-08-13 17:27:45,https://github.com/arielf/weight-loss/blob/master/README.md,0,bigdata
3qu8im,Greenplum Database is now open source,,16,8,muz3m,2015-10-30 13:56:35,http://blog.pivotal.io/big-data-pivotal/news/introducing-greenplum-worlds-first-open-source-mpp-data-warehouse,0,bigdata
2e0h60,Google unveils Mesa - Geo-Replicated Near-Realtime Scalable Data Warehouse,,18,0,mhausenblas,2014-08-19 19:44:12,http://www.infoq.com/news/2014/08/google-data-warehouse-mesa,0,bigdata
2dxacm,50 Data Science Gurus (and 10 Organizations) You Must Follow On Twitter,,16,3,JamesKotecki,2014-08-18 21:53:26,http://blog.automatedinsights.com/post/95129473817/50-data-science-gurus-and-10-organizations-you-must,0,bigdata
1xqbvb,Free Stanford University Book on Mining Massive Datasets,,17,2,KeponeFactory,2014-02-12 19:11:47,http://infolab.stanford.edu/~ullman/mmds.html,0,bigdata
1ih6t8,10 sites to get the large data set or data corpus for free. Help to test BigData projects.,,18,0,geekganesh,2013-07-17 10:06:59,http://www.findbestopensource.com/article-detail/free-large-data-corpus,0,bigdata
13mmbcb,Is the apache spark project moving away from scala?,"On databricks' page it says that they're developing a new engine for spark, called ""photon"" and that it will be written in C++ and support SQL, Dataframe, and pandas APIs. [https://www.databricks.com/product/photon](https://www.databricks.com/product/photon)

So, since there will be no more Dataset API, do you think most people will switch to python and scala will become less popular for spark development?",18,13,Outrageous-Heat-6353,2023-05-20 08:36:59,https://www.reddit.com/r/bigdata/comments/13mmbcb/is_the_apache_spark_project_moving_away_from_scala/,0,bigdata
131bjs9,RBAC 101: How to Establish Effective Role-Based Access Control for Your Data Catalog,,17,0,dahmedahe,2023-04-27 23:49:44,https://blog.opendatadiscovery.org/rbac-101-how-to-establish-effective-role-based-access-control-for-your-data-catalog-dd919ce5145c,0,bigdata
12ak3pr,Data Modeling — The Unsung Hero of Data Engineering: An Introduction to Data Modeling,,16,0,sspaeti,2023-04-03 13:14:04,https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-introduction,0,bigdata
11h28z1,Get free early release copy of “Apache Iceberg: The Definitive Guide”,,18,4,AMDataLake,2023-03-03 14:33:29,https://i.redd.it/9h3jg6retkla1.jpg,0,bigdata
uxm4we,Amazon Researchers Develop A New Way To Rewrite Database Queries To Eliminate Redundancies,"As databases grow in size, queries become slower. Database queries frequently contain multiple repetitive procedures that can be eliminated. To discover a full name record in a database for a specific individual, database queries to fetch all entries for the first name, then all entries for the second name, and compute their intersection. This requires accessing the database twice, which is a duplication that can be eliminated to save data retrieval time.

Amazon researchers present a method for rewriting complicated SQL queries to eliminate redundancy. It requires more computation after retrieval but is more efficient than doing numerous queries on the same table. Testing on the TPC-DS benchmark database with 3TB of data, their approaches reduced overall execution time by 14% compared to the baseline. When they limited their analysis to queries directly changed by their rewrite rules, they saw a 60% boost in speed, with some queries processing more than six times faster.

To enhance queries, it is critical to understand what the entire query is attempting to accomplish. A query plan generates this blueprint. A query plan is a series of actions used to perform a query, such as data scans and aggregations. Selecting the most efficient query plan from a vast number of feasible possibilities is known as query plan optimization.

[Continue Reading](https://www.marktechpost.com/2022/05/25/amazon-researchers-develop-a-new-way-to-rewrite-database-queries-to-eliminate-redundancies/)

Paper: https://assets.amazon.science/1b/08/6545f27b48248c08fdafb8d43055/computation-reuse-via-fusion-in-amazon-athena.pdf",17,1,No_Coffee_4638,2022-05-25 17:13:37,https://www.reddit.com/r/bigdata/comments/uxm4we/amazon_researchers_develop_a_new_way_to_rewrite/,0,bigdata
us9zm1,Apache Kafka 3.2 has been released,,17,0,rmoff,2022-05-18 10:44:12,/r/apachekafka/comments/urq1fa/apache_kafka_32_has_been_released/,0,bigdata
uck515,Project Nessie Update (Open Source Data Lake Table Catalog with Git-like features),,15,0,AMDataLake,2022-04-26 19:07:50,https://i.redd.it/01sayn9c9xv81.jpg,0,bigdata
ubfw6u,Expedia Data Science Interview Questions,,16,1,dataleaker,2022-04-25 08:03:05,https://interviewleaks.medium.com/expedia-leaked-data-science-interview-questions-f779de41a0a,0,bigdata
tbfwmk,Which database should i choose? why are there so many choices?,"My company is developing a new data intensive IoT project, and we are in selection of database. We have about 1000+ tankers to monitor. We only used MySQL before, but we were told that it won't support such a use case. And it seems that there are too many databases, we are completely lost. Need some real help.",17,23,Significant_Tune9219,2022-03-11 02:42:19,https://www.reddit.com/r/bigdata/comments/tbfwmk/which_database_should_i_choose_why_are_there_so/,0,bigdata
nw1sqy,How Airbyte Raised Its Series-A Round 2 Months after Its Seed,,17,0,horizone,2021-06-09 17:47:07,https://airbyte.io/blog/how-airbyte-raised-its-series-a-round-2-months-after-its-seed,0,bigdata
ni4uva,5 Data Engineering Project Ideas To Put On Your Resume,,17,0,nonkeymn,2021-05-21 22:41:03,https://youtu.be/385mKftVr3I,0,bigdata
nhof3s,12 Data Visualization Software Tools,,18,1,Big_Data_Path,2021-05-21 09:30:49,https://bigdatapath.wordpress.com/2021/05/21/12-data-visualization-software-tools/,0,bigdata
mz3lef,What Is The Difference Between A Data Engineer Vs A Software Engineer - And Why Is It Hard To Pinpoint Sometimes,"Last week someone responded to one of my posts and mentioned that in some cases data engineers are hard to define because in some roles they are more on the engineering and infrastructure  side and in others are on the data/analytics side.

Another way to put this is that in some cases data engineers are software engineers that focus on data infrastructure where as in other cases they focus heavily on building data-pipelines and other BI type work.  

These are generally two varying skillsets that range pretty wildy on a spectrum.

One type of data engineer could be better compared to a software engineer and the other could be better compared to a data scientist and a data analyst.

Based on this I attempted to flesh out this idea more in a current video where I discuss the challenges and nuances in these kind of arch-types of data engineers. For example, in my experience I have noticed that at large [non-tech companies and at start-ups data engineers](https://youtu.be/0wjngjhmtJA?t=205) can often take a more software like role because they need to develop a lot of the data infrastructure themselves.

The other thing I noticed is how data engineers try to solve problems differs from software engineers. For example, I often see that software engineers put a lot of their logic in their [application layer where as a lot of data engineers put it in the SQL/database layer](https://youtu.be/0wjngjhmtJA?t=244).

The other things I noted were salary and what some other developers opinions.

What have you guys noticed are the difference overall?",17,14,nonkeymn,2021-04-26 17:53:36,https://www.reddit.com/r/bigdata/comments/mz3lef/what_is_the_difference_between_a_data_engineer_vs/,0,bigdata
m72pkl,Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale,,17,0,_shadowbannedagain,2021-03-17 15:17:35,https://jet-start.sh/blog/2021/03/17/billion-events-per-second,0,bigdata
j9q5pk,Introducing lakeview: A Visibility Tool for AWS S3 Based Data Lakes,,17,0,ozzyboy,2020-10-12 12:41:25,https://lakefs.io/2020/10/12/introducing-lakeview-a-visibility-tool-for-aws-s3-based-data-lakes/,0,bigdata
hswlpj,"TensorFlow, Keras and deep learning, without a PhD",,16,0,brianjester,2020-07-17 14:26:38,https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0,0,bigdata
hmxt1h,Apache Flink 1.11 is out now! Read the release announcement for all new features and improvements,,17,0,Marksfik,2020-07-07 16:28:41,https://flink.apache.org/news/2020/07/06/release-1.11.0.html,0,bigdata
fzyne8,Is Python a good starting point to get into big data career?,,17,8,tidder78,2020-04-12 15:27:22,https://www.reddit.com/r/bigdata/comments/fzyne8/is_python_a_good_starting_point_to_get_into_big/,0,bigdata
fu824h,10 Open Source Data Science Projects to Make you Industry Ready!,,18,0,subhamroy021,2020-04-03 13:04:14,https://www.codeingschool.com/2020/04/10-open-source-data-science-projects-industry-ready.html,0,bigdata
frqqcb,How to Transition into a Data Science Role,"Hello Everyone!  My name is Ken. Over the last 5 years, I transitioned from being a management consultant to a data scientist (and now director of data science at my company). When I was making this move, I felt that there were limited resources out there to help me with this specific career change. 

Over the past year, I started a YouTube channel for those who were also looking to get into data science. I talk about my experience learning the skills, building a portfolio, and I even threw in a few projects I did in my free time. 

I hope that my content is informative and can help others avoid some of the mistakes that I made on the way. If this is something that may interest you, feel free to check it out here:  [https://www.youtube.com/c/kenjee1](https://www.youtube.com/c/kenjee1) 

Thanks!

\-Ken",16,10,kjee1,2020-03-30 13:24:41,https://www.reddit.com/r/bigdata/comments/frqqcb/how_to_transition_into_a_data_science_role/,0,bigdata
f0sglp,"Apache Airflow: Variables, Macros and Templating",,16,0,marclamberti,2020-02-08 14:45:07,https://youtu.be/zukGZlm00_I,0,bigdata
ehlx2u,"A new big data platform based on Mesos, Spark, Flink, Kafka and ElasticSearch ... something to follow ...",,15,3,badtrash78,2019-12-30 13:15:40,https://www.eskimo.sh,0,bigdata
dx2avh,Which of these 4 ETL tools is the best?,"Hi all. So I’ve very recently started a job as a Big Data Engineer and my very first task is to do some research on the following tools for data integration and ETL (Spark, SSIS, Apache Nifi, Talend) and present to them team so we can decide which one to use for a project.

My background is more to do with machine learning and maths so I was hoping someone here who has some relevant experience could shed some light on which of these tools they would use over the others and why?",15,10,Heretic_Raw,2019-11-16 04:38:39,https://www.reddit.com/r/bigdata/comments/dx2avh/which_of_these_4_etl_tools_is_the_best/,0,bigdata
dwae40,Tutorial on how to use Airflow without pain,[https://medium.com/@simon.hawe/how-to-use-airflow-without-headaches-4e6e37e6c2bc](https://medium.com/@simon.hawe/how-to-use-airflow-without-headaches-4e6e37e6c2bc),17,15,shawemuc,2019-11-14 14:39:46,https://www.reddit.com/r/bigdata/comments/dwae40/tutorial_on_how_to_use_airflow_without_pain/,0,bigdata
dubjcb,The 5-minute guide to using bucketing in Pyspark,,17,2,luminoumen,2019-11-10 13:40:27,https://luminousmen.com/post/the-5-minute-guide-to-using-bucketing-in-pyspark,0,bigdata
du48jy,Preview release of Spark 3.0,,16,0,None,2019-11-10 00:23:21,https://spark.apache.org/news/spark-3.0.0-preview.html,0,bigdata
dg42w6,"Facebook Debuts PyTorch 1.3 With PyTorch Mobile, Quantization, TPU Support and More",,17,0,Yuqing7,2019-10-10 20:32:37,https://medium.com/syncedreview/facebook-debuts-pytorch-1-3-with-pytorch-mobile-quantization-tpu-support-and-more-9b4fc35e5e38,0,bigdata
creaew,Introducing Prophecy.io - Cloud Native Data Engineering,"Hey guys!

We’re a team of engineers coming from enterprise data ETL backgrounds, who worked in the past on Apache Hive (Hortonworks), Kafka (Confluent), Spark, Hadoop, and even CUDA. After seeing times and times again how so many of our customers struggle with Hive/Hadoop we decided to build a product first company - that just works to solve real (and sometimes unsexy) data engineering pain points.

We want to support the entire Enterprise journey to Open Source (Apache Spark) and then to cloud (Kubernetes).

I'd love to hear what you think, what you wish we'd build, and I'll be here to answer any questions!

You can read more about it here: [https://medium.com/prophecy-io/introducing-prophecy-io-cloud-native-data-engineering-1b9247596030](https://medium.com/prophecy-io/introducing-prophecy-io-cloud-native-data-engineering-1b9247596030).",17,2,several27,2019-08-16 23:29:01,https://www.reddit.com/r/bigdata/comments/creaew/introducing_prophecyio_cloud_native_data/,0,bigdata
bxrmyz,Google Sets Aside $2.6 Billion for the Purchase of Analytics Software Company Looker,,17,1,cryptokunbo,2019-06-07 07:21:31,https://www.toks.tech/google-sets-aside-2-6-billion-for-the-purchase-of-analytics-software-company-looker/,0,bigdata
bx7d9j,"“Oh, you’re a data… something?”: The Misunderstood Role Of A Data Engineer",,17,0,Sehs,2019-06-05 20:19:39,https://medium.com/ssense-tech/oh-youre-a-data-something-the-misunderstood-role-of-a-data-engineer-5856a216f240,0,bigdata
bx1bbx,Book: Mastering Large Datasets,,17,14,jt-wololo,2019-06-05 11:06:19,https://www.manning.com/books/mastering-large-datasets,0,bigdata
ax0ixt,Redshift vs Snowflake,"Anyone got opinions on Redshift vs Snowflake for data warehousing?    


Background: Our team has been trying out Redshift and Snowflake as a future replacement for SQL Server (currently on-prem).  We've already moved our biggest data to S3 buckets 'cos SQL Server wasn't able to handle big queries. Analysts have access to S3 buckets via Athena. We initially assumed analysts wanted access to raw data and a single source of all truth, but it turns out they also want highly sanitised data that is easy to use, hence we're investigating warehousing solutions.

Snowflake is a less established technology and a relatively new company, but it seems a lot ""nicer"" to use than Redshift in almost every way so far. Even at small scale on Redshift we're run into concurrency issues and the provided tools are sparse (very similar to the Athena interface). Snowflake has cool features like multi-clustering and clusters that switch off if inactive. Redshift is a bit of a pain to scale up and scale down and takes a lot of time for snapshots to complete. Am I missing anything obviously about Redshift that makes it great? ",16,26,bleepingdba,2019-03-04 00:14:54,https://www.reddit.com/r/bigdata/comments/ax0ixt/redshift_vs_snowflake/,0,bigdata
alo0lt,Kafka Spark Streaming | Kafka Spark Streaming Example | Spark Training |...,,17,1,poojagandhi456,2019-01-31 10:02:04,https://www.youtube.com/watch?v=2z6scTH_C4c&feature=share,0,bigdata
a2zfhg,The Definitive Guide to Data Mining,,18,1,bil-sabab,2018-12-04 10:13:44,https://theappsolutions.com/blog/development/data-mining-guide/,0,bigdata
9znrhh,How Apache Flink helped power Alibaba's Singles Day 2018: data in a Flink of an eye,,16,0,Marksfik,2018-11-23 12:08:23,https://data-artisans.com/blog/singles-day-2018-data-in-a-flink-of-an-eye,0,bigdata
8zdhsh,A typical data engineering project - Netflix perspective,,18,3,hz265,2018-07-16 18:36:47,https://www.reddit.com/r/dataengineering/comments/8zd86c/a_typical_data_engineering_project_netflix/,0,bigdata
8z9tqb,Practical Apache Spark in 10 minutes. Part 4 - MLlib,,18,0,viktoriia_shulga,2018-07-16 10:33:04,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes.-part-4-mllib/?utm_source=reddit&utm_medium=bigdata&utm_campaign=spark_part4,0,bigdata
8l1cxl,Google acquires Cask Data to beef up its tools for building and running big data analytics,,18,0,LiamBigDataDonoghue,2018-05-21 14:48:05,https://techcrunch.com/2018/05/16/google-acquires-cask-data-to-beef-up-its-tools-for-building-and-running-big-data-analytics/,0,bigdata
89a0ub,"Python & Big Data: Airflow & Jupyter Notebook with Hadoop 3, Spark & Presto",,17,1,marklit,2018-04-03 05:13:28,http://tech.marksblogg.com/python-big-data-airflow-jupyter-notebook-hadoop-3-hive-presto.html,0,bigdata
7zich9,Benefits of Big Data on OpenStack,,17,0,basscheffers,2018-02-22 21:06:11,https://vexxhost.com/blog/big-data-on-openstack-benefits,0,bigdata
7q3d07,"What is multifactor and probabilistic data? Also, what are processes and streams? Are they faster than real-time?",,17,0,captainhungrycat,2018-01-13 08:21:38,https://i.redd.it/5xstb79dos901.png,0,bigdata
7hl9i7,Black Friday showed retailers still don’t know how to leverage their data,,18,0,dandesim,2017-12-04 23:05:24,https://medium.com/swlh/i-hate-black-friday-please-stop-spamming-me-de9e3d141838,0,bigdata
7eg1bp,I Completed the Data Engineering on Google Cloud Platform Specialization (Coursera) – Review,,16,0,fhoffa,2017-11-21 07:09:45,http://www.tribalism.com.au/news/i-completed-data-engineering-on-google-cloud-platform-specialization-coursera-review/,0,bigdata
6g08qd,"If Your Company Isn't Good at Analytics, It's Not Ready for AI",,16,0,fhoffa,2017-06-08 11:00:06,https://hbr.org/2017/06/if-your-company-isnt-good-at-analytics-its-not-ready-for-ai,0,bigdata
69zepx,Data is the new oil,,19,0,carpb202,2017-05-08 17:06:19,https://www.economist.com/news/leaders/21721656-data-economy-demands-new-approach-antitrust-rules-worlds-most-valuable-resource,0,bigdata
68p53d,Big data (or data lakes) vs modern data warehouse,"I'm just curious about your opinion. As a Consultant I deal with a lot of clients and different projects. Data projects are my main focus, I usually work for moderately large organisations (1k - 5k FTE).

The big dilema right now (from thier perspective) is if invest in a big data solutions (hadoop, more recently spark) or optimise existing datawarehouse. The first question I ask is: what's wrong with your current data platform. Most companies say that they need to wait weeks (or months) to get thier requirements to production. Second issue is that they are schema bounded. Third is processing time.

Two years ago, most of the Consulants i know would recommend starting a new project around technologies such as hadoop. Right now we have more information about Big data deployments. Some of them are harsh (garter's 90% of data lakes will fails).

Knowing that most Businesses don't need Real-time analytics and they can scale thier current datawarhouses, what's your recommended solution? We usually tend to work on existing system both from Business processes and technologies.

Side question, have you heard about logical data warehouses?

Thanks!

",18,10,dziarskihank,2017-05-01 22:12:13,https://www.reddit.com/r/bigdata/comments/68p53d/big_data_or_data_lakes_vs_modern_data_warehouse/,0,bigdata
5s1log,Newbie: Big Data software tools?,"Newbie to this world, wondering if someone can recommend some big data software tools used for Transport and Processing?",17,15,malpib,2017-02-04 16:48:56,https://www.reddit.com/r/bigdata/comments/5s1log/newbie_big_data_software_tools/,0,bigdata
5qi3kk,Introductory course on Big Data,,18,0,acangiano,2017-01-27 16:38:35,https://bigdatauniversity.com/courses/what-is-big-data/,0,bigdata
4vd736,"900TB, 4U, $60k -- new 10TB HDDs and 15TB SSDs breaking through storage plateau [OC]",,17,0,michaelmalak,2016-07-30 18:04:42,http://technicaltidbit.blogspot.com/2016/07/900tb-4u-60k.html,0,bigdata
4ut9yi,Introducing Apache Spark 2.0,,18,7,engwish,2016-07-27 06:35:21,https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html,0,bigdata
4kpkzr,Apache Spark as a Compiler: Joining a Billion Rows on your Laptop,,15,0,engwish,2016-05-23 19:02:49,https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html,0,bigdata
4js84g,"Google BigQuery and Dataproc shine against Amazon Redshift, EMR, Presto, Spark, ElasticSearch",,19,21,thetinot,2016-05-17 17:59:43,https://cloud.google.com/blog/big-data/2016/05/bigquery-and-dataproc-shine-in-independent-big-data-platform-comparison,0,bigdata
4cjd2d,Creating a Serverless ETL nirvana - process 1TB in 37 seconds without spinning up a single VM!,,17,2,thetinot,2016-03-30 04:53:57,http://blog.shinetech.com/2016/03/30/creating-a-serverless-etl-nirvana-using-google-bigquery/,0,bigdata
4a9f0u,An overview of Apache stream processing technologies,,18,0,None,2016-03-13 18:42:29,https://databaseline.wordpress.com/2016/03/12/an-overview-of-apache-streaming-technologies/,0,bigdata
441mpp,Comparing Spark vs the Dataflow/Beam programming model - Time to switch?,,15,4,fhoffa,2016-02-03 20:16:42,https://cloud.google.com/dataflow/blog/dataflow-beam-and-spark-comparison,0,bigdata
3w8etm,"Why machine learning is causing Google to ""rethink everything""",,18,0,SharpSightLabs,2015-12-10 15:25:58,http://www.sharpsightlabs.com/machine-learning-google-rethink/,0,bigdata
3o7843,30 free courses to learn Big Data the right way .,,16,1,hamzamu,2015-10-10 08:42:38,http://medevel.com/30-free-courses-to-learn-big-data-the-right-way.html,0,bigdata
3difeo,Analyzing 50 billion Wikipedia pageviews in 5 seconds (BigQuery beginner tutorial),,17,0,fhoffa,2015-07-16 15:04:49,https://www.reddit.com/r/bigquery/comments/3dg9le/analyzing_50_billion_wikipedia_pageviews_in_5/,0,bigdata
36pnwg,"$10 hedge fund supercomputer sweeps Wall Street Big Data with AI, ML & power from the cloud",,15,2,iamraybies,2015-05-21 03:09:43,http://www.theage.com.au/business/markets/10-hedge-fund-supercomputer-sweeps-wall-street-with-power-from-the-cloud-20150520-gh68hx.html,0,bigdata
31ev21,Apache Flink - Fast and reliable large-scale data processing engine,,19,5,based2,2015-04-04 12:02:41,http://flink.apache.org/,0,bigdata
2hwgui,Apache Storm is ready for prime time,,18,5,geekgodess,2014-09-30 16:24:01,http://www.zdnet.com/apache-storm-is-ready-for-prime-time-7000034162/,0,bigdata
24kkbz,Pokemon or BigData technology?,,17,3,thespiff,2014-05-02 20:36:57,https://docs.google.com/forms/d/1kckcq_uv8dk9-W5rIdtqRwCHN4Uh209ELPUjTEZJDxc/formResponse,0,bigdata
21y3pw,Big Data is played out. Medium data is where it's at.,,19,3,jeweloree,2014-04-01 18:59:39,http://www.tableausoftware.com/medium-data,0,bigdata
2014he,Look what came in the mail! (taking advantage of the recent OReilly sale),,16,8,flipstables,2014-03-10 06:58:30,http://imgur.com/gMn2oHQ,0,bigdata
1ukmo9,Advice on preparing for Big Data interview next Monday,"I am working on my PhD in math with a research focus that is not remotely related to big data but have an internship interview next week and am trying to prepare for it. I have slowly been learning R and am reviewing the video lectures from a big data course (https://www.coursera.org/course/datasci). They know my background isn't in big data but I still want to prepare as much as possible so they know I am serious. Do any of you guys have any advice on how I could best prepare over the next several days? All advice is appreciated.

Edit: They FINALLY got back to me. They don't have work now for me but might in the summer. I am now actively looking for an internship in the summer!",16,21,bullwinkle2059,2014-01-06 22:24:53,https://www.reddit.com/r/bigdata/comments/1ukmo9/advice_on_preparing_for_big_data_interview_next/,0,bigdata
11fwdpk,Good news for builders! OpenAI Releases APIs To ChatGPT and Whisper,"If you were as disappointed as I was when you saw that access to Meta's LLaMA models is limited to researchers, you are going to like this.  


[APIs to ChatGPT and OpenAI's speech-to-text model whisper](https://openai.com/blog/introducing-chatgpt-and-whisper-apis) are available as of yesterday. Through system-wide optimizations, they claim to have reduced inference costs by 90%. They now price ChatGPT at $0.002 per 1000 tokens. Dedicated instances are available for speedup and make economic sense if you process \~450M tokens a day.  


Machine learning progress continues to be as fast as a banana peal skating on warm vaseline. 

If you found this useful and want to stay in the loop, consider subscribing to The Decoding. I send out a weekly 5-minute newsletter that keeps professionals in the loop about machine learning and the data economy. [Click here to subscribe!](https://thedecoding.net/)",15,0,LesleyFair,2023-03-02 07:26:40,https://www.reddit.com/r/bigdata/comments/11fwdpk/good_news_for_builders_openai_releases_apis_to/,0,bigdata
10w28li,A Beginners Guide to Predictive Analytics: Turning Data Into Insights,,15,0,Emily-joe,2023-02-07 13:47:29,https://www.dasca.org/world-of-big-data/article/a-beginners-guide-to-predictive-analytics-turning-data-into-insights,0,bigdata
xzc3r3,How databases in datawarehouse different from normal relational databases?,,14,11,uncertainBoi,2022-10-09 04:08:08,https://www.reddit.com/r/bigdata/comments/xzc3r3/how_databases_in_datawarehouse_different_from/,0,bigdata
vr1ore,What are some good courses for learning about Hadoop Ecosystem?,What are some good courses for learning the Hadoop ecosystem?,16,2,RP_m_13,2022-07-04 07:16:11,https://www.reddit.com/r/bigdata/comments/vr1ore/what_are_some_good_courses_for_learning_about/,0,bigdata
uc0i9y,"Researchers Release Cleanlab 2.0: An Open-Source Python Framework For Machine Learning And Analytics With Messy, Real-World Data","Data preparation is the most time-consuming and hectic process in data science and machine learning, accounting for 80% of the labor. Messy data is a serious issue that costs businesses trillions of dollars every year.

Model performance can be harmed by data errors (for example, mislabeled samples in the training set) and dataset-level concerns like overlapping classes. Most test set errors are ubiquitous even in gold-standard benchmark datasets. This can cause data scientists to deploy worse models.

Although physically analyzing and cleaning up individual data points sounds tiresome, it frequently gives a significantly bigger payback than experimenting with advanced modeling approaches. Automatically identifying the tiny fraction of noise can considerably lessen the discomfort in this process.

[Continue Reading](https://www.marktechpost.com/2022/04/25/researchers-release-cleanlab-2-0-an-open-source-python-framework-for-machine-learning-and-analytics-with-messy-real-world-data/) | [Read the cleanlab blog](https://cleanlab.ai/blog/cleanlab-2/) | [Github](https://github.com/cleanlab/cleanlab)",17,0,No_Coffee_4638,2022-04-26 01:03:19,https://www.reddit.com/r/bigdata/comments/uc0i9y/researchers_release_cleanlab_20_an_opensource/,0,bigdata
tzlfhm,Latest AI Research at Amazon Improves Forecasting by Learning the Quantile Functions,"‘[The quantile function is a mathematical function that takes a quantile (a percentage of a distribution ranging from 0 to 1) as an input and returns the value of a variable as an output.’](https://www.amazon.science/blog/improving-forecasting-by-learning-quantile-functions) It can answer queries such as, “How much inventory do I need to maintain on hand if I want to guarantee that 95 percent of my customers receive their orders within 24 hours?” As a result, the quantile function is frequently utilized in forecasting questions.

However, in practice, there is rarely a neat method for computing the quantile function. Statisticians commonly approximate it using regression analysis for a single quantile level at a time. That means that if you want to compute it for a different quantile, you’ll need to create a new regression model, which nowadays usually entails retraining a neural network.

[Continue Reading](https://www.marktechpost.com/2022/04/08/latest-ai-research-at-amazon-improves-forecasting-by-learning-the-quantile-functions/)

Paper 1: https://www.amazon.science/publications/learning-quantile-functions-without-quantile-crossing-for-distribution-free-time-series-forecasting

Paper 2: https://www.amazon.science/publications/multivariate-quantile-function-forecaster

https://i.redd.it/9eoiirf3kfs81.gif",16,0,No_Coffee_4638,2022-04-09 04:23:41,https://www.reddit.com/r/bigdata/comments/tzlfhm/latest_ai_research_at_amazon_improves_forecasting/,0,bigdata
tj1kz1,LinkedIn Researchers Open-Source ‘FastTreeSHAP’: A Python Package That Enables An Efficient Interpretation of Tree-Based Machine Learning Models,"Researchers from LinkedIn open-source the FastTreeSHAP package which is a Python module based on the paper ‘[Fast TreeSHAP: Accelerating SHAP Value Computation for Trees](https://arxiv.org/abs/2109.09847).’ Implementing the widely-used TreeSHAP algorithm in the SHAP package allows for the efficient interpretation of tree-based machine learning models by estimating sample-level feature significance values. Its package includes two new algorithms: FastTreeSHAP v1 and FastTreeSHAP v2, both of which improve TreeSHAP’s computational efficiency by taking a different approach. 

The empirical benchmarking tests show that FastTreeSHAP v1 is 1.5x faster than TreeSHAP while keeping memory costs the same, and FastTreeSHAP v2 is 2.5x faster while using slightly more memory. The FastTreeSHAP package fully supports parallel multi-core computing to speed up its computation.

[**Continue Reading The Full Summary Article**](https://www.marktechpost.com/2022/03/20/linkedin-researchers-open-source-fasttreeshap-a-python-package-that-enables-an-efficient-interpretation-of-tree-based-machine-learning-models/)

Paper: https://arxiv.org/pdf/2109.09847.pdf

Github: https://github.com/linkedin/fasttreeshap",17,0,No_Coffee_4638,2022-03-21 02:42:24,https://www.reddit.com/r/bigdata/comments/tj1kz1/linkedin_researchers_opensource_fasttreeshap_a/,0,bigdata
t1gp4e,EU Proposes New Rules on Who Can Use and Access Data Generated in The EU Across All Economic Sectors,,17,0,No_Coffee_4638,2022-02-25 22:41:48,https://technews.pw/eu-proposes-new-rules-on-who-can-use-and-access-data-generated-in-the-eu-across-all-economic-sectors/,0,bigdata
qtpnd5,Introduction to using PyTorch for deep learning,"I created a guide on how to begin your first neural network project to analyse data using PyTorch. Feel free to have a look!

[https://taying-cheng.medium.com/all-you-need-to-know-about-pytorch-a0ba3af897fa](https://taying-cheng.medium.com/all-you-need-to-know-about-pytorch-a0ba3af897fa)",17,0,Ok-Peanut-2681,2021-11-14 13:18:25,https://www.reddit.com/r/bigdata/comments/qtpnd5/introduction_to_using_pytorch_for_deep_learning/,0,bigdata
q76kkl,Data Warehouse vs Data Lake: Differences Explained,,16,1,Big_Data_Path,2021-10-13 08:15:03,https://bigdatapath.wordpress.com/2021/10/13/data-warehouse-vs-data-lake-differences-explained/,0,bigdata
q45pcz,Serving ML Models in Production: Common Patterns,,16,1,mgalarny,2021-10-08 20:07:42,https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns,0,bigdata
pr2ehg,Being A Data Engineer: Expectations vs Reality,,16,1,SeattleDataGuy,2021-09-19 05:57:47,https://www.youtube.com/watch?v=6RiA_Qur2yo,0,bigdata
mcgxvq,Jupyter Notebooks in Production,"Hi everyone! I'm working with a few people who are developing a way to quickly and easily turn Jupyter Notebooks into hosted apps.

We're trying to get a few people to give us feedback on the software, which we're offering free for a  year as a thank you. 

Oh, did I mention free compute!? 

If you'd be interested in beta testing, let me know via a comment or send me a PM and I'll set it up!

Thanks guys, hope you're all having a great day (:",14,2,frenzdisco,2021-03-24 21:18:17,https://www.reddit.com/r/bigdata/comments/mcgxvq/jupyter_notebooks_in_production/,0,bigdata
maep9p,"Conversation with Emma Tang: ex Data Infrastructure Lead, Stripe",,15,0,waddupbrah,2021-03-22 04:18:17,https://www.softwareatscale.dev/p/software-at-scale-13-emma-tang-ex,0,bigdata
jsrt28,Kickstarting a picture book to explain big data to your parents and kids,,17,1,oneforthewall,2020-11-12 09:31:42,https://www.kickstarter.com/projects/bigdatagirl/big-data-girl,0,bigdata
jjb6du,Data Teams Is Out,"I’m proud to announce that my latest book, *Data Teams*, is available for purchase. It covers the three teams you need for analytics and how they should work with the rest of the business. This is the first book to really put data engineering at the forefront alongside data science for creating success data projects. You go to the book's website at [http://www.datateams.io/](http://www.datateams.io/) to get more information.",16,1,eljefe6a,2020-10-27 22:10:39,https://www.reddit.com/r/bigdata/comments/jjb6du/data_teams_is_out/,0,bigdata
i247x3,“The Native Spark UI is my favorite monitoring tool” — said no one ever; so we're building a better open-source Spark UI,,14,1,gingerbeardmayn,2020-08-02 01:57:04,https://www.datamechanics.co/blog-post/building-a-better-spark-ui?utm_source=reddit&utm_medium=link&utm_campaign=bigdata,0,bigdata
fxa1ts,Hazelcast Jet · Open-Source Distributed Stream Processing,,15,5,1cloud,2020-04-08 16:52:02,https://jet-start.sh/,0,bigdata
estwtf,ETL Spark Examples,"Hey everyone. I’m doing an ETL pipeline in Pyspark for my organization. Anyone have good resources to recommend? I read the ETL toolkit but that isn’t big data specific. All the examples I find online or on github are very small and seem to be written by people who spent 10 minutes on big data. I haven’t found any examples of production level robust pipelines that interact with traditional databases.

Thanks",17,3,pulsarsolar,2020-01-23 14:34:13,https://www.reddit.com/r/bigdata/comments/estwtf/etl_spark_examples/,0,bigdata
ejy88i,"Best ""database"" to handle and query large amounts of simple data","Imagine some simple structured data stored as flat TXT files.

The files have very few columns (2 or 3).  Data types are relatively short and just text.

Example:

|KeyID|Textfield1|Textfield2|
|:-|:-|:-|
|1|wqert1$2|asdfadf&@|
|2|Hufykj^(#2)|asdhflskdj57893|
|... n|||

Chalenges are:

There are thousands of these flat TXT files of varying sizes. However, they are all structured in the same way as in the example above.

Some are very long - several million lines/records.

In total there are 10s of billions of records.

&#x200B;

I would like to ""import"" all these files into ONE large ""database"" so they can be queried as ONE dataset.

Clearly any SQL-type transactional database will not be suitable or even needed for this.

Performance will be key and more important than features of the ""database"".  However it would be nice to have access to an easy-to-use data query and visualization tool (think Microsoft Power BI Desktop).

EDIT1:

I guess maybe my question is two-fold:

1. What ""database"" should I be using - i.e. Hadoop etc.?
2. What tools should I be using for querying - i.e. Presto, Hive, etc.?

Keep in mind that simplicity is desired given my limited experience. I am concerned about performance though given the large number of records. The total size if the data is not that large - around 2TB, but the number of records is really large.

What would be the best and easiest way to move forward?   What should I be looking at?

Thanks!",16,45,kakfaf,2020-01-04 15:53:01,https://www.reddit.com/r/bigdata/comments/ejy88i/best_database_to_handle_and_query_large_amounts/,0,bigdata
dq5ias,Top 8 Data Science Use Cases in Support,,15,0,techgig11,2019-11-01 15:09:44,https://www.activewizards.com/blog/top-8-data-science-use-cases-in-support/?utm_source=reddit&utm_medium=machinelearning&utm_campaign=data_science,0,bigdata
dbxj6o,GitHub Releases Dataset of Six Million Open-Source Methods for Code Search Research,,16,1,Yuqing7,2019-10-01 18:32:10,https://medium.com/syncedreview/github-releases-dataset-of-six-million-open-source-methods-for-code-search-research-383cc2ae7069,0,bigdata
cw3juj,"Datamarts, Data Vault, Data Lake... Data Swamp?",,16,5,None,2019-08-27 12:10:13,http://www.helenanderson.co.nz/data-vault-data-lake/,0,bigdata
cjvovs,The 5 Sampling Algorithms every Data Scientist need to know,,16,0,kiser_soze,2019-07-30 18:13:57,https://mlwhiz.com/blog/2019/07/30/sampling/?utm_campaign=shareaholic&utm_medium=reddit&utm_source=news,0,bigdata
ackq5n,[Discussion] Future of Machine Learning and Data Science in Industry due to Better and Better APIs?,"Someone on Reddit told me that their opinion on Machine Learning is that one should only study machine learning if they want an academic career in it. Their reasoning: because Machine Learning engineers are so expensive, the trend in tech is that companies are creating automated APIs to encapsulate the field so less skilled professions can incorporate machine learning into their stacks. In the future, APIs like Tensorflow and NumPy will allow average programmers to accomplish 90% of what an expert can achieve. It isn't a long-term skill in Industry. Also, companies are extremely picky about data scientists and other high-demand experts due to their high salaries. Basic data analysts are much more broadly employable.


How true is this perspective?


This raises the following question:


Where is the field moving? If individuals do decide to self study and then get a Master's in Machine Learning (with a specialization like data science/statistics) over the next decade, to emphasize that they are not 'script kiddies' and to possibly get into management, how employable will they be and how fierce will competition be for expert roles?

What other fields in technology do you think will be in demand and start rapidly growing over the next 5-10 years? Is that even predictable?

As Machine Learning becomes more automated, how can I make sure that the time and money I spent on my degree stays relevant / doesn't depreciate? What other skills will I need to be highly successful?",15,3,Kyak787,2019-01-04 18:01:40,https://www.reddit.com/r/bigdata/comments/ackq5n/discussion_future_of_machine_learning_and_data/,0,bigdata
9xcygw,Why you should explore Apache Flink early in your stream processing journey,,17,0,Marksfik,2018-11-15 16:58:30,https://data-artisans.com/blog/early-observations-apache-flink,0,bigdata
9uahk0,Real-Time sentiment analysis,,17,1,Manoharan-D,2018-11-05 04:23:03,http://techmano.in/blog/realtime.aspx,0,bigdata
9is0ua,"Microsoft releases automated machine learning, an end-to-end solution to build and train models that make predictions from data and then deploys them anywhere",,18,1,myinnerbanjo,2018-09-25 13:16:13,https://blogs.microsoft.com/ai/automated-ai-development/,0,bigdata
9coeen,"An introduction to Druid, your Interactive Analytics at Scale (really)",,15,2,scboffspring,2018-09-03 19:26:13,https://blog.zysset.me/introduction-to-druid/,0,bigdata
8rc1vq,Build a Data Lake on Google Cloud Platform,,16,0,fhoffa,2018-06-15 15:59:12,https://cloud.google.com/solutions/build-a-data-lake-on-gcp,0,bigdata
8jbqjx,Top 7 Data Science Use Cases in Finance,,14,0,viktoriia_shulga,2018-05-14 12:11:24,https://www.activewizards.com/blog/top-7-data-science-use-cases-in-finance/?utm_source=reddit&utm_medium=bigdata&utm_campaign=finance,0,bigdata
6rcaz2,"Top articles in big data in July, handpicked for you by data scientists",,17,2,fl4v1,2017-08-03 13:33:45,https://medium.com/p/the-best-of-big-data-new-articles-published-this-month-july-2017-acb58d4bb15d,0,bigdata
6jnll8,"This cancer data repository has 4.1 petabyte of data, will double that by end of the year.",,18,0,karis02,2017-06-26 20:06:09,http://chicagoinno.streetwise.co/2017/06/26/how-uchicagos-gdc-is-helping-cure-cancer-using-big-data/,0,bigdata
6aqwt0,More than 70% of employees have access to data they should not...,,17,2,carpb202,2017-05-12 12:11:44,https://hbr.org/2017/05/whats-your-data-strategy,0,bigdata
683pho,Spotify: Reliable export of Cloud Pub/Sub streams to Cloud Storage (for 100 billion events a day),,16,2,fhoffa,2017-04-28 16:28:18,https://labs.spotify.com/2017/04/26/reliable-export-of-cloud-pubsub-streams-to-cloud-storage/,0,bigdata
5sfrh5,"Announcing Apache Flink 1.2.0 with rescaling, mesos, security improvements, queryable state and much more",,15,0,rmetz,2017-02-06 18:02:32,http://flink.apache.org/news/2017/02/06/release-1.2.0.html,0,bigdata
5oxmzl,New York City public datasets now available on Google BigQuery,,16,2,jkestelyn,2017-01-19 16:47:01,https://cloud.google.com/blog/big-data/2017/01/new-york-city-public-datasets-now-available-on-google-bigquery,0,bigdata
5l07ov,Introducing Apache Spark 2.1,,14,0,based2,2016-12-30 00:02:52,https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html,0,bigdata
5drv2l,Apache Spark and Amazon S3 - Gotchas and best practices,,18,0,bistro17,2016-11-19 10:08:06,https://medium.com/@subhojit20_27731/apache-spark-and-amazon-s3-gotchas-and-best-practices-a767242f3d98#.c0hdqrd5q,0,bigdata
5bnv44,"In the past several years, Presidential elections have served as cutting-edge laboratories in social media, big data, and analytics—testbeds that end up getting used in the industry in later years.",,16,0,wheeler1432,2016-11-07 18:31:30,https://www.laserfiche.com/simplicity/2016-election-lessons/,0,bigdata
55e3rx,Apache Spark vs. Apache Drill – The Ramp,,15,2,pmz,2016-10-01 17:08:34,https://medium.com/the-ramp/apache-spark-vs-apache-drill-1bfd87f73d8d#.1vwba9qob,0,bigdata
5360ub,Airline Flight Data Analysis - Part 1 - Data Preparation,,16,2,diybigdata,2016-09-17 05:32:43,http://diybigdata.net/2016/08/airline-flight-data-analysis-data-preparation/?utm_source=reddit&utm_medium=social&utm_campaign=social-pug,0,bigdata
50u4ba,The time we scanned 1.09 Petabytes in 4 minutes in Google BigQuery - LIVE,,16,1,thetinot,2016-09-02 16:34:55,https://youtu.be/6Nv18xmJirs?t=35m40s,0,bigdata
4uj3hr,Apache Spark $100K enterprise data hackathon,"Devpost & IBM are hosting the Apache Spark Makers Build, a global online data competition to build Spark apps that help businesses & startups run more efficiently. There are over $100,000 in cash prizes, and submissions are open from now through August 23.

Participants will have access to enterprise data sets from Yelp, Amazon, Bitcoin, 4Quant, and others to build an analysis or app that tackles a business problem in marketing, customer care, operations, or risk management.

The grand prize is $50,000, and the top three winners each get a meeting with the judge or Spark community expert of their choice. Judges include data experts from Tesla, Netflix, IBM, and Silicon Valley Data Science.
 
Full details and registration are @ [apachespark.devpost.com](http://apachespark.devpost.com).",17,1,idesofjo,2016-07-25 15:16:13,https://www.reddit.com/r/bigdata/comments/4uj3hr/apache_spark_100k_enterprise_data_hackathon/,0,bigdata
46e8ux,"What it looks like to process 3.5 million books on Google Cloud (including 2TB SSD, 1 TB RAM, 160 cores, speeds from 750 MB/s to 45.5 GB/s)",,15,1,fhoffa,2016-02-18 10:59:09,http://googlecloudplatform.blogspot.com/2016/02/what-it-looks-like-to-process-3.5-million-books-in-Googles-cloud.html,0,bigdata
43pdvh,Is Hadoop MapReduce Dead?,"I work for an Oracle Platinum Partner. One of the most common questions our clients want answers to is, “What is the future of MapReduce as an execution engine for Hadoop?” 

Our practice director addressed this question in a well thought out explanation:  http://saturninfotech.com/blog/2016/01/28/is-mapreduce-dead/",15,6,Millenial369,2016-02-01 17:05:04,https://www.reddit.com/r/bigdata/comments/43pdvh/is_hadoop_mapreduce_dead/,0,bigdata
3sdago,The Data Science Industry: Who Does What (Infographic),,17,3,DataPoints,2015-11-11 04:22:45,http://blog.datacamp.com/data-science-industry-infographic/,0,bigdata
3o4yhn,"Building a Streaming Data Hub with Elasticsearch, Kafka and Cassandra",,14,0,aspleenic,2015-10-09 19:33:40,http://thenewstack.io/building-streaming-data-hub-elasticsearch-kafka-cassandra/,0,bigdata
3iueyn,"Doradus - a REST service that extends a Cassandra NoSQL database with a graph-based data model, advanced indexing and search features",,16,0,148-3tothe3tothe6,2015-08-29 13:46:54,https://github.com/dell-oss/Doradus,0,bigdata
38dxn3,9 Free Books for Learning Data Mining & Data Analysis,,16,0,skillcode,2015-06-03 16:13:52,http://codecondo.com/9-free-books-for-learning-data-mining-data-analysis/,0,bigdata
36x3po,Big Data Processing with Apache Spark – Part 1: Introduction,,16,0,carlosp81,2015-05-22 20:59:25,http://www.infoq.com/articles/apache-spark-introduction#.VV-X72bfbwg.reddit,0,bigdata
2zgyyf,4 Web Scraping Tools To Save You Time On Data Extraction,,16,2,vladdione,2015-03-18 14:42:24,http://tech.co/4-web-scraping-tools-save-time-data-extraction-2015-03,0,bigdata
2faesb,25 Classes to Get Started with Data Science & Big Data,,17,0,Allclasses,2014-09-02 19:59:16,http://blog.allclasses.com/25-classes-to-get-started-with-data-science-big-data/,0,bigdata
27bdz4,Census Data + Algorithm = No more gerrymandering,,17,1,JamesKotecki,2014-06-04 19:19:04,http://www.washingtonpost.com/blogs/wonkblog/wp/2014/06/03/this-computer-programmer-solved-gerrymandering-in-his-spare-time/?tid=sm_fb,0,bigdata
26uzrc,IBM Partners With 28 Universities to Help Train Tomorrow's Data Scientists,,15,1,dataconomy,2014-05-30 08:35:00,http://dataconomy.com/ibm-partners-with-28-universities-to-help-train-tomorrows-data-scientists/,0,bigdata
21qtvd,Big data: are we making a big mistake?,,18,6,skillcode,2014-03-30 13:35:41,http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html,0,bigdata
1m3w3z,Our data-driven newborn,,16,2,thecodemonkey,2013-09-10 14:57:43,https://github.com/MiniCodeMonkey/Sophie-Tracker-3000,0,bigdata
11zh8pw,A Comprehensive Collection of Data Analysis Cheat Sheets For 2023,,15,0,Shradha_Singh,2023-03-23 11:18:07,https://medium.com/@reachtoanamikasingh19/a-comprehensive-collection-of-data-analysis-cheat-sheets-for-2023-3e1f66a9a900,0,bigdata
11oc025,Data Architecture Complexity,,15,0,bigdataengineer4life,2023-03-11 05:41:38,https://youtu.be/El96tIPpvSQ,0,bigdata
11newg0,Here’s a playlist of 7 hours of music with NO VOCALS I use to focus when I’m coding /learning . Post yours as well if you also have one!,[Spotify](https://open.spotify.com/playlist/6BoTD9WeyBN5rD5uyhbROO) | [Apple](https://music.apple.com/playlist/synthwave-focus-i/pl.u-b3b8Nm9C5MmMyM) | [Youtube](https://youtube.com/playlist?list=PLwO9YUACGAzS_XaTchS3qLgK5EGsb3qwa) | [Amazon](https://music.amazon.com/user-playlists/0375bdbe2f3d41f39523ddb35fc92ca0b1r),14,1,letsstartanew2,2023-03-10 04:40:57,https://www.reddit.com/r/bigdata/comments/11newg0/heres_a_playlist_of_7_hours_of_music_with_no/,0,bigdata
vfsuq1,What's new in Spark 3.3.0,,14,0,co0lster,2022-06-19 11:08:09,https://www.datapotion.io/blog/what-new-in-spark-3-3-0,0,bigdata
v4i6to,Apache Hive for Data Engineers (Hands On) with 2 Projects,,15,0,bigdataengineer4life,2022-06-04 05:38:19,https://youtu.be/m_WY1mGAxDE,0,bigdata
uy19gx,The Streets of Monaco,,14,0,marklit,2022-05-26 06:28:08,https://tech.marksblogg.com/streets-of-monaco-openstreetmap-postgis-qgis.html,0,bigdata
uf91tz,Microsoft AI Researchers Develop MoLeR: A Deep Learning-Based Generative Model That Enables Efficient Drug Design,"Healthcare systems constantly require new drugs to address unmet medical needs across diverse therapeutic areas. Pharmaceutical industries strive to deliver new drugs to the market through the complex activities of drug discovery and development. Target identification and validation, hit identification, lead creation and optimization, and finally, the identification of a candidate for further development are all part of the discovery process. Development, on the other hand, includes optimizing chemical synthesis and formulation, doing toxicity research in animals, conducting clinical trials, and finally obtaining regulatory approval. Both of these procedures take a long time and cost a lot of money.

Expert medicinal chemists are currently working to develop “hit” molecules, which are compounds that show some potential but also some unfavorable features during early screening. Chemists aim to alter the structure of hit compounds in subsequent tests to improve their biological efficacy and eliminate potential negative effects. To focus costly and time-consuming research on the most promising compounds, computational modeling approaches have been created to forecast how the molecules will fare in the lab. 

To overcome these issues, a new study by the Microsoft Generative Chemistry team in collaboration with Novartis has developed a model named MoLeR. Their paper, “LEARNING TO EXTEND MOLECULAR SCAFFOLDS WITH STRUCTURAL MOTIFS, ” demonstrates how generative models based on deep learning may aid in transforming the drug discovery process and uncovering new molecules more quickly.

[Continue Reading](https://openreview.net/pdf?id=ZTsoE8G3GG)

Paper: https://openreview.net/pdf?id=ZTsoE8G3GG

Github: [https://github.com/microsoft/molecule-generation](https://github.com/microsoft/molecule-generation)

&#x200B;

https://i.redd.it/nqoefts2hnw81.gif",14,1,No_Coffee_4638,2022-04-30 11:18:13,https://www.reddit.com/r/bigdata/comments/uf91tz/microsoft_ai_researchers_develop_moler_a_deep/,0,bigdata
uap958,The Data Warehouse Performance Wars,,15,0,humhiprib,2022-04-24 07:21:52,https://www.firebolt.io/blog/future-of-performance-is-not-about-performance,0,bigdata
ty6wmr,Looking for intersting end to end big data pipeline projects,"I am currently looking for project ideas with dataset source to implement. 

Preferably to implement end to end on GCP with deployment.

Any suggestions are welcome. Thanks in advance. :)",15,2,ViolinistSuper8880,2022-04-07 06:36:06,https://www.reddit.com/r/bigdata/comments/ty6wmr/looking_for_intersting_end_to_end_big_data/,0,bigdata
tpoyd6,LinkedIn Engineering Team Develops ‘Opal’ to Ingest Mutable Data and Build Mutable Dataset on Top of the Immutable File System,"Trusted data platforms and high-quality data pipelines are critical at LinkedIn for accurate business KPIs and sound decision-making. Today, online data repositories account for a significant portion of LinkedIn’s data. Internet datasets must be migrated to the data lake for further processing and business analytics, whether they are SQL or NoSQL systems.

However, because the data lake architecture is immutable, it cannot be easy to reflect online table updates, inserts, and deletes. *Opal* was introduced to balance data quality, latency, scan performance, and system scalability (computational resources and operational costs).

[Continue reading here](https://www.marktechpost.com/2022/03/27/linkedin-engineering-team-develops-opal-to-ingest-mutable-data-and-build-mutable-dataset-on-top-of-the-immutable-file-system/)",14,2,No_Coffee_4638,2022-03-27 17:41:21,https://www.reddit.com/r/bigdata/comments/tpoyd6/linkedin_engineering_team_develops_opal_to_ingest/,0,bigdata
s98lfb,"In 2021, people created 1.7 MB of data every second.",,16,8,bigdatatrunk,2022-01-21 11:21:55,https://i.redd.it/sc67sj2kz0d81.jpg,0,bigdata
p2w3fn,Enso 2.0 is out! A no-code interactive ETL and data analytics tool.,,15,3,sylwiabr,2021-08-12 09:04:41,https://v.redd.it/mkvpcxa17wg71,0,bigdata
of5dg7,Data Engineering Road Map - How To Learn Data Engineering Quickly,,16,0,nonkeymn,2021-07-06 22:12:13,https://www.youtube.com/watch?v=SpaFPPByOhM,0,bigdata
m6bu8w,Real alternatives do CDP/CDH/HDP? No open source alternative in the near future?,"Hi.

CDP is already in full speed pushing the cloud vendors. HDP with Ambari is basically dead and integrated inside the Cloudera group... 

So.. what is now the open source alternative? What can we install in our customers who don't want and can't spend thousands and thousands of dollars/euros on cloud infrastructure plus CDP licensing?

Any ideas? What is expected for small businesses who are running these days HDP for example?",15,22,JohnJohnPT,2021-03-16 15:38:12,https://www.reddit.com/r/bigdata/comments/m6bu8w/real_alternatives_do_cdpcdhhdp_no_open_source/,0,bigdata
m4gidc,Step by step guide to creating a data dictionary,"Simple data starts with good organization. A data dictionary is a list of key terms and metrics with definitions; a business glossary. Although this seems like a simple exercise, it’s very difficult to align business departments with the same definitions. A large ride-sharing company had difficulties related to data definitions. At this company, it was very difficult to get aligned on the same metrics for “number of rides per week”. Why?

* The data team defines the “number of rides per week” as the total number of rides that were **completed** between Jan. 1, 2020, 12:00 AM → Jan. 7, 2020, 11:59 PM.
* The marketing team defines the “number of rides per week” as the total number of rides that were **started** between Jan. 1, 2020, 12:00 AM → Jan. 7, 2020, 11:59 PM.
* The sales team defines “number of rides per week” as the total number of riders that **paid for a ride** Jan. 1, 2020, 7:00 AM → Jan. 8, 2020, 6:59 AM

All data-driven organizations experience this problem as they begin to grow their data and people. And although it sounds like a simple problem, which might require a meeting to solve, aligning the business and data to remove confusion can be an extremely profound problem. That's why a data dictionary can be one of the most valuable tools that a data team can create to deliver results. Below are the steps that teams need to take when creating their data dictionary

### 1. Gather terms from different departments

The first step data teams need to take is to collect the different terms that all departments are using to define key business definitions. Today, this collection could be done in a spreadsheet with a list of the business concepts. One approach teams take is to look at a sample of the most used reports and dashboards by teams to get an understanding of the main metrics used across departments. This can be done easily using Secoda, which automatically indexes the most used resources by an organization. Once teams have a good understanding of the commonly used tables and dashboards, listing out all the primary axis by charts can help to understand the term as well as the definitions.

* For example, a dashboard showing the rides completed by the state is composed of two primary terms: ""rides completed"" and ""state"". The teams should compile a list of these two terms.
* One way a tool like Secoda can help with this stage is to use the ""tag"" feature to categorize which reports involve the ""rides completed"" and ""state"" terms. Additionally, Secoda automatically documents the data type, and examples of the values, additional columns in the table, owners, and a link to the report.

Once you have your list of terms, you can begin grouping them by the functional unit. For example, the marketing metrics, product metrics, operations metrics. Another suggestion is to categorize the similar dimensions that are repeated throughout the tables (product\_id, country, year, etc.) This is helpful because some columns could have names that are difficult to understand. Making a list of these definitions can help data teams standardize some of these confusing terms. After you prepare this list, ask the teams to go over their section and add any terms that you might have missed. Once the terms have been collected, you should be ready to define the terms

### 2. Give the terms a definition

The data team should try to define the terms collected based on the information that they can collect on the terms. To do this, the data team should pull any existing definitions from outdated documentation. Most teams keep this documentation in google sheets or confluence, which can make finding the definitions a tricky part of the process. Another way to find the definitions is by looking through annual reports, or from actual code such as SQL queries or Excel. Names of terms should always be written out and unabbreviated to make sure that they are universally understandable. Even non-technical staff should understand the definitions of rows or columns, even if they refer to some other definitions in the database. After this initial draft is complete, schedule time to sit with the teams and ask them if there are inconsistencies, inaccuracies, or definitions that need to be refined. This part of the process can take time and can require multiple meetings to clear up. When our team started working on this step, we found that a lot of the meeting was spent explaining ""why"", as opposed to talking through solutions. One strategy that helped make these meetings more efficient was to create detailed documentation before the meeting. This way, all stakeholders came into the meeting prepared and informed about the discussion. One additional tip is to make sure the stakeholder understands that the goal of the meeting is to find out how the metrics **should** be defined, not how it was defined in the past. If the current definition is not perfect, this is a great chance to correct it and align teams on the same definition. Remember that overly complex metrics are rarely the right solution. One rule we like to follow internally is to always remember Goodhart's law. This law states that when a measure becomes a target, it ceases to be a good measure. We don't want to create measurements that are too complex and incentivize people inappropriately.

### 3. Find alignment

In this step, it's important to look for similarities or dependencies in definitions across teams and resolve conflicts. Multiple teams may have similar definitions. For those teams and terms, bring them into the same meeting to align the teams towards one definition. When you're setting up the meeting, clarify the outcome of the meeting to get one standard definition that works for both teams.

* This might require one team to concede to another teams definition
* This might require both teams to agree to a new name for the definition.
* This might require both teams to rework their existing definition to find a completely new definition.

One suggestion is to make sure that names and definitions avoid and ambiguity. Use longer, more descriptive terms when they add more clarity. Your goal should be to eliminate any confusion about the definitions.

### 4. Get support and sign off

Once these meetings conclude and teams are on the same page, have the team leads sign off on the new definition. The team leads should be in alignment with the definitions and feel like the data team worked collaboratively with them to come to this definition. One effective strategy is to involve the leadership team in the exercise early to make sure that their team leads are signing off on the definitions. If the team leads see the value of having alignment, this can move at a much faster pace.

### 5. Centralize the document 

After you receive sign-off from team leads, publish the data dictionary as a document that can be accessed by all employees. The definitions should be understood by anyone in the company, not just the data team. Additionally, the definitions should be adopted by all teams and by leadership. Because of this, getting people to start using the right definitions is a tricky part of the process. The individual definitions should be baked into the reports and visualizations whenever possible. One of the tougher tasks is to manage the data dictionary after it's been published. One solution to this is to try to autogenerate the data dictionary definitions from a single source, such as a code repository. We believe that a tool that integrates into the database or code is a great way to manage the data dictionary.

### 6. Upkeep the data dictionary

Although the key metrics should be stable, they may need to change over time. One instance that might require key metrics to change is when a new revenue stream is introduced or when the pricing of an existing revenue line changes. These changes traditionally come from the business team and might require the data team to implement the changes into the data dictionary. When there are big changes in the business, treat the changes to the data dictionary as a key part of the product release. Communicate the changes in the dictionary definition to the rest of the teams and make sure all team leads are informed about the new changes. One strategy that we used was creating a slack channel dedicated to updates to the data dictionary. This way, all business stakeholders can stay informed on the important key business definitions. Creating a data dictionary is not a small undertaking, it requires patience and alignment with leadership. This process will likely take a few months, even with the proper tooling. That being said, having the proper tools in place to record the dictionary can make maintaining and accessing the dictionary a much easier process. We suggest tackling the process head-on and early on. Many companies wait a long time before defining their data dictionary, which can make the process much more difficult in the future. Teams that invest the time to get alignment on their data can see major benefits in the long term as they make faster decisions as a team. Our team is working towards a tool that helps teams align on key business metrics and access their data from a single dashboard. If you're interested in trying out the tool, you can sign up to try the tool for free at [Secoda.co](http://secoda.co/).",14,0,secodaHQ,2021-03-13 21:58:41,https://www.reddit.com/r/bigdata/comments/m4gidc/step_by_step_guide_to_creating_a_data_dictionary/,0,bigdata
lacsnv,"Robinhood, UiPath, Databricks Raise Mammoth Rounds",,14,0,The-Techie,2021-02-01 20:12:20,https://www.thetechee.com/2021/02/robinhood-uipath-databricks-raise.html,0,bigdata
l1znv8,Modern Big Data Architectures - Lambda,,15,1,luminoumen,2021-01-21 14:39:29,https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa,0,bigdata
hjb5pu,"Python library for advanced Google News data mining: get news data by topics, geolocation, full-text search. Plus, clusters of similar topics",,15,0,kotartemiy,2020-07-01 14:23:55,https://github.com/kotartemiy/pygooglenews,0,bigdata
havuab,Zoe: a new CLI tool for Apache Kafka,"Hi!

Within Adevinta, we are heavy users of Apache Kafka. We built our own tooling around it. Recently, we open sourced one of these tools called Zoe. It's a high level command line tool to easily interact with Kafka. We use it heavily here and it significantly optimized our workflow.

* Checkout [the repository here](https://github.com/adevinta/zoe) where you will find a screen cast that demo the tool.
* And [the documentation](https://adevinta.github.io/zoe/)

We will soon write an article about it.

Any feedback is welcome : )",13,1,wlezzar,2020-06-17 17:28:56,https://www.reddit.com/r/bigdata/comments/havuab/zoe_a_new_cli_tool_for_apache_kafka/,0,bigdata
f9pb3u,Building a Serverless data processing platform at Nielsen,"Check out our blog post, describing how we built a data processing platform that handles 250 billion events per day, using Serverless architecture.

It discusses why we decided not to use Apache Spark for this specific use case, and what benefits we got from using AWS Lambda.

Read more about our Serverless real use-case here:  
[https://medium.com/nmc-techblog/going-serverless-a-real-use-case-1e40e80a2a9c](https://medium.com/nmc-techblog/going-serverless-a-real-use-case-1e40e80a2a9c)",15,4,ItaiYaffe,2020-02-26 06:54:05,https://www.reddit.com/r/bigdata/comments/f9pb3u/building_a_serverless_data_processing_platform_at/,0,bigdata
f4h6xk,"what differences and similarities are between ""dataflow engines"" and ""stream processing systems""?","what differences and similarities are between ""dataflow engines""   and ""stream processing systems""?

In book *Data Intensive Applications*, Ch 10 *Batch Processing* says

> **Batch processing systems (offline systems)**
> 
> A batch processing system takes a large amount of input data, runs a
> job to process  it,  and  produces  some  output  data.  Jobs  often 
> take  a  while  (from  a  few minutes to several days), so there
> normally isn’t a user waiting for the job to finish. Instead, batch
> jobs are often scheduled to run periodically (for example, once a
> day). The primary performance measure of a batch job is usually
> throughput (the time it takes to crunch through an input dataset of a
> certain size). We discuss batch processing in this chapter.
> 
> **MapReduce**
> 
> A MapReduce job can only start when all tasks in the preceding jobs (that generate its inputs) have completed. ...
> 
> **Dataflow Engine**
> 
> In order to fix these problems with MapReduce, several new execution engines for distributed batch computations were developed, the most well known of which are **Spark** [61, 62], Tez [63, 64], and Flink [65, 66]. There are various differences in the way they are designed, but they have one thing in common: they handle an entire workflow as one job, rather than breaking it up into independent subjobs. Since they explicitly model the flow of data through several processing stages, these systems  are  known  as  **dataflow  engines**.  Like  MapReduce,  they  work  by  repeatedly calling a user-defined function to process one record at a time on a single thread.  They parallelize work by partitioning inputs, and they copy the output of one function over the network to become the input to another function. ... We call these functions  **operators**.

>   MapReduce is like writing the output of each command to a temporary file, whereas dataflow engines look much more like
Unix pipes. Flink especially is built around the idea of **pipelined execution**: that is, incrementally passing the output of an operator to other operators, and not waiting  for the input to be complete before starting to process it.

Ch 11 *Stream Processing* is about 



> **Stream processing systems (near-real-time systems)**
> 
> Stream processing is somewhere between online and offline/batch
> processing (so it is sometimes called near-real-time or nearline
> processing). Like a batch processing  system,  a  stream  processor 
> consumes  inputs  and  produces  outputs (rather than responding to
> requests). However, a stream job operates on events shortly after they
> happen, whereas a batch job operates on a fixed set of input data.
> This difference allows stream processing systems to have lower latency
> than the equivalent batch systems. As stream processing builds upon
> batch processing, we discuss it in Chapter 11.

> Many open source distributed stream processing frameworks are designed with analytics in mind: for example, Apache Storm, **Spark Streaming**, Flink, Concord, Samza, and Kafka Streams [74]. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.

> **Spark performs stream processing** on top of a batch processing engine by breaking the stream into microbatches, whereas Apache Flink performs batch processing on top  of  a  stream  processing  engine.

I was wondering what differences and similarities are between ""dataflow engines"" (introduced in Ch 10 Batch Processing systems), and ""stream processing systems"" (introduced in Ch 11)?

- Do they both use pipeline to connect operators?

- Do they differ in whether the input data is bounded or unbounded?

- Do they have other differences than whether the input data is bounded or unbounded?


In Spark, according to Ch 11, 

- Spark Streaming is for streaming processing systems. 

- is everything else in Spark, e.g. structured APIs (DataSets, DataFrames, SQL) and low level APIs (RDDs, and Distributed Variables), for dataflow engines in batch processing systems? or for non-dataflow engines  in batch processing systems, similar to MapReduce?


Thanks.",14,8,timlee126,2020-02-15 22:49:25,https://www.reddit.com/r/bigdata/comments/f4h6xk/what_differences_and_similarities_are_between/,0,bigdata
ecfut5,The art of joining in Spark,,14,0,travellingsalesman2,2019-12-18 17:43:54,https://link.medium.com/9wxhc5rIi2,0,bigdata
dz454g,Introducing ksqlDB,,14,0,None,2019-11-20 16:10:29,https://www.confluent.io/blog/intro-to-ksqldb-sql-database-streaming,0,bigdata
cw127q,What are open datasets/APIs for learning big data?,"Sorry for vague question. I've been learning Spark and getting familiar with big data analytics for some time now and I wanted to make a program which would use ""real big"" dataset to feel the differences in time for example.   


Any good suggestions? I've already found [waymo](https://waymo.com/open/data/) but I don't know if cars are my thing ;)",14,4,Wylfryd,2019-08-27 07:33:06,https://www.reddit.com/r/bigdata/comments/cw127q/what_are_open_datasetsapis_for_learning_big_data/,0,bigdata
cp9hmd,Why Python is getting more popular…and how to use it with SQL databases,,14,0,Big_Data_Path,2019-08-12 08:22:15,https://bigdatapath.wordpress.com/2019/08/12/why-python-is-getting-more-popular-and-how-to-use-it-with-sql-databases/,0,bigdata
c9fw0v,Data lake or data warehouse - what approach do you use?,"Saw an interesting post [here](https://medium.com/swlh/tutorial-build-your-data-lake-using-aws-s3-athena-150c1aaa44cf) about one company's approach to using AWS S3 as a data lake, and utilising Athena and Glue to access the data rather than Redshift, albeit with some limitations which are discussed. 

There's also [this example](https://kognitio.com/blog/sentiment-analysis-amazon-reviews-pt1/) of pushing Python processing down into SQL queries which run against S3 data.

Also saw a Talend article comparing data lake to data warehouse [here](https://www.talend.com/resources/data-lake-vs-data-warehouse/), although I don't necessarily agree with all their commentary.

Do you use a model which puts has your data in a data lake, allowing various tools to dip into it, and write back results?

Or do you have the data locked down in a data warehouse instead, and either request extracts from that to allow you to process the data (particularly if you want to use Python or R, rather than just SQL), or possibly push the data processing into the warehouse itself should it have that capability?",14,10,mc110,2019-07-05 13:21:17,https://www.reddit.com/r/bigdata/comments/c9fw0v/data_lake_or_data_warehouse_what_approach_do_you/,0,bigdata
c8fgvm,Distributed Systems Engineering with Apache Kafka ft. Jason Gustafson [Podcast],,16,0,vicksyu,2019-07-02 21:40:45,https://www.buzzsprout.com/186154/1355206-distributed-systems-engineering-with-apache-kafka-ft-jason-gustafson,0,bigdata
c3sx2o,The Efficiency Paradox: What Big Data Can't Do,,14,1,Mynameis__--__,2019-06-22 18:12:03,https://www.youtube.com/watch?v=4nNZJRYtYQs,0,bigdata
c2izle,Getting Your Programming Skills Ready for Data Engineering,,15,0,eljefe6a,2019-06-19 16:11:42,https://www.jesse-anderson.com/2019/06/getting-your-programming-skills-ready-for-data-engineering/,0,bigdata
bdxbdj,"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",,14,0,None,2019-04-16 18:23:53,https://www.youtube.com/c/DiscoverArtificialIntelligence,0,bigdata
b5pofk,Open Source Business Intelligence,,13,0,MrMagsnificant,2019-03-26 13:25:27,https://blog.statsbot.co/open-source-business-intelligence-523ba185d530,0,bigdata
att6ec,An introduction to Spark GraphFrame with examples analyzing the Wikipedia link graph,,14,0,hagy,2019-02-23 08:13:30,https://towardsdatascience.com/an-introduction-to-spark-graphframe-with-examples-analyzing-the-wikipedia-link-graph-67e58c20a107,0,bigdata
aot2i7,Building a Big Data Machine Learning Spark Application for Flight Delay Prediction,,13,0,pedrodc23,2019-02-09 15:06:03,https://medium.com/@pedrodc/building-a-big-data-machine-learning-spark-application-for-flight-delay-prediction-4f9507cdb010,0,bigdata
aexj50,Productionizing Streaming ML | Apache NiFi - Apache Livy- Apache Spark - Tensorflow,,15,1,AshishKhuraishy,2019-01-11 17:16:24,https://codecampanion.blogspot.com/2019/01/productionizing-streaming-ml-apache.html,0,bigdata
8immxj,Local hadoop on laptop for practice [Video],,17,0,tomer-ben-david,2018-05-11 10:16:14,https://www.youtube.com/watch?v=FFnn-x1-C0k,0,bigdata
8ib25z,Understanding Hadoop in 5 minutes...,,15,2,stackchief,2018-05-10 01:00:27,https://www.stackchief.com/blog/An%20Introduction%20to%20Apache%20Hadoop,0,bigdata
8b7ymf,The Best of AI: New Articles Published This Month (March 2018),,14,1,clementwalter,2018-04-10 14:28:48,https://blog.sicara.com/03-2018-best-ai-new-articles-this-month-2ab944d4a536,0,bigdata
80f60l,Learn the fundamentals of data analytics in a new way- Experiential learning,,15,0,ath_ank,2018-02-26 18:19:40,https://learn.analyttica.com/courses/Fundamentals_of_Data_Analytics,0,bigdata
7t5sdm,How We Work: Bloomberg's Data Technologies Engineering Team,,17,0,mpmiller6,2018-01-26 16:48:07,https://www.youtube.com/attribution_link?a=LpeSnf51Doc&u=%2Fwatch%3Fv%3DqtUu9LCNmiU%26feature%3Dshare,0,bigdata
7nmuyw,Best of AI articles published in December,,15,2,nicolattuso,2018-01-02 14:54:25,https://blog.sicara.com/12-2017-best-ai-new-articles-this-month-fc80634faf84,0,bigdata
6npdxr,Useful Data Science Resources & Recommended Study Routes,,14,0,datasciencelover,2017-07-16 23:17:25,https://dluo.me/datascienceresources/,0,bigdata
6m6hwk,From Kafka to BigQuery: A guide for loading and streaming billions of daily events,,14,0,offf55,2017-07-09 08:50:50,https://medium.com/myheritage-engineering/kafka-to-bigquery-load-a-guide-for-streaming-billions-of-daily-events-cbbf31f4b737,0,bigdata
6lcp51,"Do you have an interest in a book ""Probabilistic data structures and algorithms in big data applications""?","Hello, I'm writing a book about various space-efficient data structures and non-deterministic algorithms that are useful in stream processing and big data applications. They are well known for researches, but most practitioners still afraid to use them.

You might have heard about some of them already from different sources, Bloom filter, HyperLogLog, MinHash, etc. My goal is to explain them all together as a class of data structures and algorithms with a similar idea behind. The book will be language agnostic because I want to give understanding of how they work, but encourage users to use already maintained implementations in language they programming.

What do you think if such a book could be interesting for the community?

Thank you,
Andrii

UPDATE [02/2019]. The book has been published and available at Amazon. Free samples and more information can be found at https://pdsa.gakhov.com",15,4,gakhov,2017-07-05 08:07:51,https://www.reddit.com/r/bigdata/comments/6lcp51/do_you_have_an_interest_in_a_book_probabilistic/,0,bigdata
640hgb,The 2017 Big Data Landscape,,15,0,steccami,2017-04-07 13:49:47,https://www.linkedin.com/pulse/firing-all-cylinders-2017-big-data-landscape-matt-turck,0,bigdata
60cepf,Python for Big Data in One Picture,,14,0,pmz,2017-03-19 21:08:09,http://www.datasciencecentral.com/profiles/blogs/python-for-big-data-in-one-picture,0,bigdata
5y96es,Google Cloud Spanner: our first impressions,,14,1,codepoetics,2017-03-08 17:15:37,https://opencredo.com/google-spanner-first-look/,0,bigdata
5jk3h0,Is it possible to get into this field with a double bachelor's in CS and statistics and a decent portfolio/github?,"Live in SF, CA

Basically I'm at a loss on what I should focus on. I can graduate in 1 year with a degree in CS. Or I can spend an extra year and get a double major in CS and statistics. 

The first option, I would just focus and try to specialize in mobile programming. 

The second option I'll only pick, if it leads to a better career than if I had chosen the first one. 




However it seems like the job market for mobile developers is huge with startIng salaries at ~$80,000. Whereas a lot Data science jobs require a master's degree. Furthermore the entry level jobs related to statistics seem to be less lucrative than mobile development. 

Any advice would be appreciated. Thanks. 

",15,8,Okmanl,2016-12-21 14:13:35,https://www.reddit.com/r/bigdata/comments/5jk3h0/is_it_possible_to_get_into_this_field_with_a/,0,bigdata
5iqg3x,Free Spark MLlib Course,,13,0,acangiano,2016-12-16 20:15:58,https://bigdatauniversity.com/courses/spark-mllib/,0,bigdata
5bng01,Traffic in London episode I: processing 100 billion IoT events,,15,0,fhoffa,2016-11-07 17:24:32,http://blog.datatonic.com/2016/10/traffic-in-london-episode-i-live.html,0,bigdata
57g6ey,Top 10 Reasons Why Big Data Analytics Is The Best Career Move,,15,0,GenocideRun,2016-10-14 12:50:24,http://www.greycampus.com/blog/big-data/top-10-reasons-why-big-data-analytics-is-the-best-career-move,0,bigdata
4x0w0w,Apache Flink: The 4G of Big Data Annalytics,,15,2,None,2016-08-10 06:52:51,http://hkotadia.com/archives/5741,0,bigdata
4t921g,An awesome post by our chief scientist on how we process Big Data using 1.5Kb memory,,16,0,ArielAssaraf,2016-07-17 09:57:16,http://coralogix.com/process-big-data-using-1-5kb/,0,bigdata
4rihus,How we reindexed 36 billions documents in 5 days within the same Elasticsearch cluster,,15,2,eberkut,2016-07-06 13:36:39,https://thoughts.t37.net/how-we-reindexed-36-billions-documents-in-5-days-within-the-same-elasticsearch-cluster-cd9c054d1db8,0,bigdata
4rd4er,Largest open source data visualization site for U.S. govt is now online,,14,1,mtl1015,2016-07-05 15:09:26,http://deloitte.wsj.com/cio/2016/07/05/datausa-aggregates-visualizes-complex-data/,0,bigdata
4i009s,10 times faster with 50 nodes: Querying Presto+AWS: 44secs. Presto+GCP: 4secs. Mark Lit benchmarks Google Cloud's Dataproc,,15,2,fhoffa,2016-05-05 13:58:19,http://tech.marksblogg.com/50-node-presto-cluster-dataproc.html,0,bigdata
4gvqso,Jay Kreps (Kafka co-founder and Confluent CEO) talks about what other tech he's keeping an eye on,,15,0,S__OBrien,2016-04-28 19:47:39,https://www.youtube.com/attribution_link?a=A4RFI_P3zqQ&u=%2Fwatch%3Fv%3DFHr8sfGvlAo%26feature%3Dshare,0,bigdata
4ea820,An Overview of Apache Streaming Technologies,,15,1,None,2016-04-11 11:00:05,https://databaseline.wordpress.com/2016/03/12/an-overview-of-apache-streaming-technologies/,0,bigdata
48xfz6,Agile Anti-patterns - The key to success isn’t just standing up,,14,1,51zero,2016-03-04 14:09:06,http://www.51zero.com/blog/2016/1/7/agile-anti-patterns-the-key-to-success-isnt-just-standing-up,0,bigdata
41wmmq,"Google, Cloudera, data Artisans, Talend, Cask, PayPal submit Dataflow to Apache incubator",,15,2,fhoffa,2016-01-20 22:01:22,http://googlecloudplatform.blogspot.com/2016/01/Dataflow-and-open-source-proposal-to-join-the-Apache-Incubator.html,0,bigdata
3x2qbn,Top 9 Big Data Podcasts To Sharpen Your Business Skills,,16,0,Annjann,2015-12-16 13:14:37,http://www.datapine.com/blog/big-data-podcasts/,0,bigdata
3ridkd,"ELI5: What is Apache Spark and where can I learn Big Data Engineering. Is there a helpful online tutorial (any resource), where a complete beginner (without any experience) can learn this technology?",,16,8,Ku_503,2015-11-04 16:40:45,https://www.reddit.com/r/bigdata/comments/3ridkd/eli5_what_is_apache_spark_and_where_can_i_learn/,0,bigdata
3q50a5,Building a recommendation engine in R,,15,2,nivek_d,2015-10-25 11:27:18,http://dataanalytics.zone/2015/10/recommendation-engine-in-r/,0,bigdata
3oi600,US military exploring 'big data' to prevent violent crimes by soldiers before they happen,,16,1,djphilosopher,2015-10-12 21:37:29,http://www.independent.co.uk/news/world/americas/us-military-exploring-big-data-to-prevent-violent-crimes-by-soldiers-before-they-happen-a6691036.html,0,bigdata
3b1p8k,2015 Apache Spark Summit Videos are now online at this link,,14,6,dgleebits,2015-06-25 06:46:23,https://spark-summit.org/2015/,0,bigdata
311885,"What's the best way to learn Big Data tools such as Apache Flume, Spark, Kafka, etc.?","Hello all,

I'd like to spend some time learning some of the Apache products oriented towards ""Big Data"".  While I can watch tutorials, etc. I'd like to do some practical exercises.  The problem seems to be, that unlike learning a standard programming language, specialized hardware (read, lots of disk and memory) as well as access to huge data sets are necessary.  Can someone give me some ideas as to how to get some practical experience here?

Thanks
",15,4,redmage123,2015-04-01 08:24:39,https://www.reddit.com/r/bigdata/comments/311885/whats_the_best_way_to_learn_big_data_tools_such/,0,bigdata
2z7ucz,Introducing Apache Spark 1.3,,15,0,futureisdata,2015-03-16 09:53:22,https://databricks.com/blog/2015/03/13/announcing-spark-1-3.html,0,bigdata
2vzpvb,"Apache Kafka 0.8.2.0 Released - high-throughput, publish-subscribe messaging system rethought of as a distributed commit log",,15,0,based4,2015-02-15 18:39:22,http://mail-archives.apache.org/mod_mbox/www-announce/201502.mbox/%3C20150204025036.19F3317A35%40minotaur.apache.org%3E,0,bigdata
1q91gd,Data Science Workflow: Overview and Challenges,,16,0,mhausenblas,2013-11-09 16:23:56,http://cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext,0,bigdata
1pjjh7,"'Big Data' Is Bunk, Obama Campaign's Tech Guru Tells University Leaders",,13,9,macarthy,2013-10-30 15:07:36,http://chronicle.com/blogs/wiredcampus/big-data-is-bunk-obama-campaigns-tech-guru-tells-university-leaders/47885?cid=wc&utm_source=wc&utm_medium=en,0,bigdata
1kr09s,In-Stream Big Data Processing,,14,2,h2o2,2013-08-20 18:39:12,http://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/,0,bigdata
1dtanl,Let us now praise data engineers,,13,0,scaleoutsoftware,2013-05-06 20:17:44,http://medriscoll.com/post/49783223337/let-us-now-praise-data-engineers,0,bigdata
zelzp5,Here’s a playlist of 7 hours of music with NO VOCALS I use to focus when I’m coding /learning . Post yours as well if you also have one!,[Spotify](https://open.spotify.com/playlist/6BoTD9WeyBN5rD5uyhbROO) | [Apple](https://music.apple.com/playlist/synthwave-focus-i/pl.u-b3b8Nm9C5MmMyM) | [Youtube](https://youtube.com/playlist?list=PLwO9YUACGAzS_XaTchS3qLgK5EGsb3qwa) | [Amazon](https://music.amazon.com/user-playlists/0375bdbe2f3d41f39523ddb35fc92ca0b1r),13,1,letsstartanew2,2022-12-06 23:39:28,https://www.reddit.com/r/bigdata/comments/zelzp5/heres_a_playlist_of_7_hours_of_music_with_no/,0,bigdata
xl5g1m,ClickHouse's speed as part of a managed data stack,"Here's a blog post comparing of some of the data warehouses. It looks like Mark has recently reviewed DoubleCloud's managed ClickHouse offering: [1.1 Billion Taxi Rides in ClickHouse on DoubleCloud](https://tech.marksblogg.com/billion-taxi-rides-doublecloud-clickhouse.html)

Double.Cloud is offering a managed data stack , in addition to ClickHouse.",14,0,ageje,2022-09-22 16:04:16,https://i.imgur.com/4FzgsrB.jpg,0,bigdata
x4lgnm,The Complete Guide To Web Scraping in Python,,15,0,prasenjitc,2022-09-03 04:27:00,https://proxiesapi.com/The-Complete-Guide-To-Web-Scraping-In_Python.php,0,bigdata
wfhp9p,eCharts for Python,,15,2,marklit,2022-08-03 20:51:49,https://tech.marksblogg.com/python-data-visualisation-echarts-graphs-plots.html,0,bigdata
w9hxun,Predictive analytics: benefits and prospects - Blog | Parsers VC,,15,0,Gill_Chloet,2022-07-27 15:38:07,https://parsers.vc/blog/predictive-analytics-benefits-and-prospects/,0,bigdata
usw92x,"YouTube's Database ""Procella""",,14,0,marklit,2022-05-19 05:29:53,https://tech.marksblogg.com/youtube-database-procella.html,0,bigdata
tkv70s,Modern data glossary,"Hey everyone! Our team put together a glossary of all of the important terms you'll need to know when working with the Modern Data Stack-- great for beginners to data and those who have been in the space for a while alike. Let me know your thoughts on it and happy browsing!

  
[https://www.secoda.co/glossary](https://www.secoda.co/glossary)

&#x200B;

If you have any other terms that you think we should add, let us know",13,0,secodaHQ,2022-03-23 14:29:21,https://www.reddit.com/r/bigdata/comments/tkv70s/modern_data_glossary/,0,bigdata
srudy6,When you think about the past and present of storage costs it’s amazing how far we’ve come that we can worry more about speed and compute instead. Always look forward but occasionally look back and appreciate how far we have come.,,14,4,amdatalakehouse,2022-02-13 22:07:13,https://i.redd.it/vkhb2rjpboh81.jpg,0,bigdata
nep6t6,Evil Geniuses Data and Tech in Esports Event,"Hey everyone, my name is Sean and I’m an intern at Evil Geniuses working with the data and tech team. We will be hosting a workshop this week (5/19 at 11 AM PT / 2 PM ET) focused on the use of data in esports. The two panelists who will be running the workshop are Soham “valens” Chowdhury (Head of Data Science) and Zach Kamran (Head of Tech and Analytics). I have copied the Eventbrite link below, please free free to sign up, the event is completely free. If you have any questions, please don't hesitate to reach out to me on twitter (@reegs191)!

Eventbrite: [https://www.eventbrite.com/e/data-and-tech-innovation-in-esports-tickets-154295117851](https://www.eventbrite.com/e/data-and-tech-innovation-in-esports-tickets-154295117851)",14,2,spregan19,2021-05-17 19:19:52,https://www.reddit.com/r/bigdata/comments/nep6t6/evil_geniuses_data_and_tech_in_esports_event/,0,bigdata
mqwoua,Kafka Web UI - Kowl v1.3.0 has been released,"Kowl is an open source Web UI for Apache Kafka with a focus on a good UI & UX. Yesterday we released a new version which includes many new features. Some noteworthy features are:

\- Protobuf & MsgPack deserialization support- Preview tags have autocompletion- Reassign partitions via setup wizard (e.g. to move data away from brokers or for balancing data across brokers)- Search messages by timestamp- Show automatically recognized Kafka Version (no JMX access required!)

We described some of these features in more detail here: [https://cloudhut.dev/blog/2021-04-13-kowl-release-v1-3-0/](https://cloudhut.dev/blog/2021-04-13-kowl-release-v1-3-0/)

GitHub Repo: [https://github.com/cloudhut/kowl](https://github.com/cloudhut/kowl)  
Full release notes: [https://github.com/cloudhut/kowl/releases/tag/v1.3.0](https://github.com/cloudhut/kowl/releases/tag/v1.3.0)

What do you think, are you missing any specific features that could make your life with Kafka easier or more produktive?",14,2,leventus93,2021-04-14 18:25:32,https://www.reddit.com/r/bigdata/comments/mqwoua/kafka_web_ui_kowl_v130_has_been_released/,0,bigdata
licusq,Common Date Manipulations on Data Science SQL Interviews,,14,6,boyzone87seo,2021-02-12 15:07:11,https://youtube.com/watch?v=TYHWv1vT0Pk&feature=share,0,bigdata
lcb4be,[VIDEO] - Streaming Concepts & Introduction to Apache Flink - Event Time and Watermarks,,15,0,Marksfik,2021-02-04 08:19:33,https://www.youtube.com/watch?v=QVDJFZVHZ3c,0,bigdata
jbo85v,"Machine learning from scratch in Python, video series tutorials.",[https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub\_confirmation=1](https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub_confirmation=1),14,0,dhiraj8899,2020-10-15 14:08:24,https://www.reddit.com/r/bigdata/comments/jbo85v/machine_learning_from_scratch_in_python_video/,0,bigdata
hrj3v2,Big Data without Hadoop/HDFS? MinIO tested on Jupter + PySpark,,15,9,mszymczyk,2020-07-15 07:19:01,https://medium.com/@zorteran/big-data-without-hadoop-hdfs-minio-tested-on-jupter-pyspark-7b89a249ec94?sk=aa7f74001ab8e487774da44b2afd2b3a,0,bigdata
gseiep,Apache Airflow 2.0: What to expect?,,15,2,marclamberti,2020-05-28 20:49:47,https://marclamberti.com/blog/apache-airflow-2-0-what-to-expect/,0,bigdata
gfqyc2,"Wharton School Receives $5 Million to Launch Artificial Intelligence for Business, Extending Its Commitment to Analytics, Learning, and Engagement",,13,0,smbale,2020-05-08 10:48:02,https://mpelembe.net/asia/asia-latest/?rkey=20200507UN00187&filter=19522#.XrU4mbDsYAw.reddit,0,bigdata
g7vkew,An Intro into the Lambda Architecture,,14,0,None,2020-04-25 15:43:34,https://medium.com/@christianreifberger/processing-big-data-by-using-the-lambda-architecture-2d823610d892,0,bigdata
fih6t0,The concept of workflow engines,,13,0,alekseiko,2020-03-14 11:52:24,https://medium.com/@aleksei.kornev/the-concept-of-workflow-engines-c14e8088283,0,bigdata
drtvzw,Self-promotion and blogspam,"I typically lurk but the links to the poster's own blog and promoting their own pages has been really bad for a while now. 

I know that the blatant links are against the rules but that doesn't seem to be enforced much. Could we flair bloggers/self promoters so we know who is pushing their own media? Maybe put self-promotion in a single thread. 

The front page of /r/bigdata has 5 of the top posts submitted by their writers, one of them has 2 self-promoted posts.

It's interesting, but not really constructive. Especially when the posts are regurgitation the same things we've see hundreds of times before... are there really people here that don't know what a histogram is? Why color matters? That knowing your audience is of value?",15,3,N1934194,2019-11-05 04:05:20,https://www.reddit.com/r/bigdata/comments/drtvzw/selfpromotion_and_blogspam/,0,bigdata
dbh6f7,11 Best Open-Source Data Visualization Tools (with a comparison matrix),,14,0,happyremoteworker,2019-09-30 19:51:52,https://blog.kolabtree.com/11-best-open-source-data-visualization-tools/,0,bigdata
cnsggu,What kind of ETL challenges do engineers face with distributed Big Data systems?,"I recently started working on big data. We are currently evaluating Snowflake vs Redshift for our Data Warehouse solution.

I want to learn from other people's experiences that, what kind of ETLs/ELTs challenges the ""Data Engineers"" face in general with distributed Big Data systems? This will help me to be prepared and build my ETLs keeping what I learn in mind.

One of the pain points that I see (apart from the volume of the data) is “Late Arriving Data”. How do you handle this situation efficiently in the Big Data World?

Here are some of the challenges that I can think of -

1) **The volume of the data** \- the ETL compute power doesn't support the volume of the data and it is not auto-scalable.

2) **Late Arriving Data** \- If the data arrives 1+ day(s) late then how do you efficiently put that data in its corresponding partition?

3) **The performance of data retrieval (query)** \- I feel the query performance depends a lot on how you partition your data and use that partition efficiently in the query.

I would love to hear some of the big data challenges that experts have faced with respect to Extract-Load-Transform (ELT) / Extract-Transform-Load(ETL) and how they handled it.

Thank you in advance!",12,8,Guddi83,2019-08-08 21:36:57,https://www.reddit.com/r/bigdata/comments/cnsggu/what_kind_of_etl_challenges_do_engineers_face/,0,bigdata
cffxd7,"Data Analysis With Dask – A Python Scale-Out, Parallel Computation Framework For Big Data",,16,6,dingopole,2019-07-20 01:11:19,http://bicortex.com/data-analysis-with-dask-a-python-scale-out-parallel-computation-framework-for-big-data/,0,bigdata
cdqtkg,"An interview about Clickhouse, an open source, columnar data warehouse built for massive scale and speed to enable interactive analytics",,14,0,blarghmatey,2019-07-16 02:01:33,https://www.dataengineeringpodcast.com/clickhouse-data-warehouse-episode-88/,0,bigdata
bybl8a,How can I handle 100 million+ rows of data with python for a machine learning project?,"Hi everyone, I have just been given access to a 100m+ rows of data housed in a MySQL database on AWS.

I tried loading the database just as s test with pandas by choosing a chunk size of 1,000,000 and the connection broke after a long time trying to load.

This is my first taste of a really ""big dataset"" and I'm out of ideas how to even load the data. 

Any pointers for me?",15,25,Pimp_Fada,2019-06-08 19:28:35,https://www.reddit.com/r/bigdata/comments/bybl8a/how_can_i_handle_100_million_rows_of_data_with/,0,bigdata
budvo1,Call for Contributors - Open-Source Kafka-like streaming engine written in Rust,"Hello fellows,

I'm implementing a Kafka/Kinesis-like append-only commit-log streaming. My goals are to implement a streaming-engine that runs with a single binary, easy to distribute, no need for zookeeper, easy to host/operate, with pull-based API, similar to Kinesis, where the consumers are not registered and the consumers manage their own cursors, among other things.

[https://github.com/14-bits/voik](https://github.com/14-bits/voik)

At this point, I have built the main parts of the commit-log, I'm trying to follow [this series](https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-1-storage-mechanics/) of articles for insights on what to build first... as soon as I feel the storage mechanics is good I'll start looking into data replication and delivery.

I would really appreciate contributions both to the code from a Rust perspective (I'm quite new to the language), as much as from system's perspective and such.

The project has a small roadmap with items that I'm working, but if people are interested in collaborating we could probably move those to more meaningful issues for discussion purposes.

There is also a lot of tooling/CI/documentation and community related topics that need attention, so if you feel like participating but is not an expert, it's a good place to start.",14,2,marceloboeira,2019-05-29 12:10:51,https://www.reddit.com/r/bigdata/comments/budvo1/call_for_contributors_opensource_kafkalike/,0,bigdata
bnu5ih,Decision/classification/regression tree research papers from the last 30 years,"[https://github.com/benedekrozemberczki/awesome-decision-tree-papers](https://github.com/benedekrozemberczki/awesome-decision-tree-papers)

A curated list of decision, classification  and regression tree research papers with implementations from the  following conferences.

Machine learning:

1. NeurIPS
2. ICML
3. ICLR

Computer vision:

1. CVPR
2. ICCV
3. ECCV

Natural language processing:

1. ACL
2. NAACL
3. EMNLP

Data Mining:

1. KDD
2. ICDM
3. CIKM
4. WWW

Artificial intelligence:

1. AAAI
2. IJCAI
3. UAI
4. AISTATS",14,0,benitorosenberg,2019-05-12 21:03:58,https://www.reddit.com/r/bigdata/comments/bnu5ih/decisionclassificationregression_tree_research/,0,bigdata
b6grix,"is there a platform or package or distro, which gives a hands on experience on Apache Spark quickly, instead of installing a cluster/infra by himself?","is there a platform or package or distro, which gives a hands on experience on Apache Spark, instead of installing a cluster by himself? 

&#x200B;

So one can easy hacking with spark, instead of getting the infrastructure ready. Any free solution is preferred over paid ones. 

&#x200B;

Please suggest.",13,9,None,2019-03-28 09:43:42,https://www.reddit.com/r/bigdata/comments/b6grix/is_there_a_platform_or_package_or_distro_which/,0,bigdata
b102t7,Flink and Prometheus: Cloud-native monitoring of streaming applications,,15,0,Marksfik,2019-03-14 13:02:49,https://flink.apache.org/features/2019/03/11/prometheus-monitoring.html,0,bigdata
aypkl3,Book review: Designing Data-Intensive Applications by Martin Kleppmann,[https://blog.softwaremill.com/designing-data-intensive-applications-by-martin-kleppmann-a-review-20406d7132f9](https://blog.softwaremill.com/designing-data-intensive-applications-by-martin-kleppmann-a-review-20406d7132f9),13,1,smlaccount,2019-03-08 12:10:42,https://www.reddit.com/r/bigdata/comments/aypkl3/book_review_designing_dataintensive_applications/,0,bigdata
asluw1,Whats new in Spark 2.4!,"My first attempt to write something.  
[https://medium.com/@akhilanand.bv/whats-new-in-spark-2-4-121162f1c385](https://medium.com/@akhilanand.bv/whats-new-in-spark-2-4-121162f1c385)

Any suggestions about the write up are welcome.",13,2,akhilanandbv003,2019-02-20 08:20:04,https://www.reddit.com/r/bigdata/comments/asluw1/whats_new_in_spark_24/,0,bigdata
aqlxcj,Apache Flink: Batch as a Special Case of Streaming,,14,0,Marksfik,2019-02-14 17:05:02,https://flink.apache.org/news/2019/02/13/unified-batch-streaming-blink.html,0,bigdata
apswu2,4 Key Components of a Streaming Data Architecture,,14,0,SiegurdSilver,2019-02-12 12:38:47,https://www.upsolver.com/blog/streaming-data-architecture-key-components?utm_source=reddit&utm_medium=referral&utm_campaign=bigdata,0,bigdata
an0ehf,R and its data visualization libraries have become powerful to tackle the toughest geospatial data. Enroll in our latest GIS course for free and get started with manipulating and mapping geospatial data!,,14,0,Cocohoney16,2019-02-04 10:24:43,https://soco.ps/2UBseSr,0,bigdata
amm4dm,Implementing Neural Networks from Scratch,,14,0,None,2019-02-03 04:07:11,https://www.youtube.com/watch?v=GqfzCTpCODE,0,bigdata
akoroc,Why Tencent chooses Apache Flink over Apache Storm to power its stream processing platform,,15,0,Marksfik,2019-01-28 15:58:33,https://www.da-platform.com/blog/oceanus-platform-powered-by-apache-flink,0,bigdata
adh5ar,The Feature Store: the Future API between Data Engineering and Data Science?,,14,2,jpdowlin,2019-01-07 12:17:23,https://www.logicalclocks.com/feature-store/,0,bigdata
a710g2,A Brief Introduction to PySpark – Towards Data Science,,13,0,QuirkySpiceBush,2018-12-17 15:58:47,https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873,0,bigdata
9xqb0g,"For anyone looking to get into machine learning, I would advise that you don't learn the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",,14,3,antaloaalonso,2018-11-16 21:15:40,https://www.youtube.com/watch?v=yMzTrZ3_NIA&t=2s,0,bigdata
9fig60,Apache Kafka with and without a Data Lake,,15,2,SiegurdSilver,2018-09-13 14:28:01,https://www.upsolver.com/blog/blog-apache-kafka-and-data-lake,0,bigdata
93pn6r,Can you share your workflows around big data exploratory work?,"When ""data sciencing"" over big data sets, especially during exploratory phases with the business units, I find that I'll often work in a mix of REPL and scratch file (python) for screencasting and/or someone over your shoulder type situations... these inevitably end up as a heaps of barely stable top level code connected to endlessly chopped up data files...

It's the nature of the beast and these kinds of explorations aren't worth turning into full fledged projects that I'd put in a git repo, but I'm starting to get aggravated by the slash and dashness of it. By far, the most aggravating thing being matplotlib generated graphs that are shared in our internal message boards. These graphs get shared specifically because they capture insights very well, but I find that it's pretty much impossible to make them ""carry their baggage"" with them: code *and* the underlying data...

I tried working with jupyter notebooks for a while, but couldn't quite get a feel for it for this type of exploratory work... 

What's you guys' workflows around this type of work? How do you preserve the slightest amount of retraceability in your exploration?




",14,1,perspectiveiskey,2018-08-01 14:53:22,https://www.reddit.com/r/bigdata/comments/93pn6r/can_you_share_your_workflows_around_big_data/,0,bigdata
91c6a3,Big Data Strategy Discussion,"I work for a fortune 10 company. I was recently tasked to lead a global team to create a prototype for a big data solution. I am still somewhat new to the big data world and this is a bit of a challenge to me. My company built a tool that is designed to incorporate many open source technologies with some COTS visualization tools( Hadoop, Spark, Apache HIVE, Cognos, Power BI). I am trying to come up with a prototype or concept that leverages the power of these tools to maximize insights for our business users. I am a somewhat technical person so I am not struggling with understanding the technology. I am struggling with understanding the challenges and problems that big data is solving ( both from an organizational perspective and from a global perspective). Does anyone have any insights into what kind of challenges business users face in deriving insights from big data? Specifically what are other organizations doing to solve these problems and challenges as they come up? Any insights would be much appreciated. Thank you. ",14,15,ATXsurfer29,2018-07-23 23:32:43,https://www.reddit.com/r/bigdata/comments/91c6a3/big_data_strategy_discussion/,0,bigdata
8ry1lt,"New to hadoop and spark, where can I learn and experiment with them without having to install the software?","New to hadoop and spark, where can I learn and experiment with them without having to install the software?  
Thanks.",12,9,smallbee2,2018-06-18 09:00:48,https://www.reddit.com/r/bigdata/comments/8ry1lt/new_to_hadoop_and_spark_where_can_i_learn_and/,0,bigdata
8lrlnd,Walmart's next healthcare move: Using data to identify bad doctors,,14,0,LiamBigDataDonoghue,2018-05-24 10:32:17,https://www.techrepublic.com/article/walmarts-next-healthcare-move-using-data-to-identify-bad-doctors/,0,bigdata
8k3uus,Practical Apache Spark in 10 minutes. Part 1 - Ubuntu installation,,13,0,viktoriia_shulga,2018-05-17 12:27:27,https://www.datascience-school.com/blog/practical-apache-spark-in-10-minutes-part-1-ubuntu-installation/?utm_source=reddit&utm_medium=bigdata&utm_campaign=ubuntu_install,0,bigdata
8d4pvr,Impact of Big Data in Healthcare,,14,2,alzador123,2018-04-18 10:53:47,https://www.goodworklabs.com/big-data-in-healthcare/,0,bigdata
71mb48,Google launches Cloud Dataprep in public beta to help companies clean their data before analysis,,13,3,fhoffa,2017-09-21 21:17:00,https://venturebeat.com/2017/09/21/google-launches-cloud-dataprep-in-public-beta/,0,bigdata
6zqknj,The Data That Turned the World Upside Down,,14,0,wewewawa,2017-09-12 22:48:17,https://motherboard.vice.com/en_us/article/mg9vvn/how-our-likes-helped-trump-win,0,bigdata
6x1hdq,Bundle of O'Reilly Data Science Books from HumbleBundle,,15,1,carpb202,2017-08-30 19:15:48,https://www.humblebundle.com/books/data-science-books,0,bigdata
6qzcqo,How WePay uses stream analytics for real-time fraud detection using GCP and Apache Kafka,,13,0,fhoffa,2017-08-01 21:15:49,https://cloud.google.com/blog/big-data/2017/08/how-wepay-uses-stream-analytics-for-real-time-fraud-detection-using-gcp-and-apache-kafka,0,bigdata
6ic8bb,gab.lc - GCE BigQuery vs AWS Redshift vs AWS Athena (w/ 1TB of CSV data),,15,3,fhoffa,2017-06-20 06:12:24,https://www.gab.lc/articles/bigquery-vs-redshift-vs-athena,0,bigdata
6av4bz,CarbonData: indexed columnar data format for fast analytics on big data platform,,13,0,based2,2017-05-13 00:43:02,http://carbondata.apache.org/,0,bigdata
5pxehd,"Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance",,14,1,steccami,2017-01-24 17:26:18,http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/,0,bigdata
5ovw4t,ETL vs ELT. Pros and Cons of each approach to data ingestion,,13,6,petergorne,2017-01-19 10:35:53,http://panoply.io/blog/etl-vs-elt-the-difference-is-in-the-how/,0,bigdata
5g0mxm,"Guide: How to Set Up Spark with Jupyter painlessly on AWS EC2 clusters, with S3 I/O",,15,1,datasciencelover,2016-12-02 02:01:58,https://github.com/PiercingDan/spark-Jupyter-AWS,0,bigdata
5eeqnr,"What Neural Networks, Artificial Intelligence, and Machine Learning Actually Do",,14,2,d4v1dv00,2016-11-23 01:58:10,http://lifehacker.com/what-neural-networks-artificial-intelligence-and-mach-1789259060,0,bigdata
58bb22,Data Scientist vs Data Analyst- The Difference?,"According to Glassdoor, the national average salary for a Scientist is 113K while an analyst is 60K. So, clearly there is a difference- but what is it?

(https://www.glassdoor.com/Salaries/atlanta-data-analyst-salary-SRCH_IL.0,7_IM52_KO8,20.htm; https://www.glassdoor.com/Salaries/atlanta-data-scientist-salary-SRCH_IL.0,7_IM52_KO8,22.htm)",14,7,None,2016-10-19 16:47:21,https://www.reddit.com/r/bigdata/comments/58bb22/data_scientist_vs_data_analyst_the_difference/,0,bigdata
57n60c,Using Spark SQL for ETL,,16,0,pmz,2016-10-15 17:44:44,https://aws.amazon.com/blogs/big-data/using-spark-sql-for-etl/,0,bigdata
4sk0cp,Artificial Intelligence Is Setting Up the Internet for a Huge Clash With Europe,,15,0,dunkin1980,2016-07-12 23:21:33,http://www.wired.com/2016/07/artificial-intelligence-setting-internet-huge-clash-europe,0,bigdata
4pnxgw,Bestof /r/bigdata: /u/GooberMcNutly draws parallels between web programming in the late 90's and data science today | 'Everything that is happening in big data these days happened in web programming languages ten (twenty?) years ago.',,14,0,citizen64,2016-06-24 16:58:22,https://www.reddit.com/r/bigdata/comments/4pfuaz/why_data_scientists_are_not_the_future_of_big_data/d4l80w6,0,bigdata
4n6bx8,"As a (degree-less) software developer, what should I learn if I want to become a Big Data Practitioner/Engineer/Developer ?",[...],13,12,None,2016-06-08 17:49:32,https://www.reddit.com/r/bigdata/comments/4n6bx8/as_a_degreeless_software_developer_what_should_i/,0,bigdata
4jnn5s,"Learn modern design patterns in Big Data, Hadoop & Spark",,14,0,tony_sf,2016-05-16 22:13:27,http://media.bemyapp.com/modern-design-patterns-big-data-hadoop-spark/?utm_source=bma&utm_medium=reddit&utm_content=tony&utm_campaign=media,0,bigdata
4i9sm7,New to Big Data? You should also meet his little brother: Data.,,13,0,Insalgo,2016-05-07 12:23:51,http://www.insalgo.com/blog/2016/05/06/how-one-could-help-millions,0,bigdata
4dvyah,"""Geek Professor Drops Rap Video"" - Song About the Power of Big Data :)",,15,1,MShalyt,2016-04-08 11:26:15,https://www.youtube.com/watch?v=bSP3z0LmWEg,0,bigdata
44qi1e,How Apache Flink enables new streaming applications part 2: State and versioning,,15,0,ktzoumas,2016-02-08 11:19:51,http://data-artisans.com/how-apache-flink-enables-new-streaming-applications/,0,bigdata
41hlti,Evaluating Big Data Performance of PrestoDB and Parquet on S3 storage,,15,8,nameBrandon,2016-01-18 05:51:36,http://brandonharris.io/evaluating-big-data-performance-of-prestodb-and-parquet-on-s3-storage/,0,bigdata
3tgsdx,Graph-theory breakthrough tantalizes mathematicians,,14,2,NikiVandel,2015-11-19 18:46:47,http://www.nature.com/news/graph-theory-breakthrough-tantalizes-mathematicians-1.18801,0,bigdata
3qg33r,The New Data Engineering Ecosystem: Trends and Rising Stars,,15,0,Re_JenA,2015-10-27 17:44:37,http://insightdataengineering.com/blog/new-ecosystem/,0,bigdata
3baz7f,Spinning Up a Free Hadoop Cluster: Step by Step,,15,0,Re_JenA,2015-06-27 14:09:53,http://insightdataengineering.com/blog/hadoopdevops/,0,bigdata
32itq0,"Learn R and data science coding for free, interactively and online",,16,0,martijnT,2015-04-14 03:19:49,https://www.datacamp.com/swirl-r-tutorial,0,bigdata
3240zg,Apache Spark: A Look under the Hood,,14,0,arushkharbanda,2015-04-10 12:06:14,https://www.sigmoid.com/apache-spark-internals/,0,bigdata
3005x4,Data Engineer vs Data Scientist vs Business Analyst,,16,23,dodgyfox,2015-03-23 12:14:13,https://medium.com/@KevinSchmidtBiz/data-engineer-vs-data-scientist-vs-business-analyst-b68d201364bc,0,bigdata
2zl5k2,Looking for Open Data? Try this indexer,,14,2,krsn_,2015-03-19 14:26:22,http://www.data-search-engine.com/,0,bigdata
2ojxc8,Databricks to run free MOOC: Intro to Big Data with Apache Spark,,15,1,edXbecky,2014-12-07 15:05:52,http://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html,0,bigdata
2lbkp0,Research @ Facebook - Top Open Data Problems,,12,1,flipstables,2014-11-05 01:16:28,https://research.facebook.com/blog/1522692927972019/facebook-s-top-open-data-problems/,0,bigdata
2elwaa,An introduction to Apache Hadoop for big data,,12,1,None,2014-08-26 07:24:19,http://opensource.com/life/14/8/intro-apache-hadoop-big-data,0,bigdata
2983ss,The Elephant was a Trojan Horse: On the Death of Map-Reduce at Google,,16,1,txoki,2014-06-27 07:37:00,http://the-paper-trail.org/blog/the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/,0,bigdata
21bdjq,Yahoo! Labs datasets,,16,1,mhausenblas,2014-03-25 12:13:23,http://webscope.sandbox.yahoo.com/catalog.php,0,bigdata
1ug6fz,"A multi-threaded, Twitter-searching data miner in Python",,12,1,hlt99,2014-01-05 08:04:16,https://github.com/rc0r/AvivoreXT,0,bigdata
1t8qon,"All Packt ebooks and videos are $5 including books on Hadoop, Machine learning, Spark, Storm and many more books in the Big Data category",,16,2,Bowkers1,2013-12-19 12:28:17,https://www.packtpub.com/ebookbonanza,0,bigdata
1e4s2h,"New Big Data Book free on Amazon ""Secrets of the Big Data Revolution""",,13,0,DataDomino,2013-05-11 15:51:30,http://www.amazon.com/Secrets-Big-Data-Revolution-ebook/dp/B00CLSW0RA/ref=sr_1_1?s=digital-text&ie=UTF8&qid=1368284328&sr=1-1&keywords=secrets+of+the+big+data+revolution,0,bigdata
ydep9,think Hardoop is cutting edge?  check out Dremel,,15,3,anupakkihal,2012-08-17 11:38:31,http://www.wired.com/wiredenterprise/2012/08/google-dremel-versus-hadoop/,0,bigdata
uow0a,"Why is R so popular in the big data community? Python/Numpy/Scipy seems like a more natural choice, having memory-mapped arrays and strong HDF5 support, et cetera? Just wondering out loud.",,15,31,SIULHT,2012-06-07 00:08:55,http://www.theregister.co.uk/2012/06/03/big_data_r_statistical_analysis/,0,bigdata
151a7rr,I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube,"Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!

[https://www.youtube.com/watch?v=aiHSMYvoqYE](https://www.youtube.com/watch?v=aiHSMYvoqYE)",12,0,onurbaltaci,2023-07-16 16:14:21,https://www.reddit.com/r/bigdata/comments/151a7rr/i_recorded_a_crash_course_on_polars_library_of/,0,bigdata
14d4gua,Big data Hadoop and Spark Analytics Projects (End to End)," 

Hi Guys,

I hope you are well.

Free tutorial on Bigdata Hadoop and Spark Analytics Projects (End to End) in **Apache Spark, Bigdata, Hadoop, Hive, Apache Pig, and Scala with Code and Explanation.**

***Apache Spark Analytics Projects:***

1) [Vehicle Sales Report – Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/vehicle-sales-report-data-analysis/)

2) [Video Game Sales Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/video-game-sales-data-analysis/)

3) [Slack Data Analysis in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/slack-data-analysis/)

4) [Healthcare Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/healthcare-analytics-for-beginners-part-1/)

5) [Marketing Analytics for Beginners](https://projectsbasedlearning.com/apache-spark-analytics/marketing-analytics-part-1/)

6) [Sentiment Analysis on Demonetization in India using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/sentiment-analysis-on-demonetization-in-india-using-apache-spark/)

7) [Analytics on India census using Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/analytics-on-india-census-using-apache-spark-part-1/)

8) [Bidding Auction Data Analytics in Apache Spark](https://projectsbasedlearning.com/apache-spark-analytics/bidding-auction-data-analytics-in-apache-spark/)

***Bigdata Hadoop Projects:***

1) [Sensex Log Data Processing (PDF File Processing in Map Reduce) Project](https://projectsbasedlearning.com/bigdata-hadoop/sensex-log-data-processing-pdf-file-processing-in-map-reduce-part-1/)

2) [Generate Analytics from a Product based Company Web Log (Project)](https://projectsbasedlearning.com/bigdata-hadoop/generate-analytics-from-a-product-based-company-web-log-part-1/)

3) [Analyze social bookmarking sites to find insights](https://projectsbasedlearning.com/bigdata-hadoop/analyze-social-bookmarking-sites-to-find-insights-part-1/)

4) [Bigdata Hadoop Project - YouTube Data Analysis](https://projectsbasedlearning.com/bigdata-hadoop/youtube-data-analysis-part-1/)

5) [Bigdata Hadoop Project - Customer Complaints Analysis](https://projectsbasedlearning.com/bigdata-hadoop/customer-complaints-analysis-part-1/)

I hope you'll enjoy these tutorials.

Thanks and Regards

Bigdata Engineer",14,0,bigdataengineer4life,2023-06-19 04:31:29,https://www.reddit.com/r/bigdata/comments/14d4gua/big_data_hadoop_and_spark_analytics_projects_end/,0,bigdata
13kj14h,ODD Platform - An open-source data discovery and observability service - v0.12 release,,13,1,DarronFeldstein,2023-05-18 00:21:30,https://github.com/opendatadiscovery/odd-platform,0,bigdata
12hskrx,Apache Iceberg vs Delta Lake | IOMETE's case study,"When it comes to storing and managing large amounts of data, the choice of technology can make all the difference. At IOMETE, we have evaluated several options and ultimately decided to use  Apache Iceberg over Delta Lake. In this blog post, we will explain why we chose Iceberg over Delta Lake.

[https://iomete.com/blog/apache-iceberg-delta-lake](https://iomete.com/blog/apache-iceberg-delta-lake)",14,4,AytanJalilova,2023-04-10 19:06:06,https://www.reddit.com/r/bigdata/comments/12hskrx/apache_iceberg_vs_delta_lake_iometes_case_study/,0,bigdata
yp6d69,"We’re Christian Mouchet, Jean-Philippe Bossuat, Kurt Rohloff, Nigel Smart, Pascal Paillier, Rand Hindi, Wonkyung Jung, various researchers and library developers of homomorphic encryption to answer questions about homomorphic encryption and why it’s important for the future of data privacy! AMA",,13,2,carrotcypher,2022-11-08 00:45:46,/r/privacy/comments/yp5enz/were_christian_mouchet_jeanphilippe_bossuat_kurt/,0,bigdata
yeszjj,Predictive analytics in Venture investing,,13,0,Gill_Chloet,2022-10-27 14:10:50,https://parsers.vc/blog/predictive-analytics-in-venture-investing/,0,bigdata
uz7egl,Researchers From Imperial College London introduce TsT-GAN: A Novel Framework For Training Time-Series Generative Models,"Nowadays, data is considered a fuel in the data analytics field. The real-time applications require time series data for analysis and future prediction. But all these applications usually lack the necessary, sufficient data for analysis. Hence, various data augmentation techniques need to be adopted. Researchers from Imperial College London introduce a framework called [TsT-GAN](https://arxiv.org/pdf/2205.11164v1.pdf), based on generative adversarial networks (GAN), that are utilized to augment the time-series data. It aims to fulfill the various objectives like capturing the steps of the conditional distribution of real-time sequences and creating a model that joins the distribution of all the real-time sequences.

The paper’s significant contribution is to develop the model consisting of a generator that can produce entire joint distributions considering the distribution conditions. The training framework can be applied to any time series dataset that quantitively results in a standard method that can be trained on the synthetic test on a realistic approach while qualitatively using t-SNE. 

[Continue Reading](https://www.marktechpost.com/2022/05/27/researchers-from-imperial-college-london-introduce-tst-gan-a-novel-framework-for-training-time-series-generative-models/) | *Check out the* [*paper*](https://arxiv.org/pdf/2205.11164v1.pdf) *and* [*related codes*](https://github.com/jsyoon0823/TimeGAN)*.*

&#x200B;

https://preview.redd.it/p1qtj77xy2291.png?width=778&format=png&auto=webp&s=6d09255b613d3ed4ad11706d9d635f8d2b6316b2",13,0,No_Coffee_4638,2022-05-27 20:45:20,https://www.reddit.com/r/bigdata/comments/uz7egl/researchers_from_imperial_college_london/,0,bigdata
uypjzb,Deploying 5G Around Trees,,12,0,marklit,2022-05-27 04:00:00,https://tech.marksblogg.com/tree-heights-open5g.html,0,bigdata
uiveu7,Apache Flink: Announcing the Release of Apache Flink 1.15,,14,0,Marksfik,2022-05-05 11:32:30,https://flink.apache.org/news/2022/05/05/1.15-announcement.html,0,bigdata
u7zwk3,Apache Kafka Visualisation Tool (updated),,13,0,rmoff,2022-04-20 16:05:17,/r/apachekafka/comments/u7zmzz/apache_kafka_visualisation_tool_updated/,0,bigdata
txlwqy,Yaetos-A framework to simplify the creation of data pipelines,"Here is a blog post explaining Yaetos, an open source data framework I started a while ago and used in previous companies: [https://medium.com/@arthurprevot/yaetos-data-framework-description-ddc71caf6ce](https://medium.com/@arthurprevot/yaetos-data-framework-description-ddc71caf6ce) . I'll be happy to get some feedback.",13,0,arthurprevot,2022-04-06 13:14:34,https://www.reddit.com/r/bigdata/comments/txlwqy/yaetosa_framework_to_simplify_the_creation_of/,0,bigdata
tin0fi,Mastering SQL Query Optimization,,14,0,Koushik5586,2022-03-20 15:04:18,https://tvkoushik.medium.com/mastering-sql-query-optimization-622cc6addef2?sk=8e8edbfd5cd370461aa25cfb8d667c12,0,bigdata
m9d8qr,This Transparency Project Is Creating a Massive Collection of Police Data,,13,0,transtwin,2021-03-20 18:39:49,https://www.vice.com/en/article/5dpxvq/this-transparency-project-is-creating-a-massive-collection-of-police-data,0,bigdata
lqtfqr,Discover data in python like using excel,,13,0,Q26239951,2021-02-23 21:22:47,https://www.youtube.com/watch?v=9aMd6yNm1Ok,0,bigdata
kwm8s7,Open Beta Kafka IDE 2021.1.1 being released today! 🎉,"[Kafka IDE](https://kafkaide.com/?utm_medium=social&utm_campaign=release-2021.1.1&position=top&utm_source=Reddit&utm_term=bigdata) is a desktop client similar to Tableau or Looker that queries Apache Kafka directly from your desktop.

\---

When working with any sized cluster, you will find yourself executing the same queries over and over.

## Introducing Tabs

[KafkaIDE 2020.1.1](https://kafkaide.com/?utm_medium=social&utm_campaign=release-2021.1.1&position=middle&utm_source=Reddit&utm_term=bigdata) now supports multiple editor sheets where you can save and run your favorite queries without losing track of them. Every sheet is independent of each other. This means every tab can point to different Kafka clusters and have different selected fields or filtering where clauses.

Each tab or editor sheet has an independent topic offset window as well. We don't know if that's the best design approach so please let us know your thoughts.

## Secure connections are becoming easier to configure.

We decided to implement Kafka IDE using the latest Apache Kafka's Java libraries to support the latest features as they are released. For instance, we supported the new Protobuf and JsonSchema schemas as Confluent Schema Registry was introducing them.

Sometimes though, this approach has few downsides. One of them is Java-based Apache Kafka's official libraries only supporting SSL certificates by using [Java KeyStores](https://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html). 

Apache Kafka security is a broad and deep topic. Sometimes, it can be challenging to configure secured connections to Kafka clusters properly. 

We are happy to announce that from this version, we are also **supporting CA public certificates by using the 'ssl.ca.location' config** as an alternative to using 'ssl.truststore.location' and 'ssl.truststore.password'.

We will keep putting efforts into this matter as we release new versions.

## Full release notes

* Tabs
* Support for 'ssl.ca.location' configuration.
* Fixed an issue where the 'Apache Kafka is running locally' notification appeared even when you already had registered it.
* Fixed a UI mismatch where topic fields were incorrectly highlighted in the left panel when selected.
* Fixed an issue where dragging a where filter condition into the select box crashed the application. Now you can safely drag fields from Select and Where boxes to each other.

## We want your feedback!

If you like Kafka IDE and think we can do better, we would love to hear from you. Let us know if you are available for a quick chat.

As usual, you can download the latest version at [kafkaide.com/download](https://kafkaide.com/download/?utm_medium=social&utm_campaign=release-2021.1.1&position=bottom&utm_source=Reddit&utm_term=bigdata).",13,1,kafkaide-com,2021-01-13 18:13:22,https://www.reddit.com/r/bigdata/comments/kwm8s7/open_beta_kafka_ide_202111_being_released_today/,0,bigdata
kmbsrn,Insights about LOL using Big data analysis! [Code attached],"Hello All,   
Hope you are safe and sound :)
This semester we are taking this course called ""Big data analysis"" and I tried to get insight by using the public dataset provided by Riot games. I used patch 10.9 (you can find it [here](https://www.kaggle.com/d4sein/league-of-legends-patch-109). The insights I got are:
- Champions (pick - ban - win - lose) rates
- Champion Duos and synergies
- Item (pick - win) rates
- Item synergies (with champions and classes)
- Item suggestion (by champion)
- Also my friend did Spark streaming code that uses a heuristic that can decide which team will win in a live match, and which player of the opposite team is a threat.

Here is the [GitHub repo](https://github.com/moustafa-7/League-of-legends_Bigdata) you will find all figures and top 10 outputs in the readme section.

If you like it, please leave a star on GitHub, and If you have any suggestion for ideas or future collaboration you can reply here or pm me :)

PS: The idea originated from my TA who was a great help and mentor and also the data wasn't that polished and I never played LOL (haha) so excuse any mistakes or something that doesn't make sense (maybe I got the game logic wrong).

Take care \
Thank you",13,0,moustafa-7,2020-12-29 10:18:49,https://www.reddit.com/r/bigdata/comments/kmbsrn/insights_about_lol_using_big_data_analysis_code/,0,bigdata
k432mi,Automatic configuration tuning for Spark,"If you have been writing Spark applications for a long time, you couldn't help but come across the tuning of its configuration parameters. Now you can do it automatically with a tool that optimizes the cluster resources for you.

http://spark-configuration.luminousmen.com/",13,3,luminoumen,2020-11-30 19:24:34,https://www.reddit.com/r/bigdata/comments/k432mi/automatic_configuration_tuning_for_spark/,0,bigdata
ixduwo,Start from Excel to Machine Learning in My Career,It took me 5 years to go from an entry level position to a senior position where I do a machine learning project every quarter. I wanted to share my experience with you all and transparency about my salary in Silicon Valley. Maybe I can encourage some people to advance their career. [https://www.youtube.com/watch?v=isVHh5DV07I](https://www.youtube.com/watch?v=isVHh5DV07I),13,11,Q26239951,2020-09-22 01:25:50,https://www.reddit.com/r/bigdata/comments/ixduwo/start_from_excel_to_machine_learning_in_my_career/,0,bigdata
isjoir,[VIDEO] - Building an Event-Driven Application with Flink,,13,0,Marksfik,2020-09-14 12:13:00,https://youtu.be/t663p-qHijE,0,bigdata
i7dm1n,Need some help analyzing keyword data,"I’ve been working in SEO and online marketing for almost a decade. I’ve had one consistent challenge with analyzing potential keywords for clients. 

There are so many moving parts to keyword research that sometimes it feels impossible to actually compare a keyword’s value to another’s value.

I'm trying to create a formula that creates a metric taking all of the most relevant keyword data and combine them into one, easy to understand metric. 

I’ve been messing with a way to make a formula for the last 3 or 4 years. It’s served me (and my clients) really well for the past few years. 

But now I’m wondering if there are ways to improve it.

### What are the Valuable Keyword Metrics?

The most important data points for identifying the value of a keyword are:

* Searches per month
* Difficulty to Rank for a keyword
* How much the keyword is worth to businesses

My solution? The following formula in Google Sheets

Keyword Rating = (Keyword volume / keyword difficulty) \* CPC

* Searches per month = Keyword volume (AHREFs)
* Difficulty to Rank for keyword = Keyword difficulty (AHREFs)
* How much they keyword is worth to businesses = CPC (AHREFs)

Here’s what it looks like in Google Sheets:

https://preview.redd.it/sxensxp1l8g51.png?width=3088&format=png&auto=webp&s=c6b8cfb5aabe4a03b475571a54f13e496b59bb66

### Formula Rationale 

Search volume is only important if you can rank for the keyword. That’s why I divided keyword volume by keyword difficulty. 

As you can see in the screenshot, there’s definitely a correlation between keyword volume and Keyword Rating. But dividing by Difficulty definitely helps temper that.

I multiplied by CPC because I see it as a decent indicator of a keywords commercial value. If people (presumably competitors) are willing to spend $X on a keyword, then that’s the value the market has landed on for that keyword. Thus i think it’s a pretty good way to approximate that keyword’s value to a client. 

### Questions

* So what am I missing? Am I making any rookie math mistakes?
* Is there a better way to build this formula so the metric is more valuable? 
* Are there other metrics you’d include? If so, how would you put them into the formula?

Looking forward to hearing any thoughts or ideas from the community.",14,2,the_website_fixer,2020-08-10 21:05:04,https://www.reddit.com/r/bigdata/comments/i7dm1n/need_some_help_analyzing_keyword_data/,0,bigdata
i5vj3c,Apache Flink: The integration of Pandas into PyFlink,,12,0,Marksfik,2020-08-08 08:06:37,https://flink.apache.org/2020/08/04/pyflink-pandas-udf-support-flink.html,0,bigdata
i2s12a,Spark Performance Tuning & Best Practices,,14,0,Sparkbyexamples,2020-08-03 06:25:14,https://sparkbyexamples.com/spark/spark-performance-tuning/,0,bigdata
gx1jdn,Mathematics behind Big Data,"Hi. I am trying to work on a short essay about the role of mathematics in Big Data Analysis (exactly Big Data, not Data Science in general) . I could not find enough scientific sources that explain the math helps to handle that large-scale analyses. Any ideas where can I find the necessary info?",14,7,Staarrdustt,2020-06-05 09:45:58,https://www.reddit.com/r/bigdata/comments/gx1jdn/mathematics_behind_big_data/,0,bigdata
gueets,"How To Visualize Public Transport Using Kibana, Elasticserach, Logstash (Elastic Stack) and Kafka Streams on top of Docker",,15,0,mszymczyk,2020-06-01 05:09:21,https://medium.com/@zorteran/how-to-visualize-public-transport-using-kibana-elasticserach-logstash-elastic-stack-and-kafka-eabc6975255a,0,bigdata
fax92g,Why predictive maintenance is the next big thing in manufacturing,,14,6,TheTesseractAcademy,2020-02-28 16:27:26,https://thedatascientist.com/why-predictive-maintenance-is-the-next-big-thing-in-manufacturing/,0,bigdata
errvag,The big data maturity levels,,13,0,RealJon,2020-01-21 08:37:00,https://medium.com/@bratseth/the-big-data-maturity-levels-8b61875032cc?source=friends_link&sk=351dc04418bf220d1b76c6a25ff05721,0,bigdata
erao5d,Questions about big data & the duties of a data engineer.,"Hi everyone, 

I've been working in the big data field since 2016 as a data engineer. 

I started in the Hadoop ecosystem (Cloudera / Hortonworks distributions) and switched a couple years ago to AWS.

&#x200B;

I have a feeling that while many big data products keep getting updated, there is no actual innovation in the field, there is a sprouting of new products that do the same job of already well established projects and 99% of the times they don't make it past version 0.\*.

&#x200B;

In the meantime, I am also noticing that my day to day job has changed drastically:

back in 2016 I had to setup my cluster, model/ingest data on top of it and develop code (mainly Spark) to extract insights from data, basically porting prototypes developed by the data science team.

&#x200B;

Nowadays I feel like the job of the data engineer is becoming less and less necessary: 

* An always increasing number of data scientists learns Spark and can directly write code. An engineer could always write better code (from a SWE best practices and algorithmic performance point of view), but I am noticing that collaboration between the two professional figures is not appreciated as it was in the past. 
* In my personal experience scientists demand to work on the rawest possible data, taking from engineers also the task to perform data preparation/modelling.
* The infrastructure setup is now in charge of DevOps teams and the switch from on-premise machines to cloud services made configurations easier. 
* Many big data products are now a commodity that can be used by SWEs with no particular skillsets.

I feel like I am at a crossroads where I should choose between becoming a data scientist and becoming a devops. 

&#x200B;

These sensations seem to be confirmed by job listings too, I see data scientist positions sprouting and it becomes harder and harder to find a proper data engineering job (many of them are devops under the hood, or something completely unrelated).

&#x200B;

Does anybody of you feel the same? 

How did you adapt to change? Have you ""reinvented"" your figure of data engineer?",12,9,kigenere,2020-01-20 08:45:29,https://www.reddit.com/r/bigdata/comments/erao5d/questions_about_big_data_the_duties_of_a_data/,0,bigdata
ejjcy2,Fighting Overfitting in Deep Learning | ActiveWizards: data science and engineering lab,,13,0,techgig11,2020-01-03 18:21:28,https://activewizards.com/blog/fighting-overfitting-in-deep-learning/?utm_source=reddit&utm_medium=bigdata&utm_campaign=data_science,0,bigdata
eg61rw,ML – Introduction to Data in Machine Learning,,13,0,Big_Data_Path,2019-12-27 04:05:04,https://bigdatapath.wordpress.com/2019/12/27/ml-introduction-to-data-in-machine-learning/,0,bigdata
ea3s2f,Create your first sales dashboard in Apache Superset,,13,0,pknerd,2019-12-13 13:08:10,http://blog.adnansiddiqi.me/create-your-first-sales-dashboard-in-apache-superset/?utm_source=r_bigdata&utm_medium=reddit&utm_campaign=c_r_bigdata,0,bigdata
dy10hg,Glimpse into Spark 3.0 [Early Access],,13,0,None,2019-11-18 09:34:34,https://towardsdatascience.com/glimpse-into-spark-3-0-early-access-c1854327d6c,0,bigdata
dd5i1f,A data wiki to help you get started with understanding the complex world of data,,11,2,leenas9,2019-10-04 10:27:16,http://wiki.atlan.com,0,bigdata
das42c,Schema-on-Read vs Schema-on-Write,,12,0,luminoumen,2019-09-29 07:31:42,https://luminousmen.com/post/schema-on-read-vs-schema-on-write,0,bigdata
bq3ocr,How to visualize Graphframes in Apache Zeppelin,,14,0,mszymczyk,2019-05-18 12:51:36,https://www.mszymczyk.com/visualizing-graphframes-in-apache-zeppelin/,0,bigdata
bkhu7m,Different Types Of SQL Joins,,14,4,lokendra15,2019-05-04 05:15:37,https://www.technolush.com/blog/sql-joins,0,bigdata
bhunck,"5 Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",,12,1,None,2019-04-27 02:37:43,https://www.youtube.com/c/DiscoverArtificialIntelligence,0,bigdata
besvbn,"Big Data and Machine Learning with Nick Caldwell, CPO at Looker, former VP of Engineering at Reddit",,13,1,trentlapinski,2019-04-19 00:22:23,https://hackernoon.com/big-data-and-machine-learning-with-nick-caldwell-14ed702b1c64,0,bigdata
bcblc8,Kafka Alternative Pulsar Unifies Streaming and Queuing,,13,6,None,2019-04-12 09:37:54,https://thenewstack.io/kafka-alternative-pulsar-unifies-streaming-and-queuing/,0,bigdata
b4o6af,Customer churn prediction in telecom using machine learning and social networks analysis in big data platform,,13,4,Abdelrahimk,2019-03-23 20:59:12,https://rdcu.be/bsz9I,0,bigdata
am748d,"I created a diagram for constructing big data pipeline on AWS, does it look useful to you?",,12,10,codingrecipe,2019-02-01 21:37:51,https://coderecipe.ai/architectures/86530220,0,bigdata
al00yl,"Hackathon in Germany. February 14 -17th. Data Challenges from John Deere, SAP and Heidelberg Digital Unit. Free Hotel + Travel Expense + Beer&Pizza! 9k€ in prizes. Apply at hack-days.de",,12,4,None,2019-01-29 13:32:12,https://i.redd.it/3glvv2mu6dd21.jpg,0,bigdata
a3z78i,The Importance of Supply Chain Analytics!,,14,0,indiumsoftware18,2018-12-07 11:54:20,https://www.indiumsoftware.com/blog/the-importance-of-supply-chain-analytics/,0,bigdata
9wox5u,Building A Data Lake Platform In The Cloud At Upsolver (Interview),,13,0,blarghmatey,2018-11-13 13:10:52,https://www.dataengineeringpodcast.com/upsolver-with-yoni-iny-episode-56/,0,bigdata
967njd,Does anybody remember the name of the project that offers a stream of machine readable categorized global news?,"I heard about it in a talk, but I just can't find it again.",13,6,None,2018-08-10 14:58:49,https://www.reddit.com/r/bigdata/comments/967njd/does_anybody_remember_the_name_of_the_project/,0,bigdata
8x9hyq,Scaling Spark made simple on Kubernetes,,12,2,matyix_,2018-07-09 07:55:59,https://banzaicloud.com/blog/scaling-spark-k8s/,0,bigdata
8w40xy,[question] Apache Nifi vs ESB like Mulesoft,"For a project at my workplace, we are looking into some ETL like process where we consume data from some SaaS app, do some data transformation, and push it to another datastore. Its not really big data as of now but may grow as our customer base grows.

We were looking into various options and it seems Nifi will do our job but we also came across ESB solutions like mulesoft. I am a bit confused where exactly is the different between a product like Nifi and and ESB. They both seem to almost do the same thing. Ofcourse, Mulesoft is pretty pricy but leaving that aside, what exactly is the core difference?

Mule and nifi both seem to have a easy to use visual way of programming through their UI which can be extended by writing code.",12,6,smartfinances,2018-07-04 18:51:03,https://www.reddit.com/r/bigdata/comments/8w40xy/question_apache_nifi_vs_esb_like_mulesoft/,0,bigdata
8uh9b3,"1.1 Billion Taxi Rides with SQLite, Parquet & HDFS",,13,0,marklit,2018-06-28 07:54:52,http://tech.marksblogg.com/billion-nyc-taxi-rides-sqlite-parquet-hdfs.html,0,bigdata
8prfpq,Bistro: a radically new approach to data processing (alternative to MapReduce),,13,3,asavinov,2018-06-09 08:21:26,https://github.com/asavinov/bistro/blob/master/core,0,bigdata
8kb845,Benefits of Sentiment Analysis for Businesses | Analytics Insight,,13,0,analyticsinsight,2018-05-18 06:58:00,https://www.analyticsinsight.net/benefits-of-sentiment-analysis-for-businesses/,0,bigdata
89dkjh,Announcing MapD Cloud: self-service GPU-accelerated analytics,,13,0,tmostak,2018-04-03 13:10:02,https://www.mapd.com/blog/announcing-mapd-cloud/,0,bigdata
84r4ae,New Data Science Podcast,"Hey everyone 👋, I am thrilled to announce the official launch of my new podcast, Data Journeys!

Data Journeys is a podcast for aspiring Data Scientists by AJ Goldstein, where he interviews world-class Data Scientists about their learning journeys. The focus is on how they’ve bridged the gap between acquiring technical skills and creating real-world impact. In each episode, the goal is to equip up-and-comers with the strategies, tactics, and tools that the best in the world have used to get to where they are today.

You can listen or subscribe to the show via links to iTunes, Soundcloud, Google Play Music, and more at: https://www.ajgoldstein.com/podcast/",13,4,ajva1996,2018-03-15 23:40:06,https://www.reddit.com/r/bigdata/comments/84r4ae/new_data_science_podcast/,0,bigdata
81rxeq,Druid.io in practice,I'm curious to hear from those who have experience running Druid in production in regards to maintainability and failure modes. How often you had to deal with historical/broker/coordinator nodes failing and how that impacts their overall pipeline. Thanks!,13,6,pdeyhim,2018-03-03 18:24:52,https://www.reddit.com/r/bigdata/comments/81rxeq/druidio_in_practice/,0,bigdata
7ub0l1,Top 15 Scala Libraries for Data Science in 2018,,13,0,viktoriia_shulga,2018-01-31 16:17:27,https://www.medium.com/activewizards-machine-learning-company/top-15-scala-libraries-for-data-science-in-2018-4b2cb5c5367e/?utm_source=reddit&utm_medium=bigdata&utm_campaign=top_scala_libraries,0,bigdata
7k2qqt,"Hadoop 3.0 Ships, But What Does the Roadmap Reveal?",,12,3,None,2017-12-15 21:25:13,https://www.datanami.com/2017/12/15/hadoop-3-0-ships-roadmap-reveal/,0,bigdata
7dj2iv,Top 20 Data Science Blogs And Websites For Data Scientists,,11,0,cevizligizem,2017-11-17 06:58:04,https://www.exastax.com/big-data/top-20-data-science-blogs-websites/,0,bigdata
79vrxk,System for performing 'tensor algebra' offers 100-fold speedups over previous software packages,,12,0,eberkut,2017-10-31 13:38:08,https://phys.org/news/2017-10-tensor-algebra-fold-speedups-previous.html,0,bigdata
6y6vfa,Introduction to Blockchains & What It Means to Big Data,,14,0,NoahData,2017-09-05 09:26:35,http://www.noahdatatech.com/introduction-blockchains-means-big-data/,0,bigdata
6uijis,A Judge Just Ordered LinkedIn to Allow Scraping,,14,2,zemcunha,2017-08-18 15:24:54,http://blog.webhose.io/2017/08/17/a-judge-just-ordered-linkedin-to-allow-scraping-heres-why/,0,bigdata
6to21r,Firing on All Cylinders: The 2017 Big Data Landscape,,14,0,gregory_k,2017-08-14 17:41:05,http://mattturck.com/bigdata2017/,0,bigdata
6m9kun,kafka vs Kinesis,,13,3,izpo,2017-07-09 19:59:08,https://blog.insightdatascience.com/ingestion-comparison-kafka-vs-kinesis-4c7f5193a7cd,0,bigdata
6l3ljv,"Kafka Elasticsearch Connect: From 9,071 to 1 Line of Code",,13,0,rjurney,2017-07-03 23:55:39,https://blog.datasyndrome.com/kafka-elasticsearch-connect-from-9-071-to-1-line-of-code-33ca4a6e0f29,0,bigdata
6itevf,Google releases new TensorFlow Object Detection API,,11,0,lalypopa123,2017-06-22 13:15:54,https://techcrunch.com/2017/06/16/object-detection-api/,0,bigdata
6by5jx,The poor man's data pipeline,,13,0,None,2017-05-18 18:37:13,http://daynejones.com/posts/poor-mans-data-pipeline,0,bigdata
677zk2,How Airlines are Using Big Data,,12,0,yagmurnur,2017-04-24 09:21:20,https://www.exastax.com/big-data/how-airlines-are-using-big-data/,0,bigdata
617vsr,Intro to Data Analytics using Cassandra and Spark,,13,0,davibo,2017-03-24 09:09:08,https://opencredo.com/data-analytics-using-cassandra-and-spark/,0,bigdata
5ydglw,"Healthcare is Drowning in Data, Thirst For Knowledge",,13,4,xenonstack,2017-03-09 06:08:31,https://www.xenonstack.com/blog/healthcare-is-drowning-in-data-thirst-for-knowledge,0,bigdata
5ukk1q,Comparision between Hadoop Apache Spark and Apache Flink,,13,3,anishp1313,2017-02-17 05:50:36,http://data-flair.training/blogs/feature-comparison-between-apache-hadoop-vs-spark-vs-flink/,0,bigdata
5ossg4,Learn real-time processing with a taxi public data stream and Google Cloud Dataflow codelab,,13,0,fhoffa,2017-01-18 22:55:52,https://cloud.google.com/blog/big-data/2017/01/learn-real-time-processing-with-a-new-public-data-stream-and-google-cloud-dataflow-codelab,0,bigdata
5ok5no,CCA Spark and Hadoop Developer - Free course,,13,1,itversity,2017-01-17 19:33:02,http://www.itversity.com/courses/cca-spark-and-hadoop-developer-certification/,0,bigdata
5kq5y0,Apache Spark + NFS,"Hi all. I am wondering how Apache Spark works on top of a network-mounted file system (NFS) as the *only* storage mechanism (i.e., HDFS is NOT involved). Do you have any idea about the performance? In particular, do you know how the parallelism works? Are the workers accessing the same file/different files in parallel? 
Thanks in advance for your help.",13,9,steccami,2016-12-28 13:11:55,https://www.reddit.com/r/bigdata/comments/5kq5y0/apache_spark_nfs/,0,bigdata
5k41ar,Is a postbacc worth it to get on the right path towards a data scientist?,"Hello! I posted this in a different subreddit, received some good answers, but figured this might be a better spot for it. So here it goes...

Here's my story: I received my bachelors 3 years ago in finance and graduated debt free. I'm currently working in banking as a credit analyst, but it's not very intellectually stimulating. In school, I almost switched to cs or statistics as those two subjects interested me more, but I was already so close to graduating and there was no way I could switch. Now living in a little bit of regret, read a few Michael Lewis books, and interested in how to get into data science/analytics. 

I would apply to a masters program, but i didnt have to take more than the basic stats and cs course in undergrad, so i dont really feel prepared to jump into that. I'm thinking of getting a postbacc to both boost my GPA and to get a better foundation before leaping in (either in stats, cs, or mathematical economics. I'm not sure which would prep me the most and I'm not sure if a full blown cs program is necessary or if I can just take a few courses that are most applicable). I'm also starting to take a couple MOOC courses in stats (to brush up), python, and data analytics.  

So here's my question: Am I going down the right path or am I just kidding myself/wasting my time. I don't want to put myself in debt for a postbacc if I still can't get into a decent masters/PhD program. I want to become a data scientist, so I'm trying to figure out a way to be the ""whole package"" in my academic path.

Are there people out there that have taken this route? Either going back for a post bacc or from a nontraditional beginning?",13,9,Lafee17,2016-12-24 17:28:13,https://www.reddit.com/r/bigdata/comments/5k41ar/is_a_postbacc_worth_it_to_get_on_the_right_path/,0,bigdata
5ht3yf,Has anyone here done big data consulting?,"Hi guys,

Recently, I've found myself wanting to take more ownership of my career path, and have been looking into switching into consulting. However, unlike most of my consultant friends, I specialize in big data (primarily architecture) rather than web design/ios.

Has anyone else made this transition?

If so, I have a few questions I'm seeking answers on:

1. Where have you gone for gigs? How do you establish a network?
2. Have you noticed a significant change in your compensation compared to when you worked a 9-5?
3. What type of jobs have you taken on? Are you primarily architecting systems from the ground up or fixing already existing systems?
4. Are there any consulting firms in particular that I should look to apply to? (I've considered cloudera/hortonworks, but those seem more specific to maintaining their products than building new things. My ideal situation would be more along the lines of ""We need x feature, build it before x date"")

Thank you all so much for your help!",12,3,datshitberacyst,2016-12-11 23:16:16,https://www.reddit.com/r/bigdata/comments/5ht3yf/has_anyone_here_done_big_data_consulting/,0,bigdata
5f4ehz,"Alenka: An Open Source, GPU-Driven Database",,12,0,marklit,2016-11-27 08:23:11,http://tech.marksblogg.com/alenka-open-source-gpu-database.html,0,bigdata
5davpr,IT Training - What Happens Then?,,15,0,Juliyt,2016-11-16 18:40:50,https://www.linkedin.com/pulse/training-what-happens-erik-cox?trk=prof-post,0,bigdata
5c7nxe,Trump's win was a coup for a tiny data firm that helped the campaign target ads based on a psychological approach,,12,3,ETLData,2016-11-10 12:53:12,http://www.wsj.com/articles/inside-donald-trumps-data-analytics-team-on-election-night-1478725225,0,bigdata
5c3b2a,I'm still not sure I understand: why use Docker?,"I guess I'm missing the sudden explosion in interest regarding Docker. I've browsed through O'Reilly books, but I still don't quite see how applicable this new tool is. 

Any thoughts?",12,18,Zeekawla99ii,2016-11-09 19:56:35,https://www.reddit.com/r/bigdata/comments/5c3b2a/im_still_not_sure_i_understand_why_use_docker/,0,bigdata
56sf48,Complex Data Pipelines with Spotify's Luigi,,13,0,None,2016-10-10 15:09:35,https://www.promptworks.com/blog/configuring-complex-luigi-pipelines?utm_source=bigdata&utm_campaign=Luigi&utm_medium=reddit,0,bigdata
54qsz8,"Question and Answers with the Apache Beam Team (Google, Apache, Spotify, Cisco, Shopkick, Talend, Intel)",,15,0,fhoffa,2016-09-27 14:25:38,http://www.jesse-anderson.com/2016/07/question-and-answers-with-the-apache-beam-team/,0,bigdata
51w6fd,Facebook use case with Apache Spark at scale,,12,0,gpsis13,2016-09-09 07:19:20,https://code.facebook.com/posts/1671373793181703,0,bigdata
4xumly,"Big Data Careers: Anyone looking to pursue a successful career in DBA, give this a read",,12,3,charmedbydata,2016-08-15 17:11:41,https://www.pluralsight.com/resource-center/infographics/database-admins--a-guide-to-becoming-a-database,0,bigdata
4xgead,"BIG DATA , learning | Practice with sample code.",,11,0,anandsiwan,2016-08-12 23:06:38,http://bigishere.wordpress.com,0,bigdata
4vkglp,Some of the top big data analytics and data science blogs that might interest you,,15,0,datameer,2016-08-01 02:53:26,http://analyticscosm.com/top-big-data-analytics-blogs-by-traffic/,0,bigdata
4t5iib,Next Generation Data Architecture,,12,15,dodgyfox,2016-07-16 17:26:31,https://medium.com/@KevinSchmidtBiz/next-generation-data-architecture-31f03141a19a#.60vr8h1h2,0,bigdata
4ri0a0,Where can I practice Hadoop and it's ecosystem ?,"I recently took a course on Hadoop and it's ecosystem where I learnt the basics of Pig,Hive,Flume,Oozie.Sqoop,HBase and Hadoop. I need a place where I can practice these with questions and data sets that are provided by the website. I tried kaggle, but it has more of ML than hadoop etc. 
Moreover, I am confused whether to continue using MapReduce and Hadoop or to start learning Spark. Any help would be greatly appreciated. Thanks in advance !

EDIT: I have all the software I need (Hadoop and its ecosystem) but I don't know what to do with it. It's like knowing a language but not knowing what code to write using that language. ",12,6,None,2016-07-06 11:17:57,https://www.reddit.com/r/bigdata/comments/4ri0a0/where_can_i_practice_hadoop_and_its_ecosystem/,0,bigdata
4r6d0a,Big Data's Future Is In Predictive Analytics | Articles | Analytics,,11,0,AC_London,2016-07-04 08:53:04,https://channels.theinnovationenterprise.com/articles/big-data-s-future-is-in-predictive-analytics?utm_source=Reddit&utm_medium=Post&utm_term=Article&utm_content=Big+Data+PA&utm_campaign=PA+CH+2016,0,bigdata
4gq0jb,Big Data Tamps Down HIV Outbreaks,,12,0,Chipdoc,2016-04-27 18:59:15,http://spectrum.ieee.org/the-human-os/biomedical/diagnostics/big-data-tamps-down-hiv-outbreaks,0,bigdata
42l1ye,"Thanks to big data, US parties know all about voters",,12,0,stormforce7916,2016-01-25 12:17:46,http://phys.org/news/2016-01-big-parties-voters.html,0,bigdata
3wxph4,Insight Big Data Engineering Ecosystem: An Interactive Map,,15,1,steccami,2015-12-15 13:22:47,http://insightdataengineering.com/blog/pipeline_map.html,0,bigdata
3uzigp,"Free - Learn Hadoop and Big Data by Building 10 Projects, Over 10 Hours of videos, Free For Limited Time Period",,11,4,SamAndreas,2015-12-01 11:23:40,https://www.eduonix.com/courses/Software-Development/Learn-Hadoop-and-Big-Data-by-Building-Projects?coupon_code=Redditfree,0,bigdata
3uw9yd,What we learned from moving from Mixpanel to Amazon Redshift,,15,3,itamarwe,2015-11-30 20:06:57,https://www.alooma.com/blog/custom-analytics-amazon-redshift,0,bigdata
3r3625,MADlib: Big Data Machine Learning in SQL for Data Scientists,,14,0,based2,2015-11-01 15:29:24,http://madlib.incubator.apache.org/,0,bigdata
3qlg0v,"Spark 1.6 (Dec 2015): ""Datasets"" best of RDDs and Dataframes [OC]",,13,0,michaelmalak,2015-10-28 18:41:06,http://datascienceassn.org/content/spark-16-datasets-best-rdds-and-dataframes,0,bigdata
3qf9e1,Big Data Are Reducing Homicides in Cities across the Americas,,13,1,properown,2015-10-27 14:33:20,http://www.scientificamerican.com/article/big-data-are-reducing-homicides-in-cities-across-the-americas,0,bigdata
3m7f4p,Scale it to Billions - What They Don’t Tell you in the Cassandra README (x-post /r/cassandra),,13,1,ncooprider,2015-09-24 14:57:45,http://blog.threatstack.com/scaling-cassandra-lessons-learned,0,bigdata
3git0a,Google Dataflow Whitepaper - Unified Stream and Batch processing model,,13,1,thetinot,2015-08-10 22:53:21,http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf,0,bigdata
3d9l0m,DataCamp Gets $1M Seed Round To Develop Data Science Learning Platform,,13,2,martijnT,2015-07-14 15:54:35,http://techcrunch.com/2015/07/14/datacamp-gets-1m-seed-round-to-develop-data-science-learning-platform/,0,bigdata
3a2km5,Run a Spark Job on AWS EMR with this cool example project,,13,0,dgleebits,2015-06-16 18:59:38,https://github.com/snowplow/spark-example-project,0,bigdata
33wug6,The end of Big Data: a reasonable Internet of Things,,12,5,Ursium,2015-04-26 11:44:56,https://medium.com/ursium-blog/the-end-of-big-data-a-reasonable-internet-of-things-ff78a4428ed7,0,bigdata
2yolb8,Nobody Likes Sponsored Talks at Strata,,13,1,kiyoto,2015-03-11 14:56:12,http://blog.treasuredata.com/blog/2015/03/10/sponsored-talks-at-strata/,0,bigdata
2srjts,What is one trend that will influence big data analytics in the next 5 years?,,11,7,iKidA,2015-01-17 20:40:54,https://www.reddit.com/r/bigdata/comments/2srjts/what_is_one_trend_that_will_influence_big_data/,0,bigdata
2sbah0,Learn the Skills and Tools to be a Data Scientist,,12,0,calovell,2015-01-13 19:39:45,https://allclasses.com/Online/The-Data-Scientist-Skillset/,0,bigdata
2pvxp3,Spark implementation of the Google Correlate algorithm to quickly find highly correlated vectors in huge datasets,,15,1,based2,2014-12-20 14:29:01,https://github.com/Sotera/correlation-approximation,0,bigdata
2n4xmn,Tutorial: Apache Spark on GPU(s),,14,0,iamtrask,2014-11-23 03:23:38,http://iamtrask.github.io/2014/11/22/spark-gpu/#,0,bigdata
2lvv41,New Infographic: How to become a data scientist in 8 steps,,14,4,martijnT,2014-11-10 19:06:26,http://blog.datacamp.com/how-to-become-a-data-scientist-in-8-easy-steps-the-infographic/,0,bigdata
2jcqu9,"A curated list of big data frameworks, resources and tools",,14,3,based2,2014-10-15 20:53:47,http://blog.andreamostosi.name/big-data/,0,bigdata
2gt0m4,Big Data Modeling Methods Explained Simply,,15,0,eotpm,2014-09-18 22:32:33,https://www.linkedin.com/pulse/article/20140918162814-111366377-let-s-get-nerdy-data-analytics-for-business-leaders-explained?trk=prof-post,0,bigdata
2fty7w,Ex-Googler Shares His Big-Data Secrets With the Masses | Enterprise | WIRED,,14,3,mhausenblas,2014-09-08 18:47:56,http://www.wired.com/2014/09/metanautix/,0,bigdata
2dq6pq,Big Data Requires a New Kind of Expert: The Econinformatrician,,13,0,wowamit,2014-08-16 15:37:08,http://bigdataeconometrics.wordpress.com/2014/08/10/big-data-requires-a-new-kind-of-expert-the-econinformatrician/,0,bigdata
1uawlb,The Log: What every software engineer should know about real-time data's unifying abstraction,,13,0,trakov,2014-01-03 09:51:26,http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying,0,bigdata
1j1bdn,How to use HBase & Hadoop Clustered,,11,1,Rockytriton,2013-07-25 16:36:32,http://www.dreamsyssoft.com/blog/blog.php?/archives/5-How-to-use-HBase-Hadoop-Clustered.html#extended,0,bigdata
1if0z8,Big data book for my manager at work,"Today at work I caught my manager toting around a ""Big Data for Dummies"" book. He's a good manager, and used to be a pretty good Lisp coder decades ago, but nowadays he's involved in coming up with creative proposal ideas in a government R&D sort of environment, focused on areas of data analytics. So what he needs is concepts, vision, applications, forward looking analysis, that sort of information. He's not going to be designing architectures or coding Hadoop-based solutions or anything, though he probably wants to at least recognize the 'buzz words' with enough knowledge to not be actively dangerous.

Any suggestions on big data books (or websites or other resources) that would fit this sort of usage?",14,4,rabidstoat,2013-07-16 15:12:32,https://www.reddit.com/r/bigdata/comments/1if0z8/big_data_book_for_my_manager_at_work/,0,bigdata
18t2ga,Speeding up a Pig/HBase MapReduce job by 15X,,13,6,mikopp,2013-02-19 07:26:29,http://apmblog.compuware.com/2013/02/19/speeding-up-a-pighbase-mapreduce-job-by-a-factor-of-15/,0,bigdata
169mn7l,"Google launches BigQuery Studio, a new way to work with data | TechCrunch",,12,2,GoogleHearMyPlea,2023-09-04 08:43:38,https://techcrunch.com/2023/08/29/google-launches-bigquery-studio-a-new-way-to-work-with-data/,0,bigdata
zd1sc9,HDFS Architecture — Data Replication,,13,0,ranjeettechnincal,2022-12-05 08:58:37,https://medium.com/@thetechnogeek/hdfs-architecture-data-replication-69e20f0fb775,0,bigdata
wqh8no,How are Financial Institutions Navigating their Data-Driven Business Operations?,,12,1,Raj_9898,2022-08-17 06:08:03,https://uk.sganalytics.com/blog/why-the-road-to-profitability-for-financial-institutions-is-data-driven/,0,bigdata
v8gs0p,What are your hottest dbt repositories in 2022 so far? Here are mine!,"Here are my top5! 🚀

&#x200B;

https://preview.redd.it/5knzubyqjl491.png?width=498&format=png&auto=webp&s=92ced0c102a7477eb4914c940e8e8ce7894cdc3d

\- ⚡️ [Lightdash:](https://github.com/lightdash/lightdash) Lightdash converts dbt models and makes it possible to define and easily visualize additional metrics via a visual interface.

\- ⏎ [re\_data:](https://github.com/re-data/re-data) Re-Data is an abstraction layer that helps users monitor dbt projects and their underlying data. For example, you get alerts when a test failed or a data anomaly occurs in a dbt project.

\- 📗 [evidence:](https://github.com/evidence-dev/evidence) Evidence is another tool for lightweight BI reporting. With Evidence you can build simple reports in ""medium style"" using SQL queries and Markdown.

\- 🧱 [Kuwala](https://github.com/kuwala-io/kuwala): With Kuwala, a BI analyst can intuitively build advanced data workflows using a drag-drop interface on top of the modern data stack without coding. Behind the Scenes, the dbt models are generated so that a more experienced engineer can customize the pipelines at any time.

\- 🐍 [fal ai:](https://github.com/fal-ai/fal) Fal helps to run Python scripts directly from the dbt project. For example you can load dbt models directly into the Python context which helps to apply Data Science libaries like SKlearn and Prophet in the dbt models.",11,3,kuwala-io,2022-06-09 13:23:38,https://www.reddit.com/r/bigdata/comments/v8gs0p/what_are_your_hottest_dbt_repositories_in_2022_so/,0,bigdata
v80z39,"A peek at what Airbnb, Meta, Uber, Apple, T-Mobile, Pinterest, Autodesk, Capital One, and others are doing with their stacks",,12,0,har2018vey,2022-06-08 21:52:17,/r/dataengineering/comments/v80sfe/a_peek_at_what_airbnb_meta_uber_apple_tmobile/,0,bigdata
uv3q25,Building a Data Platform: 7 Signs It’s Time to Invest in Data Quality,,12,0,Big_Data_Path,2022-05-22 04:44:36,https://bigdatapath.wordpress.com/2022/05/22/building-a-data-platform-7-signs-its-time-to-invest-in-data-quality/,0,bigdata
uji371,Install ClickHouse Faster,,11,0,marklit,2022-05-06 07:33:06,https://tech.marksblogg.com/install-clickhouse-faster.html,0,bigdata
tpcgdt,👉 Impressed With AlphaFold? Checkout This Protein Structure Prediction Model (FastFold) That Reduces AlphaFold’s Training Time From 11 Days To 67 Hours,"DeepMind released AlphaFold 2 last year, which made headlines for its incredible accuracy in protein structure prediction. The success of AlphaFold demonstrated that deep neural networks might be used to solve challenging and critical structural biology problems.

[FastFold](https://arxiv.org/pdf/2203.00854v1.pdf) is a highly effective protein structure prediction model formulation for training and inference developed by a group of researchers from the National University of Singapore. Although AlphaFold 2 is a game-changer in protein structure prediction, training and inference remain time-consuming and costly. This is something that the study team is concerned about.

[Continue Reading This Article Here](https://www.marktechpost.com/2022/03/26/impressed-with-alphafold-checkout-this-protein-structure-prediction-model-fastfold-that-reduces-alphafolds-training-time-from-11-days-to-67-hours/)

Paper: https://arxiv.org/pdf/2203.00854v1.pdf

Github: https://github.com/hpcaitech/FastFold",12,1,No_Coffee_4638,2022-03-27 04:28:50,https://www.reddit.com/r/bigdata/comments/tpcgdt/impressed_with_alphafold_checkout_this_protein/,0,bigdata
r7ee7k,Why You Should Become A Data Engineer And Not A Data Scientist - Picking The Right Data Career,,13,1,SeattleDataGuy,2021-12-02 18:54:34,https://www.youtube.com/watch?v=KgdWvtppH50,0,bigdata
qabgse,Day In The Life Of A Data Engineer - What Do Data Engineers Do?,,12,0,SeattleDataGuy,2021-10-18 00:34:03,https://www.youtube.com/watch?v=OfN6Xm-msGs&t=1s,0,bigdata
pts5h9,The Global Big Data Analytics Market is Expected to Reach US$420.98 Billion by 2027,,13,3,Analyticsinsight01,2021-09-23 10:17:48,https://www.analyticsinsight.net/the-global-big-data-analytics-market-is-expected-to-reach-us420-98-billion-by-2027/,0,bigdata
oa1oc1,Online Courses I used To Break Into Data Engineering,,11,0,nonkeymn,2021-06-29 05:17:02,https://www.youtube.com/watch?v=lVj0RlSxTXk,0,bigdata
o6wfwo,How Apache Flink processes an astonishing 7TB/sec during the 2020 Double 11 Shopping Festival,,12,0,Marksfik,2021-06-24 08:08:00,https://www.ververica.com/blog/apache-flinks-stream-batch-unification-powers-alibabas-11.11-in-2020,0,bigdata
n8ckyn,The Business Analytics Process 👔," In this article, we will cover what the Business Analytics process is, the flow of activities to be done in a specific order to achieve our goal. 

While traditional statistical applications focus on relatively small data sets, Data Science involves vast amounts of data, typically what we call Big Data. When we talk about Data Science, we are generally talking about applying analysis techniques to large data sets that require an application from Computer Science to the Storage and Processing of large datasets.

If we’re going to work with a petabyte of data, it will require a different infrastructure than a Gigabyte. At this point, the Data Scientist must know a little about Computer Science and eventually about programming to extract as much of the equipment and computational resources as possible.

Based on this, Business Analytics should be seen as a process, that is, a flow of activities and operations to solve particular problems. For each of these steps, we have techniques, procedures, tools that can be used and used.

A company that wants to succeed from the Business Analytics application should understand this and, above all, foster the construction of this process as if it were a production line.

Of course, according to the data and the type of business problem, the Data Scientist will use different techniques at each stage, but if we know how all this works, the process is much easier to implement to achieve the desired result.

## The Business Analytics process involves interrelated steps:

**1. Problem Definition**

The starting point is the precise definition of the business problem to be solved, which will guide everything that a Data Scientist will do. The lack of clarity in the description of the business problem is the certainty of an unsuccessful Business Analytics process. It doesn’t matter if we use the best technique on the planet, the best procedure, or high precision tools if the problem is not well defined — waste of time and resources. It is necessary to delve into the business area, rounds of conversations, meetings, thoughts about the problem.

With the problem well defined, we have to think about storing data efficiently and the steps of data pre-processing because all of this is critical to the success of the analysis. That’s where the big data issue comes in; if the data set is large, we won’t be able to process all of this on a single machine, and you need to use a cluster of computers that will behave as if they were just one. In this way, we can use all the memory and processing capacity of this set of machines.

Know how to apply pre-processing ideas according to the problem and the set of data that we have at hand. In this step, we have the Data Engineer taking care of the storage infrastructure and the Data Scientist working on the pre-processing steps.

**2. Select Variables**

You must select the appropriate response variables and decide on the number of variables that we should investigate. When we collect data from the source to solve a problem, not all variables will likely be relevant.

Therefore, we have to apply some techniques to choose the best variables among the available ones to pass on. At this stage, the Data Scientist is required; if we do not make the correct choice of variables, the future steps will be compromised.

**3. Explore data**

The data needs to be tracked for outliers, and missing values need to be addressed (with missing values omitted or appropriately imputed using one of several available methods).

Therefore, we define the problem, collect the data, store the data, apply some initial pre-processing, tabulate, identify the variables most relevant to the problem. Now we have to identify values that out of the pattern, values that run away from the average.

For some problems, we want to identify the outlier. For some other type of problem, we need to remove it by influencing the mean of the data; influencing the average will influence the predictive model.

The same fits the missing values; maybe someone has not filled in some attribute at the source. We are without this record that may be important for our analysis — a decision needs to be made: remove the missing records; apply an imputation technique with some arbitrary value. Therefore, once again, we have processes, procedures, and tools for this step.

**4. Exploratory Analysis**

Before applying predictive models and sophisticated methods, the data needs to be visualized and summarized — This step is very neglected. It is an opportunity to visually identify patterns or problems that we are unable to locate during coding.

Chart building is part of the analysis process. Visualization helps us understand how data is organized. With exploratory analysis, we can know the data better, map any problems, and make better decisions.

**5. Describe Data**

The data summary involves typical summary statistics such as mean, percentile, median, standard deviation, correlation, and more advanced outlines such as critical components. By identifying the mean and the standard deviation, we can get a general idea of how the data is organized, and we can make better decisions during the process.

6. Predictive Modeling

Appropriate predictive modeling methods need to be applied. Depending on the problem, this involves linear regression, logistic regression, regression/classification trees, nearest neighbor methods, and clustering.

By now, we have the data practically ready and pre-processed. We will apply an algorithm to work with predictive modeling from the historical data after all the steps mentioned.

**7. Insights**

Finally, analytics insights need to be implemented. We need to act on the results. In this last step, the Data Scientist will work together with the Business area, explaining the insights obtained through the predictive model, explaining the conclusions, and proposing some alternatives.

The final business decision will be the Director, and it is up to the Data Scientist to provide support for the decision area with communication, storytelling, know how to transmit the information collected from the data, and with it be able to support the decision making of the company.

The business problem can be changed, and the dataset may be different, but the process is pretty much the same. When the problem and the data are changed, we should apply other techniques, tools, and procedures, but the process will invariably be the one we saw.

And there we have it. I hope you have found this useful. Thank you for reading. 🐼",12,1,Anello92,2021-05-09 11:45:04,https://www.reddit.com/r/bigdata/comments/n8ckyn/the_business_analytics_process/,0,bigdata
n3whex,Apache Flink 1.13.0 Release Announcement,,14,0,Marksfik,2021-05-03 13:47:14,https://flink.apache.org/news/2021/05/03/release-1.13.0.html,0,bigdata
mqa4c7,Apache Spark vs Ray,,11,5,mgalarny,2021-04-13 19:52:55,https://www.youtube.com/watch?v=yLKHHiT2nWw&list=PLmetp36hFxeyc9qO_5tPNMW-YD3tZfCFN&index=5,0,bigdata
mhz3nq,Data Discovery: The Future of Data Catalogs for Data Lakes,"What are some general best practices for make data lakes more discoverable? Lots of cool data discovery tools like Amundsen, Datahub, Select Star, etc. have popped up recently, but also looking for tips (like these) to make catalogs more distributed.

[https://towardsdatascience.com/data-discovery-the-future-of-data-catalogs-for-data-lakes-7b50e2e8cb28?source=friends\_link&sk=7aab6b6be181f5c92635e7c82eb43b46](https://towardsdatascience.com/data-discovery-the-future-of-data-catalogs-for-data-lakes-7b50e2e8cb28?source=friends_link&sk=7aab6b6be181f5c92635e7c82eb43b46)",13,1,Top-Substance2185,2021-04-01 16:31:22,https://www.reddit.com/r/bigdata/comments/mhz3nq/data_discovery_the_future_of_data_catalogs_for/,0,bigdata
l44r47,Solving Complex SQL Interview Questions [How to organize lengthy code so...,,12,0,boyzone87seo,2021-01-24 18:21:14,https://youtube.com/watch?v=vLjAG9eXkcU&feature=share,0,bigdata
j6rkgb,Explore over 74M companies from the whole Europe and beyond,,12,0,coinval_co,2020-10-07 14:13:04,https://www.hithorizons.com,0,bigdata
iorwzm,An interview with Martin Traverso about how the Presto distributed SQL engine reduces complexity and increases flexibility for analytical workloads across heterogeneous data sources at scale.,,11,0,blarghmatey,2020-09-08 11:32:37,https://www.dataengineeringpodcast.com/presto-distributed-sql-episode-149/,0,bigdata
ighgp4,An interview about how LinkedIn designed their metadata management platform and how it is being used to power data discovery and integration.,,13,0,blarghmatey,2020-08-25 18:03:12,https://www.dataengineeringpodcast.com/datahub-metadata-management-episode-147/,0,bigdata
i3kcld,Data Isn’t Safe With American Companies Either,,12,0,Antwanian,2020-08-04 14:31:56,https://medium.com/@austimullins/data-isnt-safe-with-american-companies-either-a08a84180352,0,bigdata
hvdq22,How Very Real Bots Spread Very Fake Science News,,13,12,scottsteinberg,2020-07-21 19:13:46,https://youtu.be/L5gyarpvaYw,0,bigdata
gyh8mc,"DataOps: What, Why and How?",,12,2,analyticsinsight,2020-06-07 18:11:07,https://www.analyticsinsight.net/dataops-what-why-and-how/,0,bigdata
glhbet,Spark Partitions,,12,1,luminoumen,2020-05-17 15:31:02,https://luminousmen.com/post/spark-partitions,0,bigdata
gj1am1,Principles of lazy data documentation — and how to get your team onboard,"Great blog on how to make data documentations less painful. Curious as to how others have gained buy in from their team for data documentation testing?

https://blog.quiltdata.com/principles-of-lazy-data-documentation-and-how-to-get-your-team-onboard-bb674b78ea73",11,1,superconductiveKyle,2020-05-13 15:35:10,https://www.reddit.com/r/bigdata/comments/gj1am1/principles_of_lazy_data_documentation_and_how_to/,0,bigdata
fopxg1,"Data Teams Going ""Remote"" - Challenges, Learnings & Observations","Folks, how are you and your data teams impacted in the current situation? Has the ""remote"" transition been easy? While my team is working hard with IT/admin to resolve their access issues + tool/tech setup, I was wondering if you had any useful tips, challenges you faced or learnings you'd like to share? Especially the impact on intangibles like collaboration, productivity/agility, comms...",13,15,2121varuag,2020-03-25 13:05:01,https://www.reddit.com/r/bigdata/comments/fopxg1/data_teams_going_remote_challenges_learnings/,0,bigdata
fcrge1,What are the options for a free hadoop stack in 2020 (and years to come),"Hi everyone, I'm a newly-transformed data engineer and searching for hadoop installation recently.

I've heard that you can just install CDH or HDP and call it a day. But after digging around I've noticed the following facts (please correct me if wrong):

- Cloudera Express was the only free edition of CDH, and now [Cloudera has discontinued it for CDH 6.3.3 and above][cdh]. That means CDH as a binary distributable is not accessible for free anymore (except for trial use of course).
- As Hortonworks merged with Cloudera, HDP 3.1.5 and above is not freely accessible anymore. [The documentation basically says][hdp] that you have to be a Cloudera customer to access the binaries.

At this point, I'm a little bit reluctant to install CDH or HDP as knowing that I'm not going to getting any upgrades (except by paying).

So what are other options to install hadoop?

Surely I can install upstream components individually by hand. But that's really painful and not good for management in general.

[cdh]: https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/install_software_cm_wizard.html#id_abd_dtm_25
[hdp]: https://docs.cloudera.com/HDPDocuments/Ambari-2.7.5.0/bk_ambari-installation/content/access_ambari_paywall.html",14,20,naitree,2020-03-03 08:05:00,https://www.reddit.com/r/bigdata/comments/fcrge1/what_are_the_options_for_a_free_hadoop_stack_in/,0,bigdata
elaonj,A gentle Introduction to Big Data. Hope it helps the community !!,,13,1,ashishmg,2020-01-07 12:04:10,https://www.datacloudschool.com/2020/01/introduction-big-data-analytics.html,0,bigdata
eh3i00,8 Top Big Data Analytics Trends That Will Dominate 2020,,11,0,Big_Data_Path,2019-12-29 08:07:30,https://bigdatapath.wordpress.com/2019/12/29/8-top-big-data-analytics-trends-that-will-dominate-2020/,0,bigdata
efbr3d,Artificial intelligence vs Machine Learning vs Deep Learning,,12,7,Big_Data_Path,2019-12-25 04:07:18,https://bigdatapath.wordpress.com/2019/12/25/artificial-intelligence-vs-machine-learning-vs-deep-learning/,0,bigdata
e8n1qq,Spark SQL Date and Time Functions,,13,0,Sparkbyexamples,2019-12-10 06:38:11,https://sparkbyexamples.com/spark/spark-sql-date-and-time-functions/,0,bigdata
e1alps,Artificial Intelligence vs. Machine Learning vs. Data Mining 101 – What’s the Big Difference?,,13,0,Big_Data_Path,2019-11-25 04:15:36,https://bigdatapath.wordpress.com/2019/11/25/artificial-intelligence-vs-machine-learning-vs-data-mining-101-whats-the-big-difference/,0,bigdata
dffa16,"You are not Google, LinkedIn or Amazon",,12,5,TheTesseractAcademy,2019-10-09 10:47:09,https://thedatascientist.com/you-are-not-google-linkedin-or-amazon/,0,bigdata
c6cv78,Using AWK and R to parse 25tb,,14,1,CrankyBear,2019-06-27 23:46:53,https://livefreeordichotomize.com/2019/06/04/using_awk_and_r_to_parse_25tb/,0,bigdata
c5fyel,7 Fastest-Growing Job Roles In Data Science & How To Work Towards Them,,13,0,TheTesseractAcademy,2019-06-25 21:45:45,https://www.analyticsindiamag.com/7-fastest-growing-job-roles-in-data-science-how-to-work-towards-them/,0,bigdata
bov8b3,"Apache Flink: Flux capacitor, huh? Temporal Tables and Joins in Streaming SQL",,14,0,Marksfik,2019-05-15 08:19:14,https://flink.apache.org/2019/05/14/temporal-tables.html,0,bigdata
bokiiu,Optimisation vs Prediction (part 2): Optimisation examples,,13,0,TheTesseractAcademy,2019-05-14 15:39:28,https://www.youtube.com/watch?v=fvECPP_koqE&t=1s,0,bigdata
bjski7,IBM Data Science Professional Certificate on Coursera,"&#x200B;

https://reddit.com/link/bjski7/video/kepucrro1rv21/player

Data Science has been ranked as one of the hottest professions and the demand for data practitioners is booming. This [Professional Certificate from IBM](https://www.coursera.org/specializations/ibm-data-science-professional-certificate) is intended for anyone interested in developing skills and experience to pursue a career in Data Science or Machine Learning.  

This program consists of 9 courses providing you with latest job-ready skills and techniques covering a wide array of data science topics including: open source tools and libraries, methodologies, Python, databases, SQL, data visualization, data analysis, and machine learning. You will practice hands-on in the IBM Cloud using real data science tools and real-world data sets.  It is a myth that to become a data scientist you need a Ph.D. 

This Professional Certificate is suitable for anyone who has some computer skills and a passion for self-learning. No prior computer science or programming knowledge is necessary. We start small, re-enforce applied learning, and build up to more complex topics.  Upon successfully completing these courses you will have done several hands-on assignments and built a portfolio of data science projects to provide you with the confidence to plunge into an exciting profession in Data Science. In addition to earning a Professional Certificate from Coursera, you will also receive a digital Badge from IBM recognizing your proficiency in Data Science. 

[**IBM Data Science Professional Certificate on Coursera - Enroll Now & Get Ready for The Future**](https://www.coursera.org/specializations/ibm-data-science-professional-certificate)",11,5,skilluponline,2019-05-02 08:37:15,https://www.reddit.com/r/bigdata/comments/bjski7/ibm_data_science_professional_certificate_on/,0,bigdata
bekyfa,People Trust AI to be Better Leaders than Politicians - Toks Tech,,13,4,cryptokunbo,2019-04-18 12:29:12,https://www.toks.tech/people-trust-ai-to-be-better-leaders-than-politicians/,0,bigdata
b1lo79,Looking for Big data online class," 

Being an IT engineer, I am exploring the Big Data specialization and wishing to earn a certificate in that field to help me get a new job later.

I've been looking around and found these two specializations:

* [Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science) from **John Hopkins University**
* [Big Data Specialization](https://www.coursera.org/specializations/big-data) from **UC San Diego**

I was hoping someone with experience in the field to advise on which to pick as it will be a big investment in time (months) and effort.

Any other suggestion out of the list is welcome.

Thank you!",12,9,dEnissay,2019-03-15 23:20:24,https://www.reddit.com/r/bigdata/comments/b1lo79/looking_for_big_data_online_class/,0,bigdata
aphn2w,Free Webinar On Data Engineering And ETL Development With Python And SQL,,12,1,nonkeymn,2019-02-11 16:00:23,https://medium.com/swlh/free-webinar-on-etl-development-with-python-and-sql-e974f388051c,0,bigdata
aldthz,"Hive complex type processing via HQL macros (No lateral view, no java)","I wrote some hive udfs you can use with a macro to process complex types without lateral view or custom udfs.

I wrote about them here:
https://tech.trivago.com/2019/01/30/a-new-functional-approach-to-complex-types-in-apache-hive/

they are open sourced aswell. https://github.com/trivago/hive-lambda-sting

Let me know what you think. 

Thanks to trivago for offering me the time to do this",12,0,None,2019-01-30 15:34:47,https://www.reddit.com/r/bigdata/comments/aldthz/hive_complex_type_processing_via_hql_macros_no/,0,bigdata
aihul4,An Interview With The Founding Members Of The LEGO Big Data Team,,12,1,blarghmatey,2019-01-22 01:59:17,https://www.dataengineeringpodcast.com/lego-enterprise-big-data-episode-66/,0,bigdata
agmxf8,The Three Components of a Big Data Data Pipeline,,12,0,eljefe6a,2019-01-16 16:38:12,http://www.jesse-anderson.com/2019/01/the-three-components-of-a-big-data-data-pipeline/,0,bigdata
agh2pc,Data is the most valuable asset ever created.,"Data is the fuel for the 4th Revolution. I seem to find myself explaining this constantly, so I put together a brief explanation of this, just in case anyone also finds themselves constantly in the same position :) Feedback is welcome!

https://www.youtube.com/watch?v=JaIpEg7VfgQ",12,1,DemocratizedWisdom,2019-01-16 03:39:42,https://www.reddit.com/r/bigdata/comments/agh2pc/data_is_the_most_valuable_asset_ever_created/,0,bigdata
abr777,1.1 Billion Taxi Rides: Spark 2.4.0 versus Presto 0.214,,14,7,marklit,2019-01-02 07:34:39,https://tech.marksblogg.com/billion-nyc-taxi-rides-spark-2-4-versus-presto-214.html,0,bigdata
a598li,Codecademy Vs DataCamp Vs Dataquest: Which one is better for taking it upon for pursuing data analytics? Price or otherwise?,,11,1,vigbig,2018-12-11 18:05:00,https://www.reddit.com/r/bigdata/comments/a598li/codecademy_vs_datacamp_vs_dataquest_which_one_is/,0,bigdata
a2n1xo,"What is Data Analytics? Definition, Types, Software, and Use Cases",,13,1,lady_monsoon,2018-12-03 09:15:50,https://theappsolutions.com/blog/development/what-is-big-data-analytics/,1,bigdata
9v2ern,The top 10 data and analytics conferences to attend in 2019,,12,1,CrankyBear,2018-11-07 19:19:40,https://www.hpe.com/us/en/insights/articles/the-top-data-and-analytics-conferences-to-attend-in-2019-1811.html,0,bigdata
9tla1f,Avro or Parquet???,,11,11,stackchief,2018-11-02 15:49:35,https://www.stackchief.com/blog/Hadoop%20Data%20Formats%3A%20Avro%20or%20Parquet%3F,0,bigdata
9sd68d,How Netflix Is Using Jupyter Notebooks In Production (Interview),,12,0,blarghmatey,2018-10-29 13:40:26,https://www.dataengineeringpodcast.com/using-notebooks-as-the-unifying-layer-for-data-roles-at-netflix-with-matthew-seal-episode-54/,0,bigdata
9rswkq,Is Hadoop Officially Dead?,,12,19,None,2018-10-27 08:59:41,https://www.datanami.com/2018/10/18/is-hadoop-officially-dead/,0,bigdata
9rdp35,What are good Interview prep materials/questions and Sample Projects for Senior Big Data Engineer/ Architect level positions?,"What are good Interview prep materials/questions and Sample Projects for Senior Big Data Engineer/ Architect level positions.

I want to brush up on architecture of below Big data technologies and also solution and application architectures using below popular projects/technologies of the big data ecosystem,

like Spark, Spark Streaming, Storm, Hive, Pig, Tez, Hadoop, MapReduce, Kafka, Flume, YARN, MongoDB, Cassandra?

Could someone point me to such resources?",11,1,astrostar3,2018-10-25 19:56:25,https://www.reddit.com/r/bigdata/comments/9rdp35/what_are_good_interview_prep_materialsquestions/,0,bigdata
9hmiad,"Hey Quadrant Fans, checkout this tweet, from Quadrant Protocol's team. 👉 Our CEO Mike Davie met Nam Nguyen, Chief Strategy Officer of Vietnam Blockchain Capital to discuss how to plug Quadrant into the Vietnam scene! Read more here!",,11,1,Cy4cent,2018-09-21 05:28:44,https://twitter.com/exploreQuadrant/status/1042930157500227585?s=20,0,bigdata
9h5qon,Banyan Network: The 3 Laws of Data Value (3 min),,11,0,mauvaison,2018-09-19 14:43:04,https://medium.com/@garymensah/banyan-network-the-3-laws-of-data-value-d9f143f326dc,0,bigdata
941wb6,Data Scientist Interviews Demystified,,11,0,DanClark-KDnuggets,2018-08-02 18:32:42,https://www.kdnuggets.com/2018/08/data-scientist-interviews-demystified.html,0,bigdata
92bmfj,Kafka on Kubernetes with Local Persistent Volumes,,14,1,matyix_,2018-07-27 11:15:09,https://banzaicloud.com/blog/kafka-on-kubernetes/,0,bigdata
8otxoz,New site that ranks the most popular data science tools,,12,6,justinontheshore,2018-06-05 19:52:14,https://datas.ci/ence,0,bigdata
8m1vzk,Use AWS Glue to run ETL jobs against non-native JDBC data sources | Amazon Web Services,,14,0,TomorrowsMom,2018-05-25 13:49:33,https://aws.amazon.com/blogs/big-data/use-aws-glue-to-run-etl-jobs-against-non-native-jdbc-data-sources/,0,bigdata
8f1f2d,Hadoop 3: Comparison with Hadoop 2 and Spark,,13,0,viktoriia_shulga,2018-04-26 09:46:40,https://www.activewizards.com/blog/hadoop-3-comparison-with-hadoop-2-and-spark/?utm_source=reddit&utm_medium=bigdata&utm_campaign=hadoop,0,bigdata
80mprc,Apache Nifi vs Gobblin vs Others?,"Hi All,

I am assessing a big-data project, we would need to pull lots of big data sets from various internet sources (ftp, api, etc), do light transformations and light data quality / sanity checking (eg: row and columnar inspections), and push it downstream. Immediate focus is batchy, but anticipate supporting streaming down the line. Ease of support at scale is a focus..

We are looking at Apache Nifi and Gobblin, which seem to overlap in intention. What sort of use cases fit which platform best? How would they compare for the use case above? What other pieces should I be looking at?

Thanks!",12,4,mdw__,2018-02-27 14:13:47,https://www.reddit.com/r/bigdata/comments/80mprc/apache_nifi_vs_gobblin_vs_others/,0,bigdata
80m0zk,A Comparison of Deep Learning Frameworks,,12,0,y-emre,2018-02-27 12:23:15,https://www.exastax.com/deep-learning/a-comparison-of-deep-learning-frameworks/,0,bigdata
7u1kc7,"Bokeh vs Dash, which is the alternative for Shiny in Python?",,12,1,fl4v1,2018-01-30 15:00:36,https://medium.com/p/bokeh-vs-dash-which-is-the-best-dashboard-framework-for-python-c5b576375f7f,0,bigdata
7mm2r3,How I became a Data Engineer from Backend Engineering,,13,0,None,2017-12-28 11:26:04,https://medium.com/@ssola/becoming-a-data-engineer-5e0f14048d42,0,bigdata
7jqxhz,"In China, a Three-Digit Score Could Dictate Your Place in Society",,14,2,pmillerbd,2017-12-14 11:08:35,https://www.wired.com/story/age-of-social-credit/,0,bigdata
7i5soe,Why you shouldn’t use pie charts - Tips for better data visualization,,11,1,berkes,2017-12-07 11:26:49,https://blog.funnel.io/why-we-dont-use-pie-charts-and-some-tips-on-better-data-visualizations,0,bigdata
7cfeqw,Deconstructing Data Science,"Hey everyone! 👋

Yesterday I launched the second post of a new Data Science blog, where I’m open-sourcing every resource I find and insight I come across in pursuit of becoming a world-class (top 5%) Data Scientist in < 6 months.

The purpose of this post is to empower others to start accelerating their own learning by:

1) deconstructing the complex craft of Data Science into it’s simple micro-skills

2) identifying the 20% of skills that contribute to 80% of outcomes

I'm writing this with learners like you and I in mind, so if you're also interested in accelerating your learning, check it out & feel free to share around:

https://ajgoldstein.com/2017/11/12/deconstructing-data-science/",11,1,ajva1996,2017-11-12 13:35:10,https://www.reddit.com/r/bigdata/comments/7cfeqw/deconstructing_data_science/,0,bigdata
7a42ba,Career progression within big data,"Hey everyone, wanted to ask you all on this sub about your experience with data and how your career has progressed or even where you've seen colleagues progress.

I'm a data engineer myself at a consulting company, so I've been exposed to tons of different areas/technologies but don't have a lot of deep experience with them. Example- I don't know the Hadoop stack very well or even Spark, and I know a lot of groups are slowly migrating away from Hadoop. My background is in ETL development, and as of late some broad work in AWS.

I always see job postings asking for experience with data science - machine learning, modeling, etc. How much knowledge in that domain do you guys have? Would a cursory understanding of data science would pretty beneficial for data engineers like myself down the road? 

 I ask because I haven't seen many data engineers progress into management, and I'm trying to see what the possibilities are down the road.",12,3,iarcfsil,2017-11-01 14:53:53,https://www.reddit.com/r/bigdata/comments/7a42ba/career_progression_within_big_data/,0,bigdata
7a0yko,Best-Ever Algorithm Found for Huge Streams of Data,,12,2,based2,2017-11-01 02:56:26,https://www.quantamagazine.org/best-ever-algorithm-found-for-huge-streams-of-data-20171024/,0,bigdata
78iz3w,New sub for sharing open source databases: OSDB • r/osdb,,12,0,toadlyBroodle,2017-10-24 21:26:40,https://www.reddit.com/r/osdb/,0,bigdata
76vp4m,Getting confused between all of the pipeline/etl tools,"So the picture is getting quite blurry between all of the pipeline/etl tools available. 

Specifically:

 * Apache NiFi
 * Apache StreamSets
 * Luigi 
 * Airflow 
 * Falcon
 * Oozie
 * A Microsoft solution?

I've got several projects that I could see a use for a pipeline/flow tool where ETLing is the point of the entire project. So what are the strengths and weaknesses of each? Where should I be using one or the other? Where does one shine where the other would be difficult to manage or be overkill for the project? Which would be the most light-weight of the tools? 

I have several projects but have two stick out in my mind. They are completely unrelated to each other at all. They do NOT overlap at all.

1) The project is a simple ETL for XML data. In simple terms, 20 or so machines write out XML log data to their local drive that is shared on the network. A python application connects to each machine's share, copies the data to the local system for archival purposes of the raw data. The same application reads the XML data from the files, extracts all of the relevant content from the XML files and stores it into a Microsoft SQL Server database. Currently the application gets run every 20 minutes through a Huey cronjob task in Python to look for new data on the share. This is a Windows-only application/ecysystem so using something in the MS world isn't out of the question either (hence why I included it).

2) The second project is more ""pipeline"". We have about 2 million files that will need to run through a process of a) Original Format --> b)Converted to an industry standard format --> c) data massaged to fit our need --> d)Data converted --> e) intermediate results are written out to disk --> f)data use to train deep learning model to train model. 

For inference of a file, steps a), b), c), d), e), f) would be performed. Step f) would be replaced with the inference of the model and then f) would pass results down to g) (another application). This is initially going to be done on Linux that they want to end up (potentially) on Windows with so that could be a consideration.

So for these what would you end up choosing? 
",12,21,kur1j,2017-10-17 02:47:29,https://www.reddit.com/r/bigdata/comments/76vp4m/getting_confused_between_all_of_the_pipelineetl/,0,bigdata
72ozlt,Strava's data lake for all of our activity data streams,,12,0,micahville,2017-09-27 01:18:56,https://medium.com/strava-engineering/from-data-streams-to-a-data-lake-b6ca17c00a23,0,bigdata
71iutn,"Can someone help me understand this sentence: ""You don’t use Hadoop for anything where you need low-latency results.""","I'm reading this book ""Big Data Principales and best practices of scalable realtime data systems"" and there's this paragraph: 

> Hadoop, for example, can parallelize large-scale batch computations on very large
amounts of data, but the computations have high latency. You don’t use Hadoop for
anything where you need low-latency results.

What do they mean exactly in the last sentence please give examples.",14,13,nomadProgrammer,2017-09-21 12:43:26,https://www.reddit.com/r/bigdata/comments/71iutn/can_someone_help_me_understand_this_sentence_you/,0,bigdata
6xhetv,Taming Big Data with Apache Spark and Python,,12,0,dnonsense,2017-09-01 20:56:41,https://medium.com/@mrtech/taming-big-data-with-apache-spark-and-python-hands-on-142923510309,0,bigdata
6sncs8,Generalists Dominate Data Science,,13,0,rjurney,2017-08-09 18:13:21,https://blog.datasyndrome.com/generalists-dominate-data-science-f01882f25347,0,bigdata
6qncnm,1.1B Taxi Rides Benchmark On The GPU- And PostgreSQL-Powered BrytlytDB,,12,0,marklit,2017-07-31 09:35:22,http://tech.marksblogg.com/billion-nyc-taxi-rides-aws-ec2-p2-16xlarge-brytlytdb.html,0,bigdata
6n7qmn,The top Big Data skills to master in 2017. Why and How? Get Answered,,13,3,shauryasinha,2017-07-14 08:20:55,http://www.springpeople.com/blog/top-big-data-skills-that-are-high-in-demand/,0,bigdata
6j7791,Visualization and large-scale processing of historical weather radar (NEXRAD Level II) data (4 days of processing in less than an hour with Dataflow),,12,0,fhoffa,2017-06-24 08:55:04,https://cloud.google.com/blog/big-data/2017/06/visualization-and-large-scale-processing-of-historical-weather-radar-nexrad-level-ii-data,0,bigdata
6ha5jj,Top 7 Big Data Use Cases in Insurance Industry,,13,0,yagmurnur,2017-06-14 20:19:08,https://www.exastax.com/big-data/top-7-big-data-use-cases-in-insurance-industry/,0,bigdata
6axlkk,Apache NiFi MiNiFi C++ 0.2.0 release,,11,0,based2,2017-05-13 12:34:30,http://mail-archives.apache.org/mod_mbox/www-announce/201705.mbox/%3C4C394A2C-C116-411C-83DD-5F2C44077D79%40apache.org%3E,0,bigdata
6ag267,"After Lambda: Exactly-once processing in Google Cloud Dataflow, Part 1",,12,0,fhoffa,2017-05-10 22:17:55,https://cloud.google.com/blog/big-data/2017/05/after-lambda-exactly-once-processing-in-google-cloud-dataflow-part-1,0,bigdata
68zptv,"Looking to start out with data analytics? then this podcast may help - An chat with Microsoft's Amy Nicholson talking about how analytics is helping us all to make more complex decisions and to augment our day to day jobs, not replace them.",,11,0,techstringy,2017-05-03 11:22:01,https://techstringy.wordpress.com/2017/05/02/telling-data-stories-amy-nicholson-ep-25/,0,bigdata
68udtf,Caching out of Hadoop: How New York Times Embraces New Technology - The New Stack,,12,0,thetinot,2017-05-02 17:01:49,https://thenewstack.io/caching-hadoop-new-york-times-embraces-new-technology,0,bigdata
64xojd,Is there such a thing as unstructured data?,"Was having a debate with a few mates, the topic of unstructured and semi-structured came up.  We were debating whether any data is truly unstructured. 

Our thinking concluded, if the data exists, it must have some basic structure, even if it is at a meta level. 

Twitter data for example, typically regarded as unstructured, but data are organized into tweets, tweets have a user attached, they may or may not have hashtags, may or may not be a DM, they have a time and date stamp, etc. So there is some primitive structure.  Depending on the language of the data, there may be sentence structure, punctuation, etc.   It convinced me to stop using the term ""Unstructured Data"" until I can prove any exists.",11,7,NotSure2505,2017-04-12 11:57:41,https://www.reddit.com/r/bigdata/comments/64xojd/is_there_such_a_thing_as_unstructured_data/,0,bigdata
608b01,A Simple Blood Test for Autism? Study’s Use Of ‘Big Data’ Validates Early Intervention,,13,0,None,2017-03-19 03:54:26,https://www.studyfinds.org/autism-blood-test-diagnosis-study/,0,bigdata
5y9u48,How much data is being created during a second on the internet,,12,1,bigdatahero,2017-03-08 19:00:12,https://i.redd.it/ss9ftzoie8ky.png,0,bigdata
5wump5,"Data Science, Big Data, AI Events and Conferences 2017",,12,4,BigCloudTeam,2017-03-01 09:35:47,http://www.bigcloud.io/data-science-dates-2/,0,bigdata
5skesy,Any good blogs that is updated regularly on bigdata trends and developments?,,12,5,Raul2351987,2017-02-07 09:14:23,https://www.reddit.com/r/bigdata/comments/5skesy/any_good_blogs_that_is_updated_regularly_on/,0,bigdata
5o9bmj,Apache Beam established as a new top-level project,,10,18,based2,2017-01-16 06:34:50,https://beam.apache.org/blog/2017/01/10/beam-graduates.html,0,bigdata
5mg9de,"Hi Redditor, which language do you use for Spark development?","Hi Redditor, which language do you use for Spark development? 
Is python enough? Or I need to learn Scala for that? Java is used for that too. But the main languages ate Python and Scala? 

Thank you!",12,24,springquery,2017-01-06 21:17:40,https://www.reddit.com/r/bigdata/comments/5mg9de/hi_redditor_which_language_do_you_use_for_spark/,0,bigdata
5h6hzc,Top Big Data Skills To Future Proof Your Career – Become A Data Engineer Now,,12,3,LOLrusty,2016-12-08 11:47:07,http://www.greycampus.com/blog/big-data/top-big-data-skills-to-future-proof-your-career-become-a-data-engineer-now,0,bigdata
5ak0vx,Splunking Kafka At Scale,,12,0,eberkut,2016-11-01 16:24:02,http://blogs.splunk.com/2016/10/31/splunking-kafka-at-scale/,0,bigdata
595wqo,Automating big-data analysis,,12,0,eberkut,2016-10-24 17:44:56,http://news.mit.edu/2016/automating-big-data-analysis-1021,0,bigdata
55isd2,Data Engineers - What is daily life on the job like? And a few other questions from a college kid if you have the time.,"Hi all, I'm currently a student with interest in becoming a data engineer.  Right now I'm studying web development while I'm in school but would like to eventually transition into a data engineering job after some time with development, if not immediately jump into a data engineering job after school.

If you have the time, I'd greatly appreciate it if you could answer some questions I had on my mind.

* What's your job title?  

What is life on the daily at your job like?

* What did you do / learn while in school to make yourself stand out to help get into your career?

* For someone with a software development background, what is the most feasible thing to do / learn from here?  Is there an order in which we should do this?

* Is it feasible at all to seek out a data engineer position immediately out of college?

* Can I stand out to employers by creating some projects at least?  What skills should those projects present to potential employers?

* Any personal advice you have to offer to those interested in your career?

* What would you have done differently to have a successful career?

* Where do you see yourself going in the next 5 years?",13,15,None,2016-10-02 15:54:52,https://www.reddit.com/r/bigdata/comments/55isd2/data_engineers_what_is_daily_life_on_the_job_like/,0,bigdata
4yzh1h,4TB smashed in 35 seconds using Google BigQuery. Simply outrageous!,,11,7,polleyg,2016-08-22 07:53:23,https://i.reddituploads.com/ef64c17c5eea4264bfc2346775fb165d?fit=max&h=1536&w=1536&s=fc3d2ac71ee31a983a88e64bd84acc44,0,bigdata
4yq9vu,How Custom Crawling and Data Mining Can Help You Grow Your Business,,12,1,paulbor04,2016-08-20 18:01:16,https://www.georanker.com/how-custom-crawling-and-data-mining-can-help-you-grow-your-business,0,bigdata
4yk3n0,Google BigQuery hits the gym to beef up even more!,,10,1,thetinot,2016-08-19 17:10:38,https://shinesolutions.com/2016/08/19/google-bigquery-hits-the-gym-to-beef-up-even-more/,0,bigdata
4xsasb,Benchmarking MapD querying 1.1 Billion Taxi Trips with 8 of the new Nvidia Pascal Titan Xs,,13,2,marklit,2016-08-15 06:23:01,http://tech.marksblogg.com/billion-nyc-taxi-rides-nvidia-pascal-titan-x-mapd.html,0,bigdata
4sc77e,Data Science Summer Reading List 2016,,10,1,ironwalker76,2016-07-11 17:42:14,http://www.datascienceassn.org/content/data-science-summer-reading-list-2016,0,bigdata
4qtryy,The White House is starting to get it right on big data: Center for Data Innovation,,11,0,spacemnky23,2016-07-01 20:12:28,https://whipboard.co/2016/06/white-house-starting-get-right-big-data/,0,bigdata
4o7mua,Here’s the data that told us Bernie Sanders would lose,,13,5,fhoffa,2016-06-15 14:42:08,https://www.washingtonpost.com/news/monkey-cage/wp/2016/06/15/heres-the-data-that-told-us-bernie-sanders-would-lose/,0,bigdata
4j8oyr,6 ways to maximize the value of Business Intelligence,,13,1,Plainbox,2016-05-13 22:13:04,http://www.mrc-productivity.com/blog/2016/05/6-ways-to-maximize-the-value-of-business-intelligence/,0,bigdata
4hepq8,Data literacy: helping non-data specialists make the most of data science,,12,0,stormforce7916,2016-05-02 05:44:37,https://gds.blog.gov.uk/2016/04/27/data-literacy-helping-non-data-specialists-make-the-most-of-data-science/,0,bigdata
4epxxh,Apache Storm 1.0.0 released,,11,1,alexeyr,2016-04-14 06:34:40,https://storm.apache.org/2016/04/12/storm100-released.html,0,bigdata
4drexf,A Billion NYC Taxi Rides on Google's BigQuery,,12,0,marklit,2016-04-07 14:59:37,http://tech.marksblogg.com/billion-nyc-taxi-rides-bigquery.html,0,bigdata
4cceeq,Can Big Data Help Psychiatry Unravel the Complexity of Mental Illness?,,13,0,burtzev,2016-03-28 21:57:22,http://www.scientificamerican.com/article/can-big-data-help-psychiatry-unravel-the-complexity-of-mental-illness/?WT.mc_id=SA_EVO_20160328,0,bigdata
46uxss,"For Data Scientists, Big Data is not so Big",,12,0,lokator9,2016-02-21 11:35:15,http://businessoverbroadway.com/for-data-scientists-big-data-is-not-so-big,0,bigdata
43y8hb,It's official: Google Dataflow accepted as Apache Beam - the new Batch + strEAM paradigm,,12,0,fhoffa,2016-02-03 04:37:29,http://mail-archives.apache.org/mod_mbox/incubator-general/201602.mbox/%3C56AF6BF1.6030400@nanthrax.net%3E,0,bigdata
3or27r,"Big Data: I'm drinking from the fire hose, where do I start?","My company hired me to manage clients and somehow this morphed into a big data role around mining our DBs and creating KPI/metrics about customer behavior.

I really enjoy this type of work and have been getting by with what little knowledge I have, but I'll admit that I feel unprepared (and somewhat incapable) to take our big data initiative to the level my company is pushing towards.

I want to make a career out of this and am willing to put in the time and effort to gain the necessary skills. Unfortunately there's no one else at my job who does this, so I'm hoping you guys could point me in the right direction. My company currently uses MySQL and Pentaho. I've gotten a certificate in the former and just started learning the latter. Are there any other programs, softwares, or courses I should be focusing on?



",14,12,drowninginbigdata,2015-10-14 18:03:43,https://www.reddit.com/r/bigdata/comments/3or27r/big_data_im_drinking_from_the_fire_hose_where_do/,0,bigdata
3fax2o,Navigating 3 Petabytes of Data: Analyzing Market Trade Data,,11,0,musing5225,2015-07-31 15:54:04,http://technology.finra.org/articles/fastola.html,0,bigdata
3f0tm7,Amazon Machine Learning: use cases and a real example in Python (+ new course),,11,0,alexcasalboni,2015-07-29 12:46:05,http://cloudacademy.com/blog/aws-machine-learning/?utm_source=reddit.com&utm_medium=social&utm_campaign=Blogpost,0,bigdata
3ayay0,Public Cloud War: AWS vs Azure vs Google,,12,0,alexcasalboni,2015-06-24 14:01:09,http://cloudacademy.com/blog/public-cloud-war-aws-vs-azure-vs-google/?utm_source=reddit.com&utm_medium=social&utm_campaign=Blogpost,0,bigdata
3ac52b,How Netflix uses Kafka,,13,0,None,2015-06-18 22:15:22,http://techblog.netflix.com/2015/06/nts-real-time-streaming-for-test.html?m=1,0,bigdata
39x5t4,NASA new big dataset on climate change,,11,1,bizzard4,2015-06-15 15:02:58,http://gizmodo.com/the-world-in-2100-according-to-nasas-new-big-dataset-1710798646?rev=1434286945343&utm_campaign=socialflow_gizmodo_facebook&utm_source=gizmodo_facebook&utm_medium=socialflow,0,bigdata
39535g,Google Prediction API: a Machine Learning black box for developers,,13,0,alexcasalboni,2015-06-09 09:02:52,http://cloudacademy.com/blog/google-prediction-api/?utm_source=reddit.com&utm_medium=social&utm_campaign=Blogpost,0,bigdata
34ejyo,Looking for a career in Big Data? We're hiring!,"We're a Big Data consulting company based out of Durham, NC, and we're looking for DevOps Engineers and Big Data Consultants to join our team.

[Mammoth Data - check us out](http://mammothdata.com/careers/)

**What we have to offer:**

* A casual and good-humored office environment
* Copious opportunities to grow your skills, both technical and soft skill sets
* Publish in syndicated technical journals
* 100% employer paid medical, dental, vision, short/long term disability, and life insurance, and paid community service hours
* Varied work projects and autonomy: **no** micro-managing

**What we're looking for:**

Required:

* Java development experience
* Revision control (e.g. Git)
* Functional unit and integration testing, load testing, best practices
* Familiarity with Unix concepts
* Ability to develop a rapport with clients
* Driven to join the Big Data revolution
* Minimum of 2 years of relevant experience
* Ability to travel to client sites

Preferred:

* ""Big Data"" Technologies: Hadoop, Hive, Spark, Kafka, Storm, HBase, Couchbase, Cassandra, Elasticsearch, etc.
* Cloud and virtualization
* Continuous integration (e.g. Jenkins)
* Identity management ",12,23,mammothdurham,2015-04-30 14:31:55,https://www.reddit.com/r/bigdata/comments/34ejyo/looking_for_a_career_in_big_data_were_hiring/,0,bigdata
32pc8z,Hadoop? Most U.S. Companies Haven't Started That Big Data Journey,,12,13,CrankyBear,2015-04-15 17:03:48,http://www.afternines.com/news/2015/4/15/hadoop-most-us-companies-havent-started-that-big-data-journey,0,bigdata
3099r5,Michael Stonebraker wins $1 million Turing Award (Inventor of core Database concepts including big data),,13,2,urmyheartBeatStopR,2015-03-25 14:52:12,https://newsoffice.mit.edu/2015/michael-stonebraker-wins-turing-award-0325,0,bigdata
2zow21,Telecom firms mine for gold in big data despite privacy concerns. You'd think the US controversy last year over privacy concerns would be enough to stop them but the wallet wants what the wallet wants.,,13,0,tanisha095,2015-03-20 11:59:27,http://www.reuters.com/article/2014/02/23/us-mobile-world-bigdata-idUSBREA1M09F20140223,0,bigdata
2x04xg,The Apache Software Foundation Announces Apache™ HBase™ v1.0,,11,0,fs111_,2015-02-24 16:38:15,https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces72,0,bigdata
2uarma,The Apache Software Foundation Announces Apache™ Samza™ as a Top-Level Project: distributed stream processing framework,,11,6,based2,2015-01-31 12:03:34,http://mail-archives.apache.org/mod_mbox/www-announce/201501.mbox/%3C2138551678.1150973.1422352856439.JavaMail.yahoo%40mail.yahoo.com%3E,0,bigdata
2tmoqh,Good books on big data? Introductory and advanced. (for a trained statistician/software engineer),,13,4,None,2015-01-25 17:42:11,https://www.reddit.com/r/bigdata/comments/2tmoqh/good_books_on_big_data_introductory_and_advanced/,0,bigdata
2sxf0m,"Right tool for the right job, or how command line can be 235x faster than Hadoop",,14,2,hagge,2015-01-19 11:55:17,http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html,0,bigdata
2ooel9,Free 8h tutorial on Big Data Analytics with Revolution R Enterprise (videos + in browser coding),,13,0,martijnT,2014-12-08 20:05:48,https://www.datacamp.com/courses/introduction-to-revolution-r-enterprise-for-big-data-analytics,0,bigdata
2jp1y5,So I've been readibg Dataclysm over the weekend and it seems like we're about to be flooded by Big Data superficial popularizers.,,12,1,steerpike0,2014-10-19 15:13:20,http://m.imgur.com/gtMii5e,0,bigdata
2ji2w4,"IT crisis looming: 'What if AWS goes pop, runs out of cash?'",,11,19,None,2014-10-17 09:00:20,http://www.channelregister.co.uk/2014/09/25/public_iaas_crisis/?utm_content=bufferafe71&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer,0,bigdata
2c54z1,Insider's Guide to Big Data Interviewing,,11,2,None,2014-07-30 14:15:49,http://www.toptal.com/big-data#hiring-guide,0,bigdata
2aivjr,Germany’s 12th Man at the World Cup: Big Data,,12,1,seby408,2014-07-12 17:58:58,http://blogs.wsj.com/cio/2014/07/10/germanys-12th-man-at-the-world-cup-big-data/,0,bigdata
1ztwto,Graduate Degrees & Online Courses for Big Data Analytics & Data Science,,12,13,ironwalker76,2014-03-07 19:11:58,http://www.datascienceassn.org/content/graduate-degrees-online-courses-data-science-and-advanced-analytics,0,bigdata
1zhbgo,Any recommendation for learning Hadoop online?,"Hi. Title pretty much says it all. I'm beginner level programmer (a bit of SQL, love playing around in Excel, a bit of Xcode) so something that starts on the ground floor would be great.

Thoughts on wiziq courses (http://www.wiziq.com/course/21308-hadoop-big-data-training)? Comments don't look great.

Thanks!",14,9,CrossyFTW,2014-03-04 00:01:08,https://www.reddit.com/r/bigdata/comments/1zhbgo/any_recommendation_for_learning_hadoop_online/,0,bigdata
1vo4w6,Apache Spark: The Next Big Data Thing?,,14,0,crazyhankie,2014-01-20 13:37:51,http://blog.mikiobraun.de/2014/01/apache-spark.html,0,bigdata
1q1nei,Presto is here! Fast big data analysis from Facebook,,14,2,fuzzyballzy,2013-11-06 19:36:49,https://www.facebook.com/notes/facebook-engineering/presto-interacting-with-petabytes-of-data-at-facebook/10151786197628920,0,bigdata
1pjfvw,"GraphChi: How a Mac Mini Outperformed a 1,636 Node Hadoop Cluster",,12,2,saulsherry,2013-10-30 14:19:11,http://www.bigdatarepublic.com/author.asp?section_id=2840&doc_id=269178,0,bigdata
1n3ppr,Big Data Market worth $46.34 Billion by 2018,,10,2,StefanKoopman,2013-09-25 13:54:34,https://www.asdreports.com/news.asp?pr_id=1854,0,bigdata
1g128f,Robert McNamara and the Dangers of Big Data at Ford and in the Vietnam War,,11,1,DevFRus,2013-06-10 05:39:20,http://www.technologyreview.com/news/514591/the-dictatorship-of-data/,0,bigdata
1cumit,"Big Data, Trying to Build Better Workers",,11,1,pacmanisfun,2013-04-22 06:43:31,http://www.nytimes.com/2013/04/21/technology/big-data-trying-to-build-better-workers.html,0,bigdata
1ccco9,BBC Horizon - The Age of Big Data,,11,1,hypercluster,2013-04-14 20:03:35,http://www.youtube.com/watch?v=EsVy28pDsYo&feature=youtube_gdata_player,0,bigdata
1b2gy8d,Seeking Open-Source Big Data Projects to Dive Into!,"Hey Data Geeks,

I'm a data analytics engineer/BI developer with five years of experience, having worked in big4 firms throughout my career. Here are the stacks I've used in my job: Alteryx, SQL, Python, and PowerBI.

Recently, I've been diving into learning PySpark, Git, CI/CD, AWS, and more for transitioning into the big data field. I'm eager to experience the challenges and rewards of working with real big data, so I'm on the lookout for open-source projects to contribute to. Can anyone point me in the right direction? Your insights and leads would be greatly appreciated.",12,3,Jkk_geek,2024-02-28 20:47:07,https://www.reddit.com/r/bigdata/comments/1b2gy8d/seeking_opensource_big_data_projects_to_dive_into/,0,bigdata
15cnwhd,I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube,"Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!

[https://www.youtube.com/watch?v=aiHSMYvoqYE](https://www.youtube.com/watch?v=aiHSMYvoqYE)",11,2,onurbaltaci,2023-07-29 09:28:17,https://www.reddit.com/r/bigdata/comments/15cnwhd/i_recorded_a_crash_course_on_polars_library_of/,0,bigdata
12te369,How does apache Beam give exactly once guarantee and do stateful calculation without checkpoint or fault tolerence?,"Things like groupby or combine needs exactly once guarantee for trivial stateful calculation like sum per key

But apache beam seems to not have checkpoint baked in to the library, does it rely on flink or spark to manage fault tolerence and consistency in state?",12,1,Rough_Source_123,2023-04-20 20:27:58,https://www.reddit.com/r/bigdata/comments/12te369/how_does_apache_beam_give_exactly_once_guarantee/,0,bigdata
11mxpjp,A 0.6 release of UI for Apache Kafka w/ cluster configuration wizard & ODD Platform integration is out!,"Hi redditors!

Today I'm delighted to bring you the latest 0.6 release of UI for Apache Kafka, packed with new features and enhancements!

This version offers:
- A configuration wizard that simplifies cluster setup (right in web UI!). Now we can launch the app via AWS AMI image and setup a cluster on the go
- Integration with OpenDataDiscovery Platform to gain deeper insight into your metadata changes
- Support for protobuf imports & file references

Other minor, yet significant, enhancements include:
- Embedded Avro embedded serde plugin
- Improved ISR display on Topic overview (now you can view it per partition!)

And a cherry on top? Now we’re able to work around kafka ACL errors so you won’t need to confront pesky permission issues when using the app.

Don’t wait, the update is already available on github & @ AWS Marketplace!

Full changelog: https://github.com/provectus/kafka-ui/releases/tag/v0.6.0
Thanks to everyone who just started and continued to contribute!
In the next release, we'll focus a bit on expanding our RBAC possibilities (support for LDAP and universal OAuth providers) and some Wizard features!",12,0,Haarolean,2023-03-09 17:12:45,https://www.reddit.com/r/bigdata/comments/11mxpjp/a_06_release_of_ui_for_apache_kafka_w_cluster/,0,bigdata
11hpdvw,Create Hive Table (Hands On) with all Complex Datatype,,10,0,bigdataengineer4life,2023-03-04 04:26:22,https://youtu.be/Fxyjf3QCp78,0,bigdata
11grmjp,Geospatial DuckDB,,10,0,marklit,2023-03-03 04:49:45,https://tech.marksblogg.com/duckdb-geospatial-gis.html,0,bigdata
zb7ztu,Apache Hive for Data Engineers (Hands On) with 2 Projects,,10,0,bigdataengineer4life,2022-12-03 04:51:47,https://youtu.be/m_WY1mGAxDE,0,bigdata
z7mlq6,Twitter Sentiment Analysis with Azure Synapse Analytics | Real time Stream | E2E Big Data Pipeline,,11,5,balramprasad,2022-11-29 07:16:22,https://youtu.be/jm4wmQPt0sc,0,bigdata
xd9qvf,Dynamic DAGs with Airflow: The best way,,11,0,marclamberti,2022-09-13 14:56:33,https://youtu.be/6eHuOd96unQ,0,bigdata
xcs6cn,Apache Iceberg 101 - Your Guide to Learning Apache Iceberg Concepts and Practices,,12,0,amdatalakehouse,2022-09-12 23:55:12,https://www.dremio.com/subsurface/apache-iceberg-101-your-guide-to-learning-apache-iceberg-concepts-and-practices/,0,bigdata
wn7qlk,Apache Hive for Data Engineers (Hands On) with 2 Projects,,10,0,bigdataengineer4life,2022-08-13 06:44:44,https://youtu.be/m_WY1mGAxDE,0,bigdata
wisjcx,"CMU Researchers Open-Source ‘auton-survival’: A Comprehensive Python Code Repository of User-Friendly, Machine Learning Tools for Working with Censored Time-to-Event Data","Machine learning is being used in almost every industry, including healthcare. However, due to the intrinsic complexity of healthcare data, classical machine learning faces various difficulties while dealing with these data. This is because healthcare outcomes like mortality, stroke, cancer initiation, and readmission frequently have a continuous time to events. Since time-to-event data frequently contains individuals whose outcomes are missing or censored owing to loss of follow-up, dealing with this type of data is much more difficult. The researchers have established that traditional classification and regression methods do not offer a simple solution to dealing with such clinical data.

Many researchers have been interested in applying deep neural networks, which may be used to create nonlinear representations of complex clin

A new study by Auton Lab at Carnegie Mellon University introduced the auton-survival package, a comprehensive Python library of user-friendly tools for machine learning applications in the presence of censored time-to-event data.

[Continue reading](https://www.marktechpost.com/2022/08/07/cmu-researchers-open-source-auton-survival-a-comprehensive-python-code-repository-of-user-friendly-machine-learning-tools-for-working-with-censored-time-to-event-data/) | *Check out the*[ *paper*](https://arxiv.org/pdf/2204.07276.pdf)*,* [*package*](https://autonlab.org/auton-survival/)",12,0,ai-lover,2022-08-07 22:51:41,https://www.reddit.com/r/bigdata/comments/wisjcx/cmu_researchers_opensource_autonsurvival_a/,0,bigdata
wf2ef9,Why Data Engineers are in Greater Demand than Data Scientists,,11,0,Emily-joe,2022-08-03 09:52:35,https://medium.com/@emily.joe685/why-data-engineers-are-in-greater-demand-than-data-scientists-eab067af66a1,0,bigdata
ue4t8g,"Created Artificial Neural Network from Scratch, without any framework",,12,0,Illustrious_Party330,2022-04-28 20:46:46,https://medium.com/itnext/building-neural-network-from-scratch-in-python-71ed71d34588,0,bigdata
szsxgp,DP-203: Practical tips for becoming an Azure Data Engineer Associate,,11,1,IFMinds,2022-02-23 21:45:43,https://medium.com/@ifmindshere/dp-203-practical-tips-for-becoming-an-azure-data-engineer-associate-fd7726e2aca7,0,bigdata
ssm3p9,Apache Iceberg Version 0.13.0 is Released,,10,2,amdatalakehouse,2022-02-14 21:37:05,https://www.dremio.com/subsurface/apache-iceberg-version-0-13-0-is-released/,0,bigdata
rsdelc,"Benthos, the awesome open source stream processor, reached 100 contributors",,11,1,mihaitodor,2021-12-30 21:58:19,https://twitter.com/Jeffail/status/1476598958445285384,0,bigdata
qr0ehg,How do Dating Sites Use Big Data and Data Science Applications,,11,0,GlobalTechsub,2021-11-10 18:10:43,https://www.globaltechoutlook.com/how-do-dating-sites-use-big-data-and-data-science-applications/,0,bigdata
qa1xlk,Open-source data lineage (now integrated with BigQuery and Snowflake!)," Hi,

We built a new open-source data observability solution for the modern data stack, starting with data lineage module that focuses on simple setup and immediate visualization of the datasets and their current status. It consists of a CLI that analyzes jobs history and metadata, so no SDKs, complex deployments or code changes are needed.

Mapping lineage from jobs history enables capturing the true state in a highly accurate manner, as it describes transformations, loads and changes that actually happened, and whether they succeeded. It also leverages the jobs history to enrich the graph with operational context such as freshness and volumes.

**We would love to get some feedback** and learn what we should add and improve!! 

It is really easy to use, as you can see here:

[https://github.com/elementary-data/elementary-lineage](https://github.com/elementary-data/elementary-lineage)",11,0,Conscious_Agent_5129,2021-10-17 16:18:25,https://www.reddit.com/r/bigdata/comments/qa1xlk/opensource_data_lineage_now_integrated_with/,0,bigdata
pt3d16,Apache Kafka 3.0 has been released,,12,0,rmoff,2021-09-22 08:50:57,/r/apachekafka/comments/pt3cme/apache_kafka_30_has_been_released/,0,bigdata
pfoexi,The Impact of Disks on RocksDB State Backend in Flink: A Case Study,,11,0,Marksfik,2021-09-01 06:50:48,https://www.ververica.com/blog/the-impact-of-disks-on-rocksdb-state-backend-in-flink-a-case-study,0,bigdata
o16b5y,Deepnote – a collaborative Jupyter platform in the browser. We just made it free for small teams,,10,4,the21st,2021-06-16 14:33:58,https://deepnote.com/teams,0,bigdata
nmoitm,ClickHouse - an open-source column-oriented database management system that allows generating analytical data reports in real time.,,11,0,beleeee_dat,2021-05-28 03:17:28,https://github.com/ClickHouse/ClickHouse,0,bigdata
na1e1j,Explaining the mechanics of Spark caching,,12,0,luminoumen,2021-05-11 16:44:51,https://luminousmen.com/post/explaining-the-mechanics-of-spark-caching,0,bigdata
n4p4pe,First post of series on big data and data engineering,,10,0,asm0dey,2021-05-04 14:05:30,https://blog.jetbrains.com/blog/2021/05/04/big-data-world-part-1-definitions/,0,bigdata
mu902a,Canada needs a better health data infrastructure to support our healthcare heroes,,11,0,None,2021-04-19 19:54:51,https://quoimedia.com/canada-needs-a-better-health-data-infrastructure-to-support-our-healthcare-heroes-2/,0,bigdata
ms64ix,"Focus on the PythonOperator in Airflow 2.0, all you need in 20mins!",,11,3,marclamberti,2021-04-16 15:55:23,https://youtu.be/FtBpD-FX1HM,0,bigdata
mnm9ho,Cookie Cutter-a useful template for data science projects,,11,1,data_alltheway,2021-04-09 17:27:35,https://www.youtube.com/watch?v=mWvW-nDZrgI,0,bigdata
mduclj,Apache Airflow Sensors : What you need to know,,12,0,marclamberti,2021-03-26 17:55:29,https://marclamberti.com/blog/airflow-sensors/,0,bigdata
lxj1zj,Code your first DAG in Apache Airflow for Beginners,,11,4,marclamberti,2021-03-04 12:08:12,https://youtu.be/IH1-0hwFZRQ,0,bigdata
lteqgg,Master Data Management on a shoestring?,"I work at a nonprofit,  and we have two databases (one on Oracle and another on MSSQL) that hold nearly the same sort of information, but serve very different business purposes. I was able to replicate the data from Oracle into MSSQL using SSIS.

After a year of being live with this solution, it is obvious that the integration has a major issue with merging a particular entity model. I think it is time we explore a master data management solution.

We have SQL Server Standard edition; Master Data Services only comes with Developer and Enterprise editions. I do not have the budget to upgrade, and for regulatory reasons cannot deploy Developer ed in a production environment. 

Since I have a non-existent budget, I have been eyeing Talend Open Studio for MDM. Given our investment in SSIS, I  hate giving up the work we did in SSIS, especially since we bought a few 3rd party components for it.

Is there anyway to use Talend's MDM solution with SSIS (a custom component perhaps)? I see there are components in Talend to import SSIS packages into Talend Open Studio.

Are there any open-source/budget friendly MDM solutions that work with SSIS? Is there anyway to license Microsoft's solution separately from upgrading SQL Server?

Please forgive me - this is not my area of expertise.",13,4,drk2cmp,2021-02-27 03:03:53,https://www.reddit.com/r/bigdata/comments/lteqgg/master_data_management_on_a_shoestring/,0,bigdata
l16d21,Top Data Science Interview Questions in 2021 [3 concepts tested on all interviews],,9,1,boyzone87seo,2021-01-20 10:09:22,https://youtube.com/watch?v=lG0PbUq4wkg&feature=share,0,bigdata
kz6200,A lightweight alternative to Amundsen for your dbt project,"You've probably heard about Data Governance, Data Lineage etc... But I was curious to explore what it meant in practice for a smaller team than the ones who created Amundsen, Marquez and other metadata engines like that.

For example, what can a team already using dbt do? It'd be good to get started with a light weight tool I think, a bit like dbt Docs, but one step further.

I've explored that topic a little in [this blog post](https://guitton.co/posts/dbt-search/) and I open sources a project where you can get started on this topic. I'm curious what you think about it.

[https://guitton.co/posts/dbt-search/](https://guitton.co/posts/dbt-search/)",11,0,laguitte,2021-01-17 13:01:32,https://www.reddit.com/r/bigdata/comments/kz6200/a_lightweight_alternative_to_amundsen_for_your/,0,bigdata
koda0n,The Data Science 2021 Free Virtual Event!,,10,0,TheTesseractAcademy,2021-01-01 15:43:48,https://www.eventbrite.co.uk/e/the-london-data-science-2021-new-year-virtual-meetup-tickets-134659475115,0,bigdata
js3y2p,Should i use big data solutions for 380 GB of data per day,"So, we have 15 machines that generate 25-30 GB of data per day (each). these are mainly semi-structured txt files (time, date, message, Product number , function). there are 5 possibles templates for the messages.

Sometimes, in order for a machine to make a certain decision, it needs to see what other machines did to a certain product and when. it needs to querry the data (The factory is fully automated).

The log files sizes are ranging between 10 KB and 10 MB. and the data won t be needed for more than 3 months at most (Also, the total size of data will not be more than 40 TB at most at any given time and probably not more than 10 TB) => this means that there should be some mechanism to delete old data.

&#x200B;

Approach one: Use the usual tools for big data : HDFS, Hadoop, Spark, HBase

&#x200B;

Approach two: Use the GPU to parse the data into JSON format and then simply store it in a MongoDB database. Use Redis for caching.

&#x200B;

&#x200B;

Time needed for a querry is not critical (Can be up to 30 second but lower is better).

&#x200B;

Would storing millions of small files in HDFS be problematic ? how to overcome this ?

&#x200B;

This is a bonus unrelated question: Is there is a scenario where using Big data tools on a single machine benifical (Single Node)",11,30,Damballa_,2020-11-11 07:43:24,https://www.reddit.com/r/bigdata/comments/js3y2p/should_i_use_big_data_solutions_for_380_gb_of/,0,bigdata
jh4ib6,Databricks Eyes Soon IPO,,12,0,The-Techie,2020-10-24 06:56:23,https://www.thetechee.com/2020/10/databricks-eyes-soon-ipo.html,0,bigdata
jc80pz,Apache Beam for Dummies - An introduction.,,12,0,mrRobot_7,2020-10-16 11:19:11,https://www.loginradius.com/engineering/blog/apache-beam-for-dummies/,0,bigdata
iyxtga,New Free Mini-Course: Introduction to Docker for Data Engineers,"Hey all,

I think Docker is often times a blind spot for Data Engineers (at least based on my experience so far). At [datastack.tv](https://datastack.tv) we just released a new course that teaches you everything you need to know about Docker as a Data Engineer. Check it out: [https://datastack.tv/docker-course.html](https://datastack.tv/docker-course.html)

This course is created by Nana Janashia 👩🏽‍💻 an amazing content creator and DevOps Engineer. She has a [YouTube channel](https://www.youtube.com/c/TechWorldwithNana) about all things DevOps.

We have some other courses as well for Data Engineers. [Browse our courses here!](https://datastack.tv/courses.html) 

Feedback is welcome! Happy learning!",11,0,alexandraabbas,2020-09-24 14:02:17,https://www.reddit.com/r/bigdata/comments/iyxtga/new_free_minicourse_introduction_to_docker_for/,0,bigdata
igs3xf,When you should ACTUALLY be using Kafka...,,11,0,stackchief,2020-08-26 04:15:33,https://www.stackchief.com/blog/When%20you%20should%20be%20using%20Kafka,0,bigdata
i5doyc,Free tutorial series on R programming language for Beginners,,12,1,Reginald_Martin,2020-08-07 13:21:55,https://www.youtube.com/watch?v=XyxxdBq3op8,0,bigdata
hzyxbx,Flink SQL Demo: Building an End-to-End Streaming Application,,12,0,Marksfik,2020-07-29 11:17:29,https://flink.apache.org/2020/07/28/flink-sql-demo-building-e2e-streaming-application.html,0,bigdata
hjzk2o,"Example use cases for Apache Flink including scenarios such as Event-driven Applications, Data Analytics Applications, Data Pipeline Applications",,11,0,Marksfik,2020-07-02 15:54:37,https://flink.apache.org/usecases.html,0,bigdata
guguin,Serverless workflows (who needs Airflow anyway!) using AWS Step Functions - 4 hacks to improve your workflows,,11,0,OrganicBig9,2020-06-01 08:33:17,https://medium.com/@james_turner/aws-step-function-tricks-ffe7eef81a5e,0,bigdata
gp23ob,List of fresh remote jobs in big data,,11,1,mrpatjay,2020-05-23 09:32:52,https://freshremote.work/data-jobs/,0,bigdata
g0gg0u,The Rise of Data Science,,10,3,Laserpainter,2020-04-13 10:32:51,https://sonasoft.com/rise-of-data-science/,0,bigdata
fm27e7,"Benchmarking Time Series workloads on Kudu, ClickHouse, InfluxDB and VictoriaMetrics",,11,2,bankim_bhavsar,2020-03-20 20:33:31,https://blog.cloudera.com/benchmarking-time-series-workloads-on-apache-kudu-using-tsbs/,0,bigdata
ffwnay,An interview about how a data hub architecture can reduce the overhead of managing data governance and compliance across an organization,,12,0,blarghmatey,2020-03-09 15:47:08,https://www.dataengineeringpodcast.com/data-hub-architecture-data-governance-episode-123/,0,bigdata
ffa3g5,How Big Data Helped Netflix Series House of Cards Become a Blockbuster?,,11,2,chralesmoore,2020-03-08 09:02:11,https://sofy.tv/blog/big-data-helped-netflix-series-house-cards-become-blockbuster/,0,bigdata
faf7cr,How to Turn Your Weaknesses Into Strength Statements In a Job Interview,"Fact is, you can get the job offer even if you’re not a perfect match  to what the job description is asking for.  You can even beat others  that are more qualified than you. It happens all the time.

But you need to know your weaknesses, how to talk about them in an interview, and turn them into **strength statements**.

Truthfully, I hate the word weakness. But for the purposes of this  article, when I say weakness I mean any lack of skill, any lack of  experience or gap in employment you might have.  Generally, you need to  know how to address any objection they may have towards your competency  for the position.

We all have weaknesses. The question is, what is the best way to talk about them and turn them into **strength statements** when they come up in an interview?

First, back up a bit and think about what it means to already have an interview.

1. Your resume made it through the ATS filters
2. A person read your resume and liked it enough to consider you a candidate and call you

That’s awesome! Your job now is to sell them on your strengths and  ease their fears about your weaknesses. By reading your resume, they may  already see some holes in your qualifications. That’s OK! They’re  always going to have some doubt about your competency unless you are  very well networked.

So how do you turn fear or doubt into trust in an interview?

## 1) Prepare your Strength Statements before your Interview

Interviews aren’t just about answering the questions you get asked.  More importantly, you need to come prepared with **Strength Statements** that can be stated alone or in almost every question you are asked.

**Strength Statements** are concise ways to communicate  your top strengths, value-adds, qualifications, and benefits you will be  bringing to the company. 

Having a solid understanding of your top qualifications will let you  easily redirect back to them if they bring up any objection toward you. 

I’ll talk more about the **Address and Redirect Method** below with word-for-word examples, which is how you shift the focus off your weaknesses and onto your Strength Statements.

But I’m not talking about any random strengths you have. You could  have skills and experience in several areas that don’t apply directly to  the job. Therefore, the first step is:

### Step 1) Determine Your Top 3 Strengths

Your top 3 strengths will be your best qualifications that apply  directly to what the job is asking for. They can be direct or  transferable skills/experience (and they should be on your resume as  well).

You probably have more than 3, but in an interview you don’t always  have the ability to say everything you want to. Therefore, it’s critical  to know what to talk about first. Think:

*What are the 3 most important things I MUST say in the interview to prove I am the best person for the job?*

But beyond simply stating your top 3 qualifications (your strengths),  consider relating them to how they will benefit the company. (your  benefits)

A strength could be your skills, experience, network, etc. For example:

* knowledge of project management
* parallel programming or 
* managing a team. 
* already knowing your teammates or the hiring manager

Those are strengths.

But you need to take it one step further and tell them how your  strengths will BENEFIT THEM. And that’s how you can make the connection  in their minds between what you have and why you are a good fit for the  job.

Ultimately they are trying to determine if you can do the job and if  you can do it well. So just talking about your strengths doesn’t fully  convey you know the problems the job is supposed to solve.

Therefore, the best **Strength Statements** mention how your qualifications will *benefit* the company in some way. Example benefits could be:

* Saving money or time
* Increased customer satisfaction
* Increased efficiency or outputs

### Step 2) Create Your Top 3 Strength Statements 

Once you’ve determined your top 3 qualifications and benefits, construct **strength statements** and write them down word-for-word.

There are several ways to communicate a strength statement in an interview. Here are a few examples:

* You can state them clearly – Strength, then Benefit

*I have a lot of experience with OpenMP and if hired I can  increase the efficiency of your existing code sets and therefore lower  computing costs.*

* You can wrap them in a behavioral-based story. 

*I once was working on this very old Fortran code and you wouldn’t believe how slow it was running until I…*

* You can lead with the Benefit and then follow with the Strength. 

*I have a lot of experience with knowing how to lower computing costs by increasing the efficiency of existing OpenMP code sets.*

You should also prepare to elaborate on your strength statements if  asked to. Therefore, make sure you have examples or stories ready to  support them.

Writing these down and practicing them out loud is one of the most  important things you can do to prepare for your interview. Why? Because  increased stress levels in your interview can botch your speech,  articulation, and memory.

But having an arsenal of prepared **strength statements**  will let you articulately convey your value and make you appear more  competent to the hiring manager in the most stressful moments.

### Example Strength Statements – Technical Support Engineer

Let’s say you are applying for a Technical Support Engineer position. Your Strength Statements might be:

* “I’m comfortable providing technical support under pressure, especially with new problems.”
* “I can handle a large support case load and still maximize customer satisfaction.”
* “I  have experience with the technology and working relationships with the  team which will allow me to start taking cases faster.”

## 2) Know your Weaknesses before your Interview

Again, a weakness is generally any lack of qualification you may have for the *specific* job you are applying to. A weakness could be:

* A lack of a hard or soft skill
* A lack of years of experience in the job *or* the industry
* A gap in employment
* A lack of education
* A lack of quality references

They will vary with each job you apply to.

You should generally already have a solid understanding of what your  weaknesses are compared to the job description. But if you didn’t study  the job description before submitting your resume, now is the time  before your interview.

1. Highlight all the hard and soft skills written in the job description. 
2. Determine which ones you don’t have or are weak at

Although the job description isn’t always the best way to determine  what’s required of the job or what qualifications you’re expected to  have. To get a better understanding of that, reach out to people at the  company on LinkedIn and ask them questions to get a deeper understanding  of the company culture, the team, and the job.

It’s better to be prepared with what’s coming. So think about what you may get asked about from your unique background.

## 3) The Address and Redirect Method

Now what do you do if you get asked about a weakness in your job  interview?  Whether you bring it up first or you wait for them to, the  strategy is the same. That is:

You **Address** it quickly and then **Redirect** the conversation back to your strength statements.

Honestly, why dwell on the negative? What the hiring manager really  wants you to do is erase their fears and convince him/her that you can  solve the problems the job entails.  Therefore, the more time you can  talk about connecting the dots between your qualifications and the  responsibilities/problems inherent in the position, the better.

And by having a solid understanding of your strengths and knowledge  of your weaknesses, you will be better able to deflect and redirect back  to what they NEED to hear before the interview is over.

The following are four examples of how to use the Address and Redirect Method with the following scenarios:

* Lack of a Skill
* Lack of Industry Experience
* Low GPA
* A Gap in Work Experience

### Example: Lack of a Skill 

Let’s say you’re applying to a Software Engineering job. You have a  lot of skill with Java but not as much with C++, which is what the job  asks for.  So what do you say if you get asked in the interview about  your knowledge of C++? 

**(Address)** *I have a little experience with C++…*

**(Redirect)** *but a few  years ago when I needed to really learn Java, I read books, I took  online courses, and I got help when needed from my coworkers. I was then  ready to write production code in just 2 month and I know that I can do  the same with C++ in this position.*

**Key Point:** If you don’t have a skill they want, then  talk about HOW you’re going to acquire that skill or gain that  experience based on HOW you gained or accomplished some previous skill  or experience in the past.

By default, always redirect back to the value and benefits you’re going to bring to the position.

### Example: Lack of Industry Experience

Let’s say you’re applying to a Sales position in the Computer Data  Storage business. You will be targeting the Financial Market, which you  have no experience in. Although, you do have a lot of experience in the  Oil and Gas Market. How would you respond to a question about your  experience in the Financial market?

**(Address)**  *…*

**(Redirect)** *The  Financial Market is interesting because <KNOWLEDGE 1> and <  KNOWLEDGE 2>. I’ve been talking to successful sales executives in the  Financial Market and what I’ve learned is that <SOLUTION TO COMMON  PROBLEM>. I have a lot of experience in the Oil and Gas market where  in my first couple years at my previous company, I was able to close $10  million in revenue by <RELATE TO SAME SOLUTION TO COMMON  PROBLEM>. I’m confident that I can achieve the same revenue targets  at COMPANY NAME. In fact, I think I can do better.*

**Key Point:** Depending on how the question comes up,  you don’t need to blatantly say you DON’T have a qualification. Instead,  you can jump right into the Redirect and talk about what you know. You  might not have the experience on your resume, but you should have done  your homework and be able to talk intelligently about the qualifications  you lack.

### Example: Low GPA

Let’s say you’re applying for your second job out of college or grad  school and your grades weren’t that great. You only have a year of job  experience and you left out your GPA on your resume. You are asked in an  interview what your GPA was. How would you respond?

**(Address)** *My GPA in school could have been better,*

**(Redirect)** *but since  then I’ve consistently added a lot of value in my past roles. For  example, at my last job I increased customer conversions by 50% by  rewriting the website’s sales copy.*

**Key Point:** You don’t always have the chance to  mention transferable skills/experience when redirecting away from a  weakness like in the last two examples. In this example, you need to go  right into one of your strength statements to ease the hiring manager’s  underlying fear that your low GPA means you won’t be able to do the job.

### Example: A Gap in Work Experience

Let’s say you have a sizable gap in your work history. For whatever  reason, you needed to take a long leave of absence.  How would you  address this in an interview?

**(Address)** *I had to take a leave of absence for a personal issue and now it is resolved.*

**(Redirect)** *…*

**Key Point:** You don’t need to lie, but you don’t need  to go into the details of a work-history gap either. You have no  obligation to tell them what it was for, so don’t disqualify yourself by  doing so. If they ask you for details, then you can just say:

*“I’d prefer to leave it at that. It was a personal issue and now it is resolved.*

Don’t redirect to anything. Just let them move on and ask you their  next question. By handling this situation in this way, you are calmly  redirecting the conversation back to questions where you can communicate  your competencies.

## Conclusion

Successfully addressing questions that expose your weaknesses starts  with a solid understanding of your strengths. When you know how to  connect the dots between your qualifications and the  responsibilities/problems inherent in the position, you can better  deflect or redirect any objection they may have against you.

The more you can redirect the conversation back to what you want them  to hear with powerful strength statements, the easier it will be to  ease any fears they may have towards your ability to do the job well.

Use the Address and Redirect Method and these four examples to craft  your own Strength Statements and steer the interview ship away from  their fears and objections. And when you do, you can better prove that  you are the hire they’ve been looking for. 

&#x200B;

Read More @ HPC Careers | [hpccareers.com](https://hpccareers.com) | Career Strategies for Ambitious HPC/Big Data Professionals

Original Article: [https://hpccareers.com/how-to-turn-your-weaknesses-into-strength-statements-in-a-job-interview/](https://hpccareers.com/how-to-turn-your-weaknesses-into-strength-statements-in-a-job-interview/)",12,1,agmckee,2020-02-27 17:06:56,https://www.reddit.com/r/bigdata/comments/faf7cr/how_to_turn_your_weaknesses_into_strength/,0,bigdata
f4fbos,Apache Druid + Superset for real time analytics?,"Hello everyone,

After having several nightmares with external vendors our team is transforming our work around open-source solutions. 

We have some event data coming from Kafka streams which is processed and stored in Hadoop clusters.  The data contains timestamps, metrics and status information. Currently we use hive access to create dashboards with proprietary commercial solutions which doesn't provide the fast and real time access we need.

We are planning to use Apache Druid and Superset to produce real-time analytics views for our end user.

Being a noob in this domain I wanted to ask others if there are other options that are better than what we are planning to deploy.

Apache Kafka + Hive+ Apache Druid + Superset

Update: Besides general visualization. We would like to have some map visualization.",11,8,rsd_syd,2020-02-15 20:39:02,https://www.reddit.com/r/bigdata/comments/f4fbos/apache_druid_superset_for_real_time_analytics/,0,bigdata
esron5,[2013] You May Not Need Big Data After All,,13,2,None,2020-01-23 11:14:19,https://hbr.org/2013/12/you-may-not-need-big-data-after-all,0,bigdata
epd6us,"We have the results from ""What should you call a group of Data Scientists Poll""!","CLUSTER is your winner!

I really wanted kaggle gaggle to win but the people have spoken

Check out the rest of the results here : [https://greatexpectations.io/blog/datasci-counter-poll/](https://greatexpectations.io/blog/datasci-counter-poll/)",11,0,superconductiveKyle,2020-01-16 03:00:44,https://www.reddit.com/r/bigdata/comments/epd6us/we_have_the_results_from_what_should_you_call_a/,0,bigdata
elxzap,The Power Of Streaming ETL — Flatten JSON With ksqlDB,,11,0,TheSqlAdmin,2020-01-08 20:03:03,https://medium.com/searce/the-power-of-streaming-etl-flatten-json-with-ksqldb-6e8735ed1a9d,0,bigdata
ebib0n,Podcast: Machine learning in finance,,10,0,TheTesseractAcademy,2019-12-16 17:39:00,https://thedatascientist.com/podcast-machine-learning-in-finance/,0,bigdata
e5i98e,An interview about building a successful data team and managing their career growth to power a successful financial business,,10,0,blarghmatey,2019-12-03 14:58:04,https://www.dataengineeringpodcast.com/citadel-data-engineering-episode-109/,0,bigdata
e2fskv,New big data algorithms improve earthquake detection and monitor livestock health,,10,2,qptbook,2019-11-27 13:09:39,https://www.youtube.com/watch?v=gUsmFRaH1cY,0,bigdata
do8t4i,The Complete Data Science LinkedIn Profile Guide,https://bigcloud.io/the-complete-data-science-linkedin-profile-guide/,12,0,BigCloudTeam,2019-10-28 13:26:06,https://www.reddit.com/r/bigdata/comments/do8t4i/the_complete_data_science_linkedin_profile_guide/,0,bigdata
d9c8jw,11 important big data lessons gleaned from TED Talks,,12,0,BridgeofBirds,2019-09-26 00:45:57,https://www.hpe.com/us/en/insights/articles/big-data-ted-talks-11-important-lessons-1909.html,0,bigdata
d5gpzj,"YouTube's Database ""Procella""",,11,0,marklit,2019-09-17 13:07:23,https://tech.marksblogg.com/youtube-database-procella.html,0,bigdata
czxpub,7 mistakes when using Apache Cassandra,,9,0,smlaccount,2019-09-05 07:48:40,https://blog.softwaremill.com/7-mistakes-when-using-apache-cassandra-51d2cf6df519,0,bigdata
ctwlav,Artificial Intelligence vs. Machine Learning vs. Deep Learning: What is the Difference?,,11,1,techgig11,2019-08-22 12:15:31,https://activewizards.com/blog/artificial-intelligence-vs-machine-learning-vs-deep-learning-what-is-the-difference/?utm_source=reddit&utm_medium=machinelearning&utm_campaign=data_science,0,bigdata
cl5qws,Time Series Databases and Operational Historians: Get the Best of Both Worlds With CrateDB and the Crate IoT Data Platform,,12,0,None,2019-08-02 15:40:18,https://crate.io/a/time-series-databases-and-operational-historians-get-the-best-of-both-worlds-with-cratedb-and-the-crate-iot-data-platform/,0,bigdata
c952ja,"What do I do with 44,000,000 rows of data? (28GB)","Bare with me..I’m new to data management. I work for a huge corporate company as a finance and accounting auditor. I took on the task of learning and using data analytics for my audits. 

So far, I am using SQL Server Management Studio to get my data with an ODBC connection. I have created and executed my stored procedure, and put my data into a table. Now I need to create reports. My end game is to make my data user friendly for upper management to access. I would like to use something that provides a drill down option. 

-Microsoft Access won’t work because of the size. 
-I’ve used Excel Power Pivot and this works..but only shows me the first 1,000 rows when I perform the drill down. 
-I’ve used Power BI desktop, but we don’t have Power BI servers, so I can’t publish it for others to access. (Buying Power BI isn’t a feasible option right now.)
-I put my Power Pivot on Sharepoint but it’s so large that I get yelled at by the IT team because it’s expensive to add more storage for class 1 data.

-I’ve explored the option of using Python and Jupyter Notebook, but with it being open source, I’m afraid of the potential lack of security (my data cannot get out!!) and I would have to LEARN Python.
-I’ve explored Microsoft Visual Studio because I’m familiar with SQL. Maybe this is a good option? It’s no Power BI, but the functionality is certainly there.
-I’ve been told I can’t really use ‘R’ because my data is heavy in numbers, not just text. Also, I would have to learn R.
-Maybe SQL Server Reporting Services (SSRS)?  Never really used it, but I could learn. Manipulate the data then maybe a dashboard site?

My data is going to be viewed and interacted with by senior managers who don’t necessarily want to download anything special, let alone learn how to use it. 

I’m willing to learn, I’ve displayed that thus far... but I work full time, in a masters program full time, studying for the CPA exam full time, and studying for the aCAP full time. AKA my time is limited, so any learning would need to be manageable and initially short-term. Please don’t tell me to just “get someone else to do it.” It’s my project and this finely crafted dataset is my baby. I learn incredibly fast and had no idea what data analytics even was two months ago. I’ve come too far to just “give up.”

Thank you!",12,9,caramelempanada101,2019-07-04 17:13:47,https://www.reddit.com/r/bigdata/comments/c952ja/what_do_i_do_with_44000000_rows_of_data_28gb/,0,bigdata
c7df49,How use Apache Airflow with Kubernetes Executor,,12,0,marclamberti,2019-06-30 13:06:12,https://marclamberti.com/blog/airflow-kubernetes-executor/,0,bigdata
bz0p50,I Come Not To Bury Cloudera But To Praise It,,11,6,eljefe6a,2019-06-10 17:31:59,http://www.jesse-anderson.com/2019/06/i-come-not-to-bury-cloudera-but-to-praise-it/,0,bigdata
byu5yf,What is Hive? Architecture & Modes,,12,0,Big_Data_Path,2019-06-10 05:53:46,https://bigdatapath.wordpress.com/2019/06/10/what-is-hive-architecture-modes/,0,bigdata
bxe94l,Using AWK and R to parse 25tb,,10,0,coffeewithalex,2019-06-06 08:23:23,https://livefreeordichotomize.com/2019/06/04/using_awk_and_r_to_parse_25tb/,0,bigdata
bo7prq,Optimisation vs Prediction (part 1): Main differences and when to use them,,11,0,TheTesseractAcademy,2019-05-13 19:07:50,https://www.youtube.com/watch?v=7oJRaxrIqNw,0,bigdata
bo2ufo,Big Data Vs Data Science Vs Data Analytics | Data Science vs Machine Learning,,12,0,zeulg,2019-05-13 12:29:01,http://www.youtube.com/watch?v=IV2iqFIuwVA,0,bigdata
bmoimd,"Collaboration between data engineers, data analysts and data scientists","Hello bigdata community,  


There is several articles on the differences between data engineers, data analysts and data scientists, but not really on how they collaborate. So, I just wanted to share with you this article : [https://medium.com/dailymotion/collaboration-between-data-engineers-data-analysts-and-data-scientists-97c00ab1211f](https://medium.com/dailymotion/collaboration-between-data-engineers-data-analysts-and-data-scientists-97c00ab1211f)


Hope you will enjoy.  


Cheers!",12,0,schrute_dataeng,2019-05-09 20:04:54,https://www.reddit.com/r/bigdata/comments/bmoimd/collaboration_between_data_engineers_data/,0,bigdata
bkyp75,Should data science be driven by theory or by experimental evidence?,,11,2,TheTesseractAcademy,2019-05-05 15:01:58,https://thedatascientist.com/data-science-driven-theory-experimental-evidence/,0,bigdata
bh5hgt,10 Best Big Data Analytics Tools for 2019 – With Uses & Limitations,,11,2,Big_Data_Path,2019-04-25 06:43:56,https://bigdatapath.wordpress.com/2019/04/25/10-best-big-data-analytics-tools-for-2019-with-uses-limitations/,0,bigdata
bcefzl,Circuit breakers in data pipeline,,12,0,satishtvv,2019-04-12 14:51:16,https://quickbooks-engineering.intuit.com/taming-data-quality-with-circuit-breakers-dbe550d3ca78,0,bigdata
b583wv,Tracking Changes In Dimensions in a BigQuery Data Warehouse,,11,0,milike45,2019-03-25 08:48:09,https://aliz.ai/tracking-changes-in-dimensions/,0,bigdata
azub6o,Youfit Health Clubs Uses Data Analytics To Double Online Revenue,,13,1,MrComedy325,2019-03-11 14:49:36,https://www.enterprisedigi.com/data-analytics/articles/youfit-data-digital-transformation,0,bigdata
aycpag,TIBCO Acquires High Performance In-Memory Data Platform SnappyData,,12,1,SnappyAlligator,2019-03-07 13:38:28,https://www.tibco.com/press-releases/2019/tibco-announces-acquisition-high-performance-memory-data-platform-snappydata,0,bigdata
ang6py,What kind of bigdata jobs are more well suited for mathematicians?,"I know linear algebra and probability are usually examples of maths used in bigdata jobs. However, I'd like to submerge a little bit more in this world. I'm interested in knowing where I can be useful. The more abstract the problem is, all the better.

Context: I'm a graduate in (pure) mathematics, but with 7+ work experience as a programmer (a little bigdata, java developer, cybersecurity, etc).",11,2,None,2019-02-05 17:07:48,https://www.reddit.com/r/bigdata/comments/ang6py/what_kind_of_bigdata_jobs_are_more_well_suited/,0,bigdata
alyokk,"3 ways that big data reveals what you really like to watch, read and listen to",,10,0,dreamzdaisy,2019-02-01 05:16:28,https://theconversation.com/3-ways-that-big-data-reveals-what-you-really-like-to-watch-read-and-listen-to-109798,0,bigdata
a9q6qf,Airlines will use 'big data' to avoid dangerous turbulence - Airline Ratings,,12,0,key_info,2018-12-26 17:00:14,https://www.airlineratings.com/news/airlines-use-big-data-to-avoid-dangerous-turbulence/,0,bigdata
a0igrv,Building The Dremio Open Source Data-as-a-Service Platform (Interview),,10,1,blarghmatey,2018-11-26 11:15:24,https://www.dataengineeringpodcast.com/dremio-with-tomer-shiran-episode-58/,0,bigdata
9u001m,ETL for third party connections,"I am consulting a bunch of small tech companies and writing multiple individual connectors to connect client data warehouses like postgresql, firebase, bigquery and even salesforce for my clients to collect their data event triggers or schedule data query that I can run analytics on.

This is becoming a lot of work and very cumbersome to maintain and scale. I am more of a ML/Analytics guy and have not worked with warehousing a lot. I am sure I am not doing this correctly and hence wanted to get your feedback on what is the right approach to take to write a single unified etl pipeline layer which allows me to

1. auth/connect with the warehouses
2. Mimic their schema to run analytics
3. Setup triggers on their warehouses
4. run queries from my ORM that can translate to the format the warehouse will take

I am a python junkie and would prefer if I didn't have to move out of python. Although there are multiple open source implementations doing some sort of the above problem, I was not sure how to choose them and integrate it with my codebase seamlessly. Many of them have a significant learning curve and require modifications to the way I consume data and hence wanted to ensure I do this correctly. I also see that graphQl is picking up pace on db querying as well and just wanted to stay on top of this.

If you think this is a common problem and has been solved already, can you direct me to the right place, since google searches and talking to people is giving me more noise than signal to come to a conclusion.

&#x200B;

From what I get, Nifi seems like a good platform to look into, but I wanted to double check, especially if it is the right tool if the data warehouses are 3rd party connections.

&#x200B;

Asking in good faith",12,17,Capt_Clueless,2018-11-04 02:16:04,https://www.reddit.com/r/bigdata/comments/9u001m/etl_for_third_party_connections/,0,bigdata
9p7gxr,What are best resources to learn Hadoop?,,10,11,None,2018-10-18 08:50:38,https://www.reddit.com/r/bigdata/comments/9p7gxr/what_are_best_resources_to_learn_hadoop/,0,bigdata
9oe0h4,"What is Flink - Introduction to Flink | If Hadoop is 2G, Spark is 3G then Apache Flink is 4G in Big data stream processing",,9,0,jiveshgarg1,2018-10-15 16:06:13,https://youtu.be/9Tn4KRyupUY,0,bigdata
9ns32d,Comment on Cloudera Hortonworks merger,,11,0,None,2018-10-13 07:39:55,https://youtu.be/Cbnu_bYJY9w,0,bigdata
9md4xb,Beginners guide for Cloudera and how to configure it on AWS – EC2!,,9,3,NoahData,2018-10-08 08:39:07,https://indiumsoftware.com/blog/how-to-configure-cloudera-on-aws-ec2/,0,bigdata
9ljkx8,"Quadrant Protocol's CEO, Mike Davie, is featured in IBTimes publication! Find out how SmartCities Are Using Blockchain To Revolutionize Big Data.",,12,0,Cy4cent,2018-10-05 05:36:33,https://www.ibtimes.com/how-smart-cities-are-using-blockchain-revolutionize-big-data-2721596?amp=1&__twitter_impression=true,0,bigdata
9ky7ro,Spark Streaming with Python examples?,"Hi,

A while back I started learning about data science and I did all the tutorials in python. Now I have a practical issue that I would like to solve and it seems like Spark Streaming would be a great fit for me. Naturally I would like to continue using python but I noticed that on the official site there are no examples for Python [https://spark.apache.org/examples.html](https://spark.apache.org/examples.html) 

I found some other links with examples that should be sufficient but I was still wondering if there is a reason for why there are not examples on the official site? Would python still be a good choice to use here? 

I would appreciate if any of you could share your experience on the subject. 

Thanks",11,4,prole92,2018-10-03 05:07:01,https://www.reddit.com/r/bigdata/comments/9ky7ro/spark_streaming_with_python_examples/,0,bigdata
9jtzcd,Bigdata without Java? Most tools I see are Java based!,Are there big data tools for non Java developers? Most tools like Hadoop are Java based. Does it mean I have to learn Java for my BigData venture to be great?,10,11,zemuldo,2018-09-29 04:59:59,https://www.reddit.com/r/bigdata/comments/9jtzcd/bigdata_without_java_most_tools_i_see_are_java/,0,bigdata
9ifjgd,Working with Data Feeds,,11,0,marklit,2018-09-24 06:04:28,http://tech.marksblogg.com/working-with-data-feeds.html,0,bigdata
8vcmqn,I'm looking for U.S. real estate data. Anyone know of a centralized database with information on real estate transactions?,,10,3,ordinary_mind,2018-07-01 20:02:53,https://www.reddit.com/r/bigdata/comments/8vcmqn/im_looking_for_us_real_estate_data_anyone_know_of/,0,bigdata
8rhh4t,How is Singapore Gearing Up for Big Data and AI Revolution | Analytics Insight,,11,0,analyticsinsight,2018-06-16 05:53:54,https://www.analyticsinsight.net/how-is-singapore-gearing-up-for-big-data-and-ai-revolution/,0,bigdata
8ho803,"Building Data Science Capabilities That Scale - Webinar with DataScience.com founder, Ian Swanson",,13,0,None,2018-05-07 15:12:06,https://blog.thedataincubator.com/2018/03/ds30-data-science-com-ian-swanson/?utm_source=reddit&utm_medium=Social,0,bigdata
8gbyqv,Google Cloud Composer is now in beta: build and run practical workflows with minimal effort [managed Apache Airflow!],,13,2,fhoffa,2018-05-01 21:12:27,https://cloud.google.com/blog/big-data/2018/05/cloud-composer-is-now-in-beta-build-and-run-practical-workflows-with-minimal-effort,0,bigdata
8ebhh9,Market Research Using Conjoint Analysis In R,,11,1,martmull,2018-04-23 13:35:15,https://blog.sicara.com/market-research-survey-conjoint-analysis-r-code-3d4f6190c2aa,0,bigdata
88p0b6,Which is the most popular / recommended programming languages for learning big data?,"I would like to learn more about which is the most popular or most recommended programming languages in big data. Is this Java, or Python or R or Scala. What is the order of popularity of these languages? ",11,10,suthukrish,2018-04-01 06:39:16,https://www.reddit.com/r/bigdata/comments/88p0b6/which_is_the_most_popular_recommended_programming/,0,bigdata
87rrgh,Why ‘Big Data’ Will Force Insurance Companies to Think Hard About Race,,10,0,punkthesystem,2018-03-28 13:09:15,https://www.insurancejournal.com/blogs/right-street/2018/03/27/484530.htm,0,bigdata
86nk65,Study Partner for learning Bigdata/Hadoop,"Anyone interested in learning BigData/Hadoop technologies with me.
I myself starting now on my own using youtube videos,books, google etc.
I have experience in RDBMS,ETL,BI .",11,27,Student_HDFS,2018-03-23 19:50:44,https://www.reddit.com/r/bigdata/comments/86nk65/study_partner_for_learning_bigdatahadoop/,0,bigdata
85sr9b,Bitcoin price forecasting with deep learning algorithms,,13,2,viktoriia_shulga,2018-03-20 13:37:39,https://www.activewizards.com/blog/bitcoin-price-forecasting-with-deep-learning-algorithms/?utm_source=reddit&utm_medium=bigdata&utm_campaign=bitcoin,0,bigdata
7z6hrw,Crunchbase opens a marketplace for 3rd-party data in bid to be the ‘master database for companies’,,11,0,MarketIntelligent,2018-02-21 15:48:58,https://techcrunch.com/2018/02/21/crunchbase-opens-a-marketplace-for-3rd-party-data-in-bid-to-be-the-master-database-for-companies/,0,bigdata
7wolhs,Food stamp fraud caught by big data,,10,4,redditcdnfanguy,2018-02-10 22:07:12,http://www.oregonlive.com/portland/index.ssf/2018/02/minimart_owner_gave_100_cash_f.html,0,bigdata
7wj8xj,Big Data Overview,,11,4,Big_Data_Path,2018-02-10 04:16:58,https://bigdatapath.wordpress.com/2018/02/10/big-data-overview/,0,bigdata
7nnufj,What channels regarding Hadoop/Spark do you watch?,"Hi Reddit!

I used Hadoop/Spark some time ago, when switched work a bit and now want to update my knowledge of the platform again. I see a lot of blog articles and study materials from 2011-2015 years, some from 2016 and very rare from 2017. I also saw a lot of changes in Hadoop during last years (hello HA!). Therefore, I want to ask you, dear community, what blogs do you read regarding AI/Machine Learning/Data Warehousing/Data engineering/related things? What conference records do you watch? Where do you talk with other bright minds?

I know, Reddit would be most obvious answer, but /r/bigdata community itself has only 18K readers (if to compare with /r/cats, which has 556K readers), so I guess there must be some other interesting places to dig information in as well.

Thanks in advance!",11,1,m3tamaker,2018-01-02 17:27:13,https://www.reddit.com/r/bigdata/comments/7nnufj/what_channels_regarding_hadoopspark_do_you_watch/,0,bigdata
7ljs35,SiriDB: An Open Source Scalable Timeseries Database For Your System Metrics (Interview),,11,0,blarghmatey,2017-12-22 19:48:08,https://www.dataengineeringpodcast.com/siridb-with-jeroen-van-der-heijden-episode-11/,0,bigdata
7gogio,Apache Spark running on AWS Lambda,,11,0,chief_data_officer,2017-11-30 18:32:51,https://github.com/qubole/spark-on-lambda,0,bigdata
77wrxb,"How Russia can use Metadata to target public and private sector executives, the use of big data by special interest groups to impact elections, and the emergence of data brokers as threats to National Security.",,11,0,hacking_love,2017-10-21 23:21:05,https://www.youtube.com/watch?v=RIvntp5N6jc,0,bigdata
77viu4,How the absence of big data doesn’t stop you from learning cool stuff,,11,0,gretayld,2017-10-21 19:40:44,https://youtu.be/cxICiOaMIzY,0,bigdata
76chye,Support Vector Machines for Classification,,10,0,mubumbz,2017-10-14 14:46:27,https://mubaris.com/2017-10-14/svm-python,0,bigdata
6vyd9p,Looking for a good hadoop forum any suggestions?,"I am looking for some good hadoop forums to ask questions I am already aware of **reddit hadoop , cloudera forum, hortonworks** forum and use them was wondering if you have any others to suggest.

This is basically a follow up to this question.

https://www.reddit.com/r/bigdata/comments/37fsc9/looking_for_a_good_hadoop_not_cloudera_not/


Thanks",11,2,yanks09champs,2017-08-25 13:20:06,https://www.reddit.com/r/bigdata/comments/6vyd9p/looking_for_a_good_hadoop_forum_any_suggestions/,0,bigdata
6jmd7m,How Qubit deduplicates streaming data at scale with Google Cloud Platform (>500K messages per second),,13,0,fhoffa,2017-06-26 16:58:40,https://cloud.google.com/blog/big-data/2017/06/how-qubit-deduplicates-streaming-data-at-scale-with-google-cloud-platform,0,bigdata
6ie3nq,Apache Spark Streaming Tutorial: Identifying Trending Twitter Hashtags,,10,0,ptiunov,2017-06-20 13:44:00,https://www.toptal.com/apache/apache-spark-streaming-twitter,0,bigdata
6hzh6e,Livy: REST Service for Apache Spark - alpha,,13,1,based2,2017-06-18 12:49:54,http://livy.io/overview.html,0,bigdata
6h1fvk,Designing Data Driven Election Campaigns,"In light of elections in the recent years, it is an open secret that candidates have employed Big Data, Digital marketing strategies and Psychometric analysis to sway voter opinion positively towards the favored candidates. 

I want to know what is the complete development process for designing such data driven campaigns. I'll be grateful if any experts can shed light on this.",12,6,None,2017-06-13 17:33:30,https://www.reddit.com/r/bigdata/comments/6h1fvk/designing_data_driven_election_campaigns/,0,bigdata
6fdlqn,Big Data in Sports: Going for the Gold,,11,0,mcclintockelbert,2017-06-05 11:34:05,https://insidebigdata.com/2017/06/04/big-data-sports-going-gold/,0,bigdata
6eu7ki,Big data career -guide me,"I have been working in customer communication management platform in IT for the past 7 years.

one year back I started learning hadoop eco system basics and have worked a bit on the data analytics part in my project .Basically I am a fresher in this area

Now that I am interested in big data. What is the best path for me in terms of building a career . Data engineer or Data scientist(possible to become one without a degree?)",12,1,Madmadridista,2017-06-02 12:57:55,https://www.reddit.com/r/bigdata/comments/6eu7ki/big_data_career_guide_me/,0,bigdata
6a3lrs,"MapD, the GPU-powered DB, is now Open Source; Here's how to compile it.",,12,0,marklit,2017-05-09 05:45:23,http://tech.marksblogg.com/compiling-mapd-ubuntu-16.html,0,bigdata
679v2d,MLJAR: Machine Learning in your browser!,,9,1,pp314159,2017-04-24 15:47:12,https://mljar.com,0,bigdata
65u1zn,New to hadoop ecosystem: where the hell is Flume writing to?,"(EDIT: SOLVED)

I'm trying to learn the hadoop ecosystem, and I was following [this guide] (http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/) to learn it. I'm using the cloudera quickstart VM (virtualbox) and made a few changes to make this functional. I think it should be working. I'm only trying to do the Flume part -- i.e., I just want to get the twitter data and put it in HDFS.

The issue I'm having is... I don't see the data anywhere on HDFS. I'm pretty sure the data ***is*** being taken, because the log looks like this:

     >more /var/log/flume-ng/flume-cmf-flume-AGENT-quickstart.cloudera.log 


    2017-04-16 23:42:46,952 INFO org.apache.flume.source.twitter.TwitterSource: Processed 51,500 docs

    2017-04-16 23:42:49,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop1/92.242.140.2:8020. Already tried 4 time(s); maxRetries=45

    2017-04-16 23:42:49,959 INFO org.apache.flume.source.twitter.TwitterSource: Processed 51,600 docs

    2017-04-16 23:42:53,985 INFO org.apache.flume.source.twitter.TwitterSource: Processed 51,700 docs

So it seems to me like Flume *is* drawing data from twitter, based on the # of docs. 

But when I do

    hadoop fs -ls  /user/

I don't see my flume directory anywhere.

My sink path is:

     TwitterAgent.sinks.HDFS.hdfs.path = hdfs://hadoop1:8020/user/flume/tweets/%Y/%m/%d/%H/


I'm a little lost here. Any help would be much appreciated.",11,9,millenniumpianist,2017-04-17 06:47:51,https://www.reddit.com/r/bigdata/comments/65u1zn/new_to_hadoop_ecosystem_where_the_hell_is_flume/,0,bigdata
61izhb,"Hadoop Has Failed Us, Tech Experts Say",,10,14,arthur_z,2017-03-26 00:52:18,https://www.datanami.com/2017/03/13/hadoop-failed-us-tech-experts-say/,0,bigdata
5x2uzu,Data engineering needs DevOps to navigate big data ecosystem,,11,2,earleanperchinski,2017-03-02 12:12:10,http://searchdatamanagement.techtarget.com/opinion/Data-engineering-needs-DevOps-to-navigate-big-data-ecosystem,0,bigdata
5wnizr,Machine Learning For A Stronger IoT Security Environment,,11,0,LOLrusty,2017-02-28 11:42:56,http://www.greycampus.com/blog/big-data/machine-learning-for-a-stronger-iot-security-environment,0,bigdata
5pq77j,6 Predictions For The $203 Billion Big Data Analytics Market,,11,0,jasonk-iri,2017-01-23 17:28:51,http://www.forbes.com/sites/gilpress/2017/01/20/6-predictions-for-the-203-billion-big-data-analytics-market/#71cf47246c66,0,bigdata
5po5zv,"BigQuery vs. Athena: User Experience, Cost, and Performance",,13,0,sjscott80,2017-01-23 11:38:22,http://logz.io/blog/bigquery-vs-athena/,0,bigdata
5d62rt,"Announcing GPUs for Google Cloud Platform (Tesla P100, K80 and AMD FirePro S9300 x2)",,12,0,fhoffa,2016-11-16 00:09:14,https://cloudplatform.googleblog.com/2016/11/announcing-GPUs-for-Google-Cloud-Platform.html,0,bigdata
595hs9,"Based on a survey of 400 professionals, Experian illustrates the importance of collecting and utilizing quality data in your business strategy",,13,1,KellyFriedman,2016-10-24 16:35:06,http://visual.ly/how-build-business-case-data-quality-your-company,0,bigdata
580200,Picking a cloud database for analytics: the SQL options,,12,1,fhoffa,2016-10-17 21:44:47,http://blog.parsely.com/post/4516/cloud-sql-bigquery-redshift/,0,bigdata
56mmt0,"How Machine Learning, Big Data And AI Are Changing Healthcare Forever",,11,4,Josh_Rod18,2016-10-09 15:26:40,http://www.forbes.com/sites/bernardmarr/2016/09/23/how-machine-learning-big-data-and-ai-are-changing-healthcare-forever/#689cfc1c4f49,0,bigdata
54qf9o,Data mined from social media the real prize in Clinton Trump debate,,10,0,jonfla,2016-09-27 12:55:38,https://www.cnet.com/news/donald-trump-hillary-clinton-presidential-debates-president-isis-fact-check/,0,bigdata
532kxd,Detecting sentiment using Machine Learning in Spark,,11,0,gavlaaaaaaaa,2016-09-16 15:56:47,http://www.lewisgavin.co.uk/Sarcasm-Detector/,0,bigdata
51eh75,Infographic: Why Business Intelligence is Key to Competitive Advantage | Articles | Big Data,,13,0,AC_London,2016-09-06 09:58:49,https://channels.theinnovationenterprise.com/articles/why-business-intelligence-is-key-to-competitive-advantage?utm_source=Reddit&utm_medium=Post&utm_term=Article&utm_content=BI+Infographic&utm_campaign=BI+CH+2016,0,bigdata
50cn2s,Spark comparison: AWS EMR vs. GCP Dataproc,,11,0,fhoffa,2016-08-30 18:30:31,https://www.oreilly.com/ideas/spark-comparison-aws-vs-gcp,0,bigdata
4qdqms,Instrumenting your data for real time analysis,,11,1,cloudedstrife,2016-06-29 04:31:26,http://blog.traintracks.io/instrumenting-your-data-for-real-time-analysis/,0,bigdata
4o2dvj,Building a Streaming Analytics Stack with Apache Kafka and Druid,,12,1,fjpower,2016-06-14 16:56:59,http://www.confluent.io/blog/building-a-streaming-analytics-stack-with-apache-kafka-and-druid?utm_content=29716026&utm_medium=social&utm_source=twitter,0,bigdata
4nc7v1,MLDB: the open-source Machine Learning Database,,10,1,ddcarnage,2016-06-09 18:36:24,http://mldb.ai,0,bigdata
4huaxj,How Atlassian is Using Data-Driven Decisions in Marketing,,12,3,S__OBrien,2016-05-04 12:48:10,https://www.youtube.com/attribution_link?a=AngeAd1vgIs&u=%2Fwatch%3Fv%3DpMqxrymmGOw%26feature%3Dshare,0,bigdata
4hpxuz,Kaggle Python Tutorial on Machine Learning,,12,0,stearnsw,2016-05-03 20:17:37,https://www.datacamp.com/courses/kaggle-python-tutorial-on-machine-learning,0,bigdata
4gzcl0,Could use some advice on Spark/EMR setup.,"I am running into an issue where YARN is killing my containers for exceeding memory limits:

    Container killed by YARN for exceeding memory limits. physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.

I have 20 nodes that are of m3.2xlarge so they have:
  
    cores: 8
    memory: 30
    storage: 200 gb ebs

The gist of my application is that I have a couple 100k assets for which I have historical data generated for each hour of the last year, with a a total dataset size of 2TB. I need to use this historical data to generate a forecast for each asset. My setup is that I first use s3distcp to move the data stored as indexed lzo files to hdfs. I then pull the data in and pass it to sparkSql to handle the json:

     val files = sc.newAPIHadoopFile(""hdfs:///local/*"",
      classOf[com.hadoop.mapreduce.LzoTextInputFormat],classOf[org.apache.hadoop.io.LongWritable],
      classOf[org.apache.hadoop.io.Text],conf)
    val lzoRDD = files.map(_._2.toString)
    val data = sqlContext.read.json(lzoRDD)

I then use a groupBy to group the historical data by asset, creating a tuple of (assetId,timestamp,sparkSqlRow). I figured this data structure would allow for better in memory operations when generating the forecasts per asset.

     val p = data.map(asset =>  (asset.getAs[String](""assetId""),asset.getAs[Long](""timestamp""),asset)).groupBy(_._1)

I then use a foreach to iterate over each row, calculate the forecast, and finally write the forecast back out as a json file to s3.

     p.foreach{ asset =>
      (1 to dateTimeRange.toStandardHours.getHours).foreach { hour =>
        // determine the hour from the previous year
        val hourFromPreviousYear = (currentHour + hour.hour) - timeRange
        // convert to seconds
        val timeToCompare = hourFromPreviousYear.getMillis
        val al = asset._2.toList

        println(s""Working on asset ${asset._1} for hour $hour with time-to-compare: $timeToCompare"")
        // calculate the year over year average for the asset
        val yoy = calculateYOYforAsset2(al, currentHour, asset._1)
        // get the historical data for the asset from the previous year
        val pa = asset._2.filter(_._2 == timeToCompare)
          .map(row => calculateForecast(yoy, row._3, asset._1, (currentHour + hour.hour).getMillis))
          .foreach(json => writeToS3(json, asset._1, (currentHour + hour.hour).getMillis))
      }
    }

- Is there a better way to accomplish this so that I don't hit the memory issue with YARN? 
- Is there a way to chunk the assets so that the foreach only operates on about 10k at a time vs all 200k of the assets? 

Any advice/help appreciated!",12,11,ninja_coder,2016-04-29 13:23:51,https://www.reddit.com/r/bigdata/comments/4gzcl0/could_use_some_advice_on_sparkemr_setup/,0,bigdata
4glwem,Apache Kafka - Jay Kreps Interview,,12,0,_fxdx,2016-04-27 00:28:27,https://www.youtube.com/attribution_link?a=o_TgJh670rA&u=%2Fplaylist%3Flist%3DPLdwhGUMAeoN5iScqoCAmggrTAiyWARCDR,0,bigdata
4bh3i5,What to Do with All That Bandwidth? GPUs For Graph And Predictive Analytics,,13,0,harrism,2016-03-22 12:01:06,https://devblogs.nvidia.com/parallelforall/gpus-graph-predictive-analytics/,0,bigdata
45xaq7,csv2parquet: Create Parquet files from CSV,,11,5,redsymbol,2016-02-15 16:37:23,https://github.com/redsymbol/csv2parquet,0,bigdata
44x35m,Just Using Big Data Isn’t Enough Anymore,,9,1,stormforce7916,2016-02-09 15:14:12,https://hbr.org/2016/02/just-using-big-data-isnt-enough-anymore,0,bigdata
1browv0,Apache Hive 4.0 has been released ,"Hi Guys,

Apache Hive 4.0 has been released . It's a really cool project , do check it out.

https://github.com/apache/hive

https://hive.apache.org/general/downloads/

https://hive.apache.org/",8,5,wizard_of_menlo_park,2024-03-30 18:38:28,https://www.reddit.com/r/bigdata/comments/1browv0/apache_hive_40_has_been_released/,0,bigdata
18s1myk,Understanding Sorting & Partitioning (Find more at youtube.com/@alexmerceddata),,9,0,AMDataLake,2023-12-27 14:14:51,https://v.redd.it/wf8ovkvkiu8c1,0,bigdata
